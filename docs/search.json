[
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/280",
    "text": "Nanodrop 260/280\nThe standard measures of nucleic acid purity from Nanodrop measurements are the 260/280 and 260/230 absorption ratios. The standard advice is that, for a pure RNA sample, 260/280 should be around 2.0, while pure DNA should be around 1.8. Lower ratios are typically indicative of contamination with protein, or some specific contaminants such as phenol.\nThe 260/280 ratios measured for the DNase-treated samples are as follows:\n\nCodeg_260_280 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.8, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 1.9, ymax = 2.1, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_280), shape = 16) +\n  scale_y_continuous(limits = c(0.8, 2.3), breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/280nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_280\n\n\n\nFigure 5: 260/280 ratios for DNase-treated samples.\n\n\n\nNone of the kits returned 260/280 ratios at the expected level for pure RNA, though most were not drastically low. Several returned ratios close to (though below) 1.8, which might be indicative of significant residual DNA in the sample; it’s hard to distinguish between this and protein contamination without doing DNA Qubit measurements on the DNase-treated samples. Free dNTP nucleotides produced by the DNase treatment still absorb at DNA-like ratios, so DNA-like readings may not be indicative of poor quality here.\nAs with yield, the Qiagen AllPrep PowerViral kit does notably worse than the other kits."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/230",
    "text": "Nanodrop 260/230\nThe 260/230 absorption ratio gives a useful indicator of the presence of various contaminants, including phenol, carbohydrates, guanidine, and various salts. The standard advice is that a pure sample should have a 260/230 of around 2.0-2.2, with lower ratios indicating contamination. Readings for our DNase-treated samples are as follows:\n\nCodeg_260_230 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.9, ymax = 2.3, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 2.0, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_230), shape = 16) +\n  scale_y_continuous(#limits = c(0.8, 2.3),\n                     breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/230nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_230\n\n\n\nFigure 6: 260/230 ratios for DNase-treated samples.\n\n\n\nAs we can see, many kits have significantly lower 260/230 ratios than the recommended range, indicating contamination – most likely with guanidinium salts involved in nucleic acid extraction. Combined with the 260/280 results above, this suggests that many kits would benefit from an additional cleanup step. As with other metrics, the Qiagen AllPrep PowerViral kit comes out looking worst.\nTwo kits (NucleoSpin Virus and Zymo quick-RNA) have at least one replicate with significantly higher 260/230 ratios than the recommended values. It’s not very clear what could cause this, but one resource I found suggested misblanking as the likely culprit. I’d recommend repeating the nanodrop measurements for these samples to see if the pattern persists."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop/Qubit ratio",
    "text": "Nanodrop/Qubit ratio\nAn informal heuristic measurement of quality is to compare the measured RNA concentration with Qubit (which is highly specific) to Nanodrop (which is not); the higher the latter compared to the former, the more other material is likely contributing to the Nanodrop reading. The ratios for our DNase-treated samples are as follows:\n\nCodeg_ratio &lt;- ggplot(data, aes(x=kit,y=qubit_nanodrop_ratio)) +\n  geom_point(shape = 16) + \n  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), \n                     name = \"RNA concentration ratio (Qubit/Nanodrop)\", expand = c(0,0)) +\n  scale_x_discrete(name = \"Extraction Kit\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_ratio\n\n\n\nFigure 7: Nanodrop/Qubit ratios for DNase-treated samples.\n\n\n\nAccording to this metric, the Invitrogen PureLink RNA, QIAamp MinElute Virus, and Zymo quick-RNA kits come out looking relatively good, while the Qiagen AllPrep PowerViral kit comes out looking worst."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will's Public NAO Notebook",
    "section": "",
    "text": "Workflow analysis of Spurbeck et al. (2023)\n\n\nCave carpa.\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFollowup analysis of Yang et al. (2020)\n\n\nDigging into deduplication.\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Yang et al. (2020)\n\n\nWastewater from Xinjiang.\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImproving read deduplication in the MGS workflow\n\n\nRemoving reverse-complement duplicates of human-viral reads.\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Rothman et al. (2021), part 2\n\n\nPanel-enriched samples.\n\n\n\n\n\nFeb 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Rothman et al. (2021), part 1\n\n\nUnenriched samples.\n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 3\n\n\nFixing the virus pipeline.\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 2\n\n\nAbundance and composition of human-infecting viruses.\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 1\n\n\nPreprocessing and composition.\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating BLAST validation of human viral read assignment\n\n\nExperiments with BLASTN remote mode\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProject Runway RNA-seq testing data: removing livestock reads\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Project Runway RNA-seq testing data\n\n\nApplying a new workflow to some oldish data.\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the effect of read depth on duplication rate for Project Runway DNA data\n\n\nHow deep can we go?\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing viral read assignments between pipelines on Project Runway data\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInitial analysis of Project Runway protocol testing data\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing options for read deduplication\n\n\nClumpify vs fastp\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Ribodetector and bbduk for rRNA detection\n\n\nIn search of quick rRNA filtering.\n\n\n\n\n\nOct 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\n\n\nTwo tools – how do they perform?\n\n\n\n\n\nOct 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow does Element AVITI sequencing work?\n\n\nFindings of a shallow investigation\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction experiment 2: high-level results & interpretation\n\n\nComparing RNA yields and quality across extraction kits for settled solids\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "href": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "title": "Comparing FASTP and AdapterRemoval for MGS pre-processing",
    "section": "",
    "text": "The first major step in our current MGS pipeline uses AdapterRemoval to automatically identify and remove sequencing adapters, as well as trimming low-quality bases and collapsing overlapping read pairs (it can also discard low-quality reads entirely, but our current pipeline doesn’t use this). An alternative tool, that can do all of this as well as read deduplication, is fastp. I asked the pipeline’s current primary maintainer if there was a good reason we were using one tool instead of the other, and he said that there wasn’t. So I decided to do a shallow investigation of their relative behavior on some example MGS datasets to see how they compare.\nThe data\nTo carry out this test, I selected three pairs of raw Illumina FASTQC files, corresponding to one sample each from two different published studies as well as one dataset provided to us by Marc Johnson:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR19607374\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nFor each sample, I generated FASTQC report files for the raw data, then ran FASTP and AdapterRemoval independently on the FASTQ files and tabulated the results\nThe commands\nFor processing with FASTP, I ran the following command:\nfastp -i &lt;raw-reads-1&gt; -I &lt;raw-reads-2&gt; -o &lt;output-path-1&gt; -O &lt;output-path-2&gt; --failed_out &lt;output-path-failed-reads&gt; --cut_tail --correction\n(I didn’t run deduplication for this test, as AdapterRemoval doesn’t have that functionality.)\nFor processing with AdapterRemoval, I first ran the following command to identify adapters:\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --identify-adapters --threads 4 &gt; adapter_report.txt\nI then ran the following command to actually carry out pre-processing, using the adapter sequences identified in the previous step (NB the minlength and maxns values are chosen to match the FASTP defaults):\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --basename &lt;output-prefix&gt; --adapter1 &lt;adapter1&gt; --adapter2 &lt;adapter2&gt; --gzip --trimns --trimqualities --minlength 15 --maxns 5\n1. Rothman et al. (SRR19607374)\nThis sample from Rothman et al. contains 11.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 39 seconds.\nFASTP detected and trimmed adapters on 3.88M reads (note: not read pairs).\nA total of 133 Mb of sequence was trimmed due to adapter trimming, and 55 Mb due to other trimming processes, for a total of 188 Mb of trimmed sequence.\nA total of 367,938 read pairs were discarded due to failing various filters, leaving 11.21M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 323.9 seconds (a bit under 5.5 minutes).\nAdapterRemoval detected and trimmed adapters on 3.96M reads (note: again, not read pairs).\nA total of 135 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 2,347 read pairs were discarded due to failing various filters, leaving the final read number almost unchanged.\n\n\nCode# Calculate read allocations for Rothman\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed = c(1748896043+1748896043,1627794563+1627794563,1681118328+1681115431)\nbp_discarded = c(0,54014538,276107+271850)\nbp_trimmed = c(0,bp_passed[1]-bp_passed[2]-bp_discarded[2],bp_passed[1]-bp_passed[3]-bp_discarded[3])\n# Tabulate\ntab_rothman &lt;- tibble(status = status, bp_passed = bp_passed, bp_discarded = bp_discarded, bp_trimmed = bp_trimmed)\ntab_rothman\n\n\n\n  \n\n\nCode# Visualize\ntab_rothman_gathered &lt;- gather(tab_rothman, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_rothman &lt;- ggplot(tab_rothman_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_rothman\n\n\n\n\nFASTQC results:\n\nPrior to adapter removal with either tool, the sequencing reads appear good quality, with a consistent average quality score of 30 across all bases in the forward read and ~29 in the reverse read. FASTP successfully raises the average quality score in the reverse read to 30 through trimming and read filtering, while AdapterRemoval leaves it unchanged.\nFASTQC judges the data to have iffy sequence composition (%A/C/G/T); neither tool affects this much.\nAll reads in the raw data are 151bp long; unsurprisingly, trimming by both tools results in a left tail in the sequencing length distribution that was absent in the raw data.\nAs previously observed, the raw data has very high duplicate levels, with only ~26% of sequences estimated by FASTQC to remain after deduplication. Increasing the comparison window to 100bp (from a default of 50bp) increases this to ~35%. Neither tool has much effect on this number – unsurprisingly, since neither carried out deduplication.\nFinally, adapter removal. Unsurprisingly, the raw data shows substantial adapter content. AdapterRemoval does a good job of removing adapters, resulting in a “pass” grade from FASTQC. Surprisingly, despite trimming adapters from fewer reads, fastp does even better (according to FASTQC) at removing adapters.\n\nThe images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n2. Crits-Christoph et al. (SRR23998357)\nThis sample from Rothman et al. contains 48.46M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 99 seconds.\nFASTP detected and trimmed adapters on 13.41M reads (note: not read pairs).\nA total of 270 Mb of sequence was trimmed due to adapter trimming, and 43 Mb due to other trimming processes, for a total of 313 Mb of trimmed sequence.\nA total of 1.99M read pairs were discarded due to failing various filters, leaving 47.47M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 1041.3 seconds (a bit over 17 minutes).\nAdapterRemoval detected and trimmed adapters on 8.22M reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 32,381 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\n\nCode# Calculate read allocations for CritsCristoph\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_cc = c(3683175308+3683175308,3465517525+3467624847,3634186441+3634143668)\nbp_discarded_cc = c(0,120701257,2057612+2224529)\nbp_trimmed_cc = c(0,bp_passed_cc[1]-bp_passed_cc[2]-bp_discarded_cc[2],bp_passed_cc[1]-bp_passed_cc[3]-bp_discarded_cc[3])\n# Tabulate\ntab_cc &lt;- tibble(status = status, bp_passed = bp_passed_cc, bp_discarded = bp_discarded_cc, bp_trimmed = bp_trimmed_cc)\ntab_cc\n\n\n\n  \n\n\nCode# Visualize\ntab_cc_gathered &lt;- gather(tab_cc, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_cc_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nFASTQC results:\n\nAs with Rothman, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp in this case) and high numbers of duplicates. They also, unsurprisingly, have high adaptor content.\nAs with Rothman, fastp successfully improves read quality scores, while AdapterRemoval has little effect. Also as with Rothman, neither tool (as configured) has much effect on sequence composition or duplicates.\n\nIn this case, fastp is highly effective at removing adapter sequences, while AdapterRemoval is only weakly effective. I wonder if I misconfigured AR somehow, because I’m surprised at how many adapter sequences remain in this case. The images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n3. Johnson (COMO4)\nThis sample from Johnson contains 15.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 33 seconds.\nFASTP detected and trimmed adapters on 158,114 reads (note: not read pairs).\nA total of 1.3 Mb of sequence was trimmed due to adapter trimming, and 14.6 Mb due to other trimming processes, for a total of 15.9 Mb of trimmed sequence.\nA total of 0.33M read pairs were discarded due to failing various filters, leaving 15.25M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 311.4 seconds (a bit over 5 minutes).\nAdapterRemoval detected and trimmed adapters on 155,360 reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 5,512 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\nFASTQC results:\n\nAs with previous samples, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp again) and high numbers of duplicates.\nUnlike previous samples, the raw data for this sample shows very low adapter content – plausibly they underwent adapter trimming before they were sent to us?\nNeither tool achieves much visible improvement on adapter content – unsurprisingly, given the very low levels in the raw data.\n\n\nCode# Calculate read allocations for Johnson\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_como = c(1183987584+1183987584,1157539804+1157540364,1182999562+1182970312)\nbp_discarded_como = c(0,36950446,418230+257734)\nbp_trimmed_como = c(0,bp_passed_como[1]-bp_passed_como[2]-bp_discarded_como[2],bp_passed_como[1]-bp_passed_como[3]-bp_discarded_como[3])\n# Tabulate\ntab_como &lt;- tibble(status = status, bp_passed = bp_passed_como, bp_discarded = bp_discarded_como, bp_trimmed = bp_trimmed_como)\ntab_como\n\n\n\n  \n\n\nCode# Visualize\ntab_como_gathered &lt;- gather(tab_como, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_como_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nDeduplication with fastp\nGiven that all three of these samples contain high levels of sequence duplicates, I was curious to see to what degree fastp was able to improve on this metric. To test this, I reran fastp on all three samples, with the --dedup option enabled. I observed the following:\n\nRuntimes were consistently very slightly longer than without deduplication.\nThe number of successful output reads declined from 11.21M to 9.29M for the Rothman sample, from 47.47M to 31.47M, and from 15.25M to 11.03M for the Johnson sample.\nRelative to the raw data, and using the default FASTQC settings, the predicted fraction of reads surviving deduplication rose from 26% to 29% for the Rothman sample, from 45% to 64% for the Crits-Cristoph sample, and from 26% to 32% for the Johnson sample, following fastp deduplication. That is to say, by this metric, deduplication was mildly but not very effective.\nThis relative lack of efficacy may simply be because FASTP identifies duplicates as read pairs that are entirely identical in sequence, while FASTQC only looks at the first 50 base pairs of each read in isolation.\nI think I need to learn more about read duplicates and deduplication before I have strong takeaways here.\nConclusions\nTaken together, I think these data make a decent case for using FASTP, rather than AdapterRemoval, for pre-processing and adapter trimming.\n\nFASTP is much faster than AdapterRemoval.\nFor those samples with high adapter content, FASTP appeared more effective than AdapterRemoval at removing adapters, at least for those adapter sequences that could be detected by FASTQC.\nFASTP appears to be more aggressive at quality trimming reads than AdapterRemoval, resulting in better read quality distributions in FASTQC.\nFASTP provides substantially more functionality than AdapterRemoval, making it easier for us to add additional preprocessing steps like read filtering and (some) deduplication down the line.\n\nHowever, one important caveat is that it’s unclear how well either tool will perform on Element sequencing data – or how well FASTQC will be able to detect Element adapters that remain after preprocessing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "title": "How does Element AVITI sequencing work?",
    "section": "",
    "text": "In September 2023, the NAO team sent several samples to the MIT BioMicro Center, for library preparation and sequencing using their new Element AVITI sequencer. This machine works on quite different principles from Illumina sequencing, but also produces high-volume, paired-end, high-accuracy short reads. Since it looks like we might be using this machine quite a lot in the future, it pays to understand what it's doing. However, I found most quick explanations of Element sequencing much harder to follow than equivalent explanations of Illumina's sequencing technology (e.g. here).\nTo try and understand this better, I dug deeper, using a combination of talks by Element staff on YouTube, their core methods paper, and aggressive interrogation of Claude 2. Given my difficulty understanding this, I figured others on the team might also benefit from a quick-ish write-up of my current best understanding, presented here. Note that this does not go into the performance of Element sequencing, only the underlying mechanisms. Note also that, given the lack of very detailed documentation about many aspects of the process, my understanding here is inevitably more high-level than it would be for e.g. Illumina sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "title": "How does Element AVITI sequencing work?",
    "section": "4a. Background and justification",
    "text": "4a. Background and justification\n\nWhen a polymerase binds a DNA strand, it first positions itself over the boundary between the double-stranded primer region and the single-stranded template region. It then recruits and positions a nucleotide complementary to the first base of the template region, using a combination of base pairing and direct interactions between the nucleotide and the polymerase enzyme itself. Finally, it incorporates the new nucleotide into the elongating daughter strand by connecting it to the end of that strand via a new phosphodiester bond.\n\nUsually, the polymerase then repeats the cycle by recruiting and incorporating a nucleotide complementary to the next base of the template strand; however, if the incorporated nucleotide is a chain terminator, it is unable to do this, and stalls.\n\nIn Illumina sequencing, the terminator nucleotide incorporated by the polymerase is fluorescently labeled, and is imaged following incorporation. The fluorophore is then cleaved off along with the terminator group, and the cycle repeats. As a result, the process of daughter strand elongation and base calling are closely bound together.\nIn Element sequencing, the goal is to separate the processes of daughter strand elongation (above) and base calling, so that the two can be optimized separately. To achieve this, the aim is to call the next unincorporated position in the template sequence, rather than (as in Illumina sequencing) the most recently incorporated position.\nOne theoretical way to do this would be to use an engineered polymerase that is able to recruit complementary nucleotides but not incorporate them. One could supply this polymerase with fluorescent nucleotides, and it would recruit the one complementary to the next position on the template strand. This would occur simultaneously at many different locations on each polony, corresponding to the different copies of the library sequence produced by RNA. One could then image the flow cell to identify the nucleotide type recruited at each polony.\nThe problem with the above approach is low signal persistence. Without incorporation, recruitment of nucleotides by the polymerase is weak and transient: the nucleotide binds its complementary base and the polymerase, remains for a short time, then dissociates. The result is that, for any given polony, too few nucleotides are recruited at any one time to give a sufficient signal for imaging.\nIn order for an approach like this to work, then, we need a way to improve signal persistence without relying on covalent incorporation of nucleotides. Enter avidity sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "title": "How does Element AVITI sequencing work?",
    "section": "4b. Base calling by avidity",
    "text": "4b. Base calling by avidity\n\nThe avidity of a molecular interaction is the accumulated strength of that interaction across multiple separate noncovalent bonds. Even if any single one of these bonds is weak and transient, the overall interaction can be strong and stable if the two molecules interact at many different points.\nIn Element avidity sequencing, the avidite is a large molecular construct, comprising a fluorescently labeled protein core connected to some number of (identical) nucleotides via flexible linker regions. Each of these nucleotide groups can be independently recruited by a polymerase bound to a polony, and positioned based on base-pairing interactions. While each of these nucleotide:template:polymerase interactions is too weak and transient to sustain a strong signal, the avidite as a whole is bound to the polony via multiple such interactions, producing a strong and stable interaction overall.\n\n\nExample avidite structure from the avidity sequencing paper. The core of the molecule consists of fluorescently labeled streptavidin, bound to linker regions via streptavidin:biotin interactions. Three of the four linkers shown here end in nucleotides (specifically, adenosine); the fourth mediates core:core interactions to produce an even larger avidite complex.\n\nExample avidite arm structure, with biotin at one end (top-left) and adenosine at the other (bottom-right).\n\nThe base-calling phase of the avidity sequencing cycle thus proceeds as follows:\n\nPrior to the base-calling phase, the polymerase and nucleotides involved in the elongation phase are detached and washed away.\nThe flow cell is then washed with a mixture containing an engineered polymerase as well as four fluorescently-labeled avidites (one each for A, C, G and T). The engineered polymerase (henceforth the avidite-binding polymerase, or ABP) is distinct from that used for elongation, and is capable of binding a template strand and recruiting a complementary nucleotide, but not capable of incorporation.\nThe ABPs bind to the double-stranded regions of each polony and position themselves at the boundary with the single-stranded template region. They then attempt to recruit nucleotides complementary to the next position on the template strand. The only nucleotides available are those attached to the avidites, which are thus recruited. \nSince each copy of the template sequence in each polony is synchronized, each polymerase bound to each polony attempts to recruit the same nucleotide type, and thus interacts with the same type of avidite. Each avidite molecule is thus recruited to multiple points on the polony, producing a stable overall interaction.\n\n\n\nMultiple copies of the same avidite molecule are thus recruited to each polony, producing a strong and uniform fluorescent signal.\n\n\n\nThe flow cell is then imaged to identify the avidite bound to each polony, and thus the next nucleotide in each read. After this, the ABPs and avidites are detached and washed away, and the cycle proceeds to the next elongation phase (see above)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html",
    "href": "notebooks/2023-10-13_rrna-removal.html",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "",
    "text": "See also:\nA useful step in processing wastewater MGS data is the removal of (primarily bacterial) ribosomal RNA sequences. These often make up a substantial fraction of all reads in the dataset, and their removal can both speed up downstream processing and potentially improve certain downstream metrics (e.g. library complexity). Our current pipeline counts and lists rRNA reads using Ribodetector, a deep-learning-based tool that is sensitive and specific, but slow. This slowness makes it annoying to work Ribodetector into our broader pipeline – for example, the time taken to classify reads with Ribodetector is much more than the time saved on downstream steps by excluding rRNA reads from our pipeline.\nI wanted to see if there were alternative rRNA detection methods that gave good-enough results while being fast enough to include in the preprocessing phase of the pipeline. To that end, I investigated bbduk, a k-mer based contamination-detection approach suggested in this biostars thread1.\nTo compare these tools, I applied bbduk and Ribodetector to three samples from pre-existing wastewater metagenomics datasets. Two of these, from Johnson and Crits-Christoph, were the same as in my last methods comparison post. I initially also used the same sample from the third dataset, Rothman et al. (2021), but switched to a different sample when I realised the first had undergone panel enrichment for respiratory viruses and so wasn’t truly untargeted.\nFor each sample, I ran rRNA removal on the FASTQ files that had been preprocessed with FASTP (without deduplication. For SortMeRNA and bbduk, I used reference files (1,2) downloaded from the SILVA database.\nThe commands I ran were as follows:"
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "href": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "High-level output",
    "text": "High-level output\nRunning Ribodetector in high-MCC mode set took 1266 seconds (just over 21 minutes). The tool identified 6.87M out of 15.25M read pairs as ribosomal: 45.0%. Re-running in high-recall mode took a similar amount of time (1264 seconds) and identified 7.76M reads as ribosomal: 50.9%.\nRunning bbduk took a total of 36 seconds. The tool identified 7.57M out of 15.25M read pairs as ribosomal: 49.63%, a little under the result for Ribodetector’ high-recall mode.\nThe default bbduk command given above uses the default k-mer size of 27. Increasing k will increase the precision and decrease the recall of the bbduk algorithm, potentially moving the results closer to the high-MCC version of Ribodetector. After some experimentation, I found that a k-mer length of 43 returned a ribosomal fraction of 45.39%, only slightly above high-MCC ribodetector."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "href": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Overlap between tools",
    "text": "Overlap between tools\nTo compare the output of Ribodetector (high-MCC and high-recall) and bbduk (low and high k) in more detail, I extracted and downloaded the read IDs from their respective rRNA files and compared the overlap between the lists of IDs:\n\nCodedata_dir &lt;- \"../data/2023-10-16_ribodetection/\"\njohnson_bbduk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_failed_ids.txt.gz\")\njohnson_bbduk_highk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_highk_failed_ids.txt.gz\")\njohnson_rd_precision_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_failed_ids_1.txt.gz\")\njohnson_rd_recall_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_recall_failed_ids_1.txt.gz\")\njohnson_bbduk_reads &lt;- readLines(johnson_bbduk_reads_path)\njohnson_bbduk_highk_reads &lt;- readLines(johnson_bbduk_highk_reads_path)\njohnson_rd_precision_reads &lt;- readLines(johnson_rd_precision_reads_path)\njohnson_rd_recall_reads &lt;- readLines(johnson_rd_recall_reads_path)\njohnson_read_ids &lt;- list(bbduk = johnson_bbduk_reads,\n                 bbduk_highk = johnson_bbduk_highk_reads,\n                 rd_precision = johnson_rd_precision_reads,\n                 rd_recall = johnson_rd_recall_reads)\n\n\n\nCodeg_venn_johnson &lt;- ggVennDiagram(johnson_read_ids, label_alpha=0, edge_size = 0) +\n  scale_fill_gradient(low = \"#FFFFFF\", high = \"#4981BF\")\ng_venn_johnson\n\n\n\n\nSome observations:\n\nAs expected, the rRNA reads returned by bbduk with a high k-value are a strict subset of those returned with a low k-value, and those returned by Ribodetector in high-precision mode are a strict subset of those returned in high-recall mode.\n\nAcross all read IDs identified as ribosomal under any of the four conditions, 82% (6.42M read pairs) were identified as such by all four conditions. Of those that remain:\n\n5% are identified by all conditions except high-k bbduk;\n6% are identified by all conditions except high-precision Ribodetector;\n2% are identified by both high-recall conditions, but neither high-precision condition;\n3% are identified by high-recall Ribodetector only;\n1% are identified by low-k bbduk only;\n1% by some other combination of conditions\n\n\nOverall, it seems that, while the two high-recall methods exhibit quite good agreement (sharing &gt;95% of identified sequences), the two high-precision methods agree less well (with &gt;10% of all identified sequences identified by one but not the other)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "href": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Footnotes",
    "text": "Footnotes\n\nI also began investigating SortMeRNA, a published tool based on heuristic alignment, which generally appeared to perform second-best on the quality metrics from the Ribodetector paper. However, I quickly dropped SortMeRNA, as it was substantially slower than Ribodetector.↩︎\nAt least as far as Kraken2 is concerned. I don’t entirely trust Kraken2’s assignments, but am going with them for now. I might come back and dig more into this aspect of things later if it seems useful.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-19_deduplication.html",
    "href": "notebooks/2023-10-19_deduplication.html",
    "title": "Comparing options for read deduplication",
    "section": "",
    "text": "See also:\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\nComparing Ribodetector and bbduk for rRNA detection\n\nDuplicate read pairs can arise in sequencing data via several mechanisms.\n\nBiological duplicates are sequences that arise from different source nucleic-acid molecules that genuinely have the same sequence; these tend to arise when a particular gene or taxon is both extremely abundant and has low sequence diversity. In our case, the most likely cause of biological duplicates are ribosomal RNAs.\nTechnical duplicates, meanwhile, arise when the same input molecule produces multiple reads. Subgroups of technical duplicates include PCR duplicates arising from amplification of a single input sequence into many library molecules, and several forms of sequencing duplicates arising from errors in the sequencing process. For example, Illumina sequencing on unpatterned flow cells can give rise to optical duplicates, where a single cluster on the flow cell is falsely identified as two by the base-calling algorithm.\n\nIn general, we want to remove or collapse technical duplicates, while retaining biological duplicates. Unfortunately, in the absence of UMIs, there’s generally no way to distinguish biological and PCR duplicates; however, many forms of sequencing duplicates can often be identified from the sequence metadata provided in the FASTQ file.\nA number of tools are available that attempt to remove some or all of the duplicate sequences in a file. Some of these use cluster positioning information to distinguish sequencing duplicates from other duplicates, while others identify duplicates based purely on their base sequence. In the latter case, the tricky part is identifying the correct threshold for duplicate identification. Due to sequencing errors, requiring perfect base identity between two reads in order to designate them as duplicates often results in true duplicates surviving the deduplication process. On the other hand, designating two reads as duplicates based on too low a sequence identity (or too short a subsequence) will result in spurious deduplications that needlessly throw away useful data.\nAt the time of writing, our standard sequencing pipeline carries out deduplication very late in the process, during generation of JSON files for the dashboard (i.e. after generating clade counts). Read pairs are identified as duplicates if they are identical in the first 25 bases of both the forward and the reverse read, requiring 50nt of matches overall.\nI wanted to see if there was a widely-used read deduplication tool that we could apply to our pipeline, ideally early on as part of sample preprocessing. I started out comparing four approaches, before fairly quickly cutting down to two:\n\nfastp is a FASTQ pre-processing tool previously investigated here. If run with the --dedup`flag, it will remove read pairs that are exact duplicates of one another. As far as I know, fastp doesn’t have the ability to identify or remove inexact duplicates, or to distinguish sequencing duplicates from other duplicates. It thus represents a fast but relatively unsophisticated option.\nClumpify is a tool that was originally developed to reduce space and improve processing speed by rearranging fastq.gz files. It identifies duplicates by looking for reads (or read pairs) that match exactly in sequence, except for a specified number of permitted substitutions. This, to me, is the obvious way to detect duplicates, and this is the only deduplication tool I’ve found that does it this way. It also allows distinguishing of optical vs other duplicates and specific removal of only optical duplicates if desired.\nGATK Picard’s MarkDuplicates and samtools markdup are two functions for removing duplicate reads in SAM/BAM files. I tried both, but found them to be slow, confusing, and apparently unable to actually detect any duplicates in the files I ran them on. It’s possible that both of these tools only work on mapped reads (which would make sense given their demand for SAM/BAM files); it’s also possible that I could make them work given more time and effort, but I didn’t want to put this in unless the other tools I found proved inadequate.\n\nTo test fastp and Clumpify, I ran them on the same samples I used for looking into ribodetection:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR14530880\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nIn each case, I tested deduplication at two points in the pipeline: immediately after preprocessing (with FASTP, without deduplication), and following segregation of rRNA reads using bbduk.\n1. Johnson (COMO4)\nFollowing fastp preprocessing, the Johnson sample contains 15.25M read pairs, 74.36% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (as identified by bbduk), this number fell to 69.72%, while for ribosomal reads it rose to 79.13%. Needless to say, all of these fail FASTQC’s read-duplication QC test.\nRunning clumpify on the fastp output took 16 seconds and removed 5.04M read pairs (33.0%) as duplicates, 11576 of which were optical. Running it on the bbduk non-ribosomal output took 9 seconds and removed 2.71M (35.3%) reads as duplicates, 7107 of which were optical. After deduplication with clumpify, FASTQC identifies 24.19% of sequences as duplicates in the post-fastp dataset, and 18.38% in the post-bbduk dataset. In both cases, this is a dramatic reduction, sufficient for FASTQC to now mark the duplication level QC as passing where it was previously failing.\nRunning fastp with deduplication enabled on the fastp output took 33 seconds and removed 4.21M read pairs (27.6%) as duplicates. Running it on the bbduk non-ribosomal output took 17 seconds and removed 2.33M read pairs (30.4%) as duplicates. In both cases, the number of reads removed is lower than that removed by clumpify, consistent with the fact that fastp requires complete identity between duplicates while clumpify allows a small number of mismatches. For whatever reason, this difference resulted in a dramatic difference in FASTQC quality metrics: after deduplication with fastp, FASTQC identifies 68.09% of reads as duplicates in the full dataset, and 59.78% in the post-bbduk dataset. Both of these are high enough to result in a failure for the corresponding QC test.\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n24.19\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n68.09\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n18.38\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n59.78\nFailed\n\n\n2. Rothman (SRR14530880)\nFollowing fastp preprocessing, the Rothman sample contains 13.61M read pairs, 81.27% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (6.20M read pairs, as identified by bbduk), this number fell to 78.79%. Both of these failed QC.\nRunning clumpify on the fastp output took 14 seconds and removed 7.41M read pairs (54.5%) as duplicates. Running it on the bbduk non-ribosomal output took 7 seconds and removed 2.96M read pairs (47.8%) as duplicates. In both cases, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 26 seconds and removed 6.87M read pairs (50.5%) as duplicates. Running it on the bbduk non-ribosomal output took 14 seconds and removed 2.71M read pairs (43.6%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n27.9\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n62.9\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n33.0\nWarning\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n63.2\nFailed\n\n\n\nAs before, the difference is large, with Clumpify performing dramatically better.\n3. Crits-Christoph et al. (SRR23998357)\nFinally, we have the sample from Crits-Christoph et al. (2021). Following fastp processing, this sample contained 47.47M read pairs, 55.9% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (42.45M read pairs, as identified by bbduk), this number fell to 53.1%. Both of these failed QC.\nRunning clumpify on the fastp output took 90 seconds and removed 19.25M read pairs (40.6%) as duplicates. Running it on the bbduk non-ribosomal output took 65 seconds and removed 16.49M read pairs (38.8%) as duplicates. As with Rothman, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 94 seconds and removed 16.58M read pairs (34.9%) as duplicates. Running it on the bbduk non-ribosomal output took 85 seconds and removed 14.12M read pairs (33.3%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\nBoth programs perform much better here than for previous samples, with even fastp reducing levels of duplicates low enough to avoid failing. Nevertheless, clumpify continues to perform substantially better.\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n10.64\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n35.22\nWarning\n\n\nRibodepleted (BBDUK)\nClumpify\n9.89\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n32.07\nWarning\n\n\n\n\nCodeduplicate_data_path &lt;- \"../data/2023-10-19_deduplication/duplicate-data.csv\"\nduplicate_data &lt;- read_csv(duplicate_data_path, show_col_types = FALSE) %&gt;%\n  mutate(sample_display = paste0(sample, \" (\", dataset, \")\"),\n         dedup_method = fct_inorder(dedup_method),\n         sample_display = fct_inorder(sample_display))\ng_duplicate &lt;- ggplot(duplicate_data, aes(x=dedup_method, y=fastqc_pc_dup, fill=sample_display)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(name = \"% Duplicates (FASTQC)\", limits = c(0,100),\n                     breaks = seq(0,100,20), expand = c(0,0)) +\n  scale_x_discrete(name = \"Deduplication method\") +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Sample\") +\n  facet_grid(. ~ processing_stage) +\n  theme_base\ng_duplicate\n\n\n\n\n4. Conclusion\nI was originally planning to do more validation here, but I honestly don’t think it’s required. Of the two methods I managed to successfully apply to these samples, Clumpify is faster; applies an intuitively more appealing deduplication method; removes more sequences; and achieves much better FASTQC results. It’s also very easy to use and configure. I recommend it as our deduplication tool going forward.\nIn terms of when to apply the tool, I think it probably makes most sense to apply deduplication downstream of counting (if we use Ribodetector) or detecting and filtering (if we use bbduk) ribosomal reads, to avoid spurious filtering of rRNA biological duplicates."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html",
    "href": "notebooks/2023-10-24_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-01_project-runway-comparison.html",
    "href": "notebooks/2023-11-01_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication, at least on its own, though it might still be successful if applied twice (after reverse-complementing one of the FASTQ files) or in combination with another tool.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I’ve updated negatively about Clumpify, which appears to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates). I’m not sure yet how to handle this; possibly it makes sense to run reads through multiple fast deduplication tools in order to catch as many duplicates as possible.\n\nFASTP deduplication is also unable to catch these cases.\nI’m current unaware of a public tool that will catch these. We might need to build our own."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html",
    "href": "notebooks/2023-10-31_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-comparison.html",
    "href": "notebooks/2023-11-02_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication. UPDATE: I found an option for Clumpify that seems to solve this problem, at least in this case. See Conclusions for more.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I initially updated negatively about Clumpify, which appeared to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates).\n\nHowever, I found an option for Clumpify which addresses this issue, at least in this case. Specifically, one can configure Clumpify to unpair reads, perform deduplication on the forward and reverse reads all together, and then restore pairing. This successfully removes this class of duplicates.\nI’m a little worried that this approach might sometimes cause complete loss of all duplicate reads (rather than all-but-one-pair) when the best-quality duplicate differs between forward and reverse reads. I tried this out by artificially modifying the quality scores for the duplicate reads from D23-13405-2, and this doesn’t seem to be the case at least in this instance: when quality across read pairs was concordant, I was able to control which read pair survived as expected, and when it was discordant, one read pair survived anyway. Still, this remains a niggling doubt."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "",
    "text": "One relevant question for both Project Runway and other NAO sequencing is: what is the maximum read depth at which we can sequence a given sample while retaining an acceptable level of sequence duplication?\nAs discussed in a previous entry, duplicate reads can arise in sequencing data from a variety of processes, including true biological duplicates present in the raw sample; processing duplicates arising from amplification and other processes during sample and library prep; and sequencing duplicates arising from various processes on the actual flow cell.\nAs we sequence more deeply, we expect the fraction of biological and processing duplicates (but not, I think, sequencing duplicates) in our read data to increase. In the former case, this is because we are capturing a larger fraction of all the input molecules in our sample; in the latter, because we are sequencing copies of the same sequence over and over again. Intuitively, I expect the increase in processing duplicates to swamp that in biological duplicates at high read depth, at least for library prep protocols that involve amplification1.\nOne simple approach to investigate the overall effect of read depth on duplication levels in the sample is rarefaction: downsampling a library to different numbers of reads and seeing how the duplication rate changes as a function of read count. In this notebook entry, I apply this approach to sequencing data from the Project Runway initial DNA dataset, to see how duplication rate behaves in this case."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt might be worth explicitly modeling the difference in behavior between different kinds of duplicates as sequencing depth increases, to see if these intuitions are borne out.↩︎\nProbably the biggest two improvements that could be made to this model in future are (i) introducing biological duplicates, and (ii) introducing sequence-specific bias in PCR amplification and cluster formation.↩︎\nI’d appreciate it if someone else on the team can check my math here.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "href": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "title": "Project Runway RNA-seq testing data: removing livestock reads",
    "section": "",
    "text": "In my last entry, I presented my Nextflow workflow for analyzing viral MGS data, as well as the results of that workflow applied to our recent BMC RNA-seq dataset. One surprising thing I observed in those data was the presence of bovine and porcine sequences confounding my pipeline for identifying human-infecting-virus reads. To address this problem, I added a step to the pipeline to remove mammalian livestock sequences in a manner similar to the pre-existing human-removal step, by screening reads against cow and pig genomes using BBMap. In this short entry, I present the results of that change.\nAs expected, the bulk of the pipeline performed identically to the last analysis. Mammalian read depletion removed between 8k and 18k reads per protocol (0.007% to 0.017%):\n\nCode# Import stats\nkits &lt;- tibble(sample = c(\"1A\", \"1C\", \"2A\", \"2C\", \"6A\", \"6C\", \"SS1\", \"SS2\"),\n               kit = c(rep(\"Zymo quick-RNA kit\", 2),\n                       rep(\"Zymo quick-DNA/RNA kit\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (new)\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (old)\", 2))) %&gt;%\n  mutate(kit = fct_inorder(kit))\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\ndata_dir_1 &lt;- \"../data/2023-12-19_rna-seq-workflow/\"\ndata_dir_2 &lt;- \"../data/2023-12-22_bmc-cow-depletion/\"\nbasic_stats_path &lt;- file.path(data_dir_2, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir_2, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir_2, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir_2, \"qc_quality_sequence_stats.tsv\")\n# Extract stats\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\n# Plot stages\nbasic_stats_stages &lt;- basic_stats %&gt;% group_by(kit,stage) %&gt;% \n  summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx), .groups = \"drop\", mean_seq_len = mean(mean_seq_len), percent_duplicates = mean(percent_duplicates))\ng_reads_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_read_pairs, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_bases_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_bases_approx, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Bases (approx)\", expand=c(0,0)) +\n  theme_kit\nlegend &lt;- get_legend(g_reads_stages)\ntnl &lt;- theme(legend.position = \"none\")\nplot_grid((g_reads_stages + tnl) + (g_bases_stages + tnl), legend, nrow = 2,\n          rel_heights = c(4,1))\n\n\n\n\n\nCode# Import composition data\ncomp_path &lt;- file.path(data_dir_2, \"taxonomic_composition.tsv\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  mutate(classification = sub(\"Other_filtered\", \"Other filtered\", classification)) %&gt;%\n  arrange(desc(p_reads)) %&gt;% mutate(classification = fct_inorder(classification))\ncomp_kits &lt;- inner_join(comp, kits, by=\"sample\") %&gt;%\n  group_by(kit, classification) %&gt;%\n  summarize(t_reads = sum(n_reads/p_reads), n_reads = sum(n_reads), .groups = \"drop\") %&gt;%\n  mutate(p_reads = n_reads/t_reads) %&gt;% ungroup\n# Plot overall composition\ng_comp &lt;- ggplot(comp_kits, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- comp_kits %&gt;% filter(p_reads &lt; 0.1)\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,0.02), breaks = seq(0,0.02,0.004),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp_minor\n\n\n\n\nTo identify human-viral reads, I used the multi-stage process specified in the last entry. Specifically, after removing human reads with BBmap, the remaining reads went through the following steps:\n\nAlign reads to a database of human-infecting virus genomes with Bowtie2, with permissive parameters, & retain reads with at least one match. (Roughly 20k read pairs per kit, or 0.25% of all surviving non-host reads.)\nRun reads that successfully align with Bowtie2 through Kraken2 (using the standard 16GB database) and exclude reads assigned by Kraken2 to any non-human-infecting-virus taxon. (Roughly 5500 surviving read pairs per kit.)\nCalculate length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\). Filter out reads that don’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\n\nCode# Import Bowtie2/Kraken data and perform filtering steps\nmrg_path &lt;- file.path(data_dir_2, \"hv_hits_putative_all.tsv\")\nmrg0 &lt;- read_tsv(mrg_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev))\nmrg1 &lt;- mrg0 %&gt;% filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2)\nmrg2 &lt;- mrg1 %&gt;% \n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\")))) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nmrg_all &lt;- bind_rows(mrg0, mrg1, mrg2)\n# Visualize\ng_mrg &lt;- ggplot(mrg_all, aes(x=adj_score_fwd, y=adj_score_rev, color=assigned_hv)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby Kraken2\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(filter_step~kit, labeller = labeller(filter_step=function(x) paste(\"Step\", x), kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nApplying all of these filtering steps left a total of 227 read pairs across all kits: 14 fewer than in the previous analysis. The 14 excluded read pairs included all 11 cow and pig reads, as well as three other read pairs classified as non-viral by BLAST. After removing these reads, the distribution of read scores vs BLAST HV status looked like the following, with many fewer high-scoring non-HV reads:\n\nCodemrg_old_path &lt;- file.path(data_dir_1, \"hv_hits_putative.tsv\")\nmrg_old &lt;- read_tsv(mrg_old_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev)) %&gt;% \n  filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2) %&gt;%\n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\"))[1])) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nhv_blast_old &lt;- list(\n  `1A` = c(rep(TRUE, 40), TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE),\n  `1C` = c(rep(TRUE, 5), \"COWS\", TRUE, TRUE, FALSE, TRUE, rep(FALSE, 9)),\n  `2A` = c(rep(TRUE, 10), \"COWS\", TRUE, \"COWS\", \"COWS\", TRUE,\n           FALSE, \"COWS\", FALSE, TRUE, TRUE,\n           FALSE, FALSE, FALSE, FALSE, TRUE),\n  `2C` = c(rep(TRUE, 5), TRUE, \"COWS\", TRUE, TRUE, TRUE, \n           TRUE, TRUE, \"COWS\", TRUE, \"COWS\", \n           FALSE, FALSE, FALSE), \n  `6A` = c(rep(TRUE, 10), FALSE, TRUE, FALSE, FALSE, FALSE, \n           FALSE, TRUE, TRUE, FALSE), \n  `6C` = c(rep(TRUE, 5), \"PIGS\", TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, FALSE, FALSE, FALSE), \n  SS1  = c(rep(TRUE, 5), rep(FALSE, 10),\n           \"FALSE\", \"COWS\", \"FALSE\", \"FALSE\", \"FALSE\",\n           rep(FALSE, 15)\n           ),\n  SS2  = c(rep(TRUE, 25), TRUE, \"COWS\", TRUE, TRUE, TRUE,\n           TRUE, TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, TRUE, FALSE, FALSE,\n           FALSE, FALSE, TRUE, FALSE, FALSE,\n           rep(FALSE, 5),\n           FALSE, FALSE, FALSE, FALSE, TRUE,\n           FALSE, FALSE, TRUE, TRUE, FALSE)\n  )\nmrg_old_blast &lt;- mrg_old %&gt;% group_by(sample) %&gt;%\n  mutate(seq_num = row_number()) %&gt;% ungroup %&gt;%\n  mutate(hv_blast = unlist(hv_blast_old),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\nmrg_old_blast_filtered &lt;- mrg_old_blast %&gt;% filter(seq_id %in% mrg2$seq_id)\ng_mrg3_1 &lt;- mrg_old_blast_filtered %&gt;% mutate(hv_blast = hv_blast == \"TRUE\") %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=hv_blast)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby BLAST\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(kraken_label~kit, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg3_1\n\n\n\n\nRepeating the exercise from the last entry, in which different score thresholds are assessed along different performance metrics, we find a significant improvement in optimal F1 score compared to the pre-filtering dataset, with a maximum F1 of 0.958 for a conjunctive threshold and 0.966 for a disjunctive threshold (both at a threshold score value of 20):\n\nCode# Test sensitivity and specificity\ntest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, hv_blast %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(hv_blast, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nstats_conj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=TRUE) %&gt;% bind_rows\nstats_disj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=FALSE) %&gt;% bind_rows\nstats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n  pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n  mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats &lt;- ggplot(stats_all, aes(x=threshold, y=value, color=metric)) +\n  geom_line() +\n  geom_vline(data = threshold_opt, mapping=aes(xintercept=threshold), color=\"red\",\n             linetype = \"dashed\") +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats\n\n\n\n\nAs such, it appears that filtering mammalian livestock genomes is quite successful in improving our HV detection pipeline, at least for this dataset."
  },
  {
    "objectID": "notebooks/2024-01-30_blast-validation.html",
    "href": "notebooks/2024-01-30_blast-validation.html",
    "title": "Automating BLAST validation of human viral read assignment",
    "section": "",
    "text": "In previous entries, I presented the results of my Nextflow workflow on recent BMC RNA-seq data. To validate and calibrate my Bowtie2/Kraken2 pipeline for identifying human-viral reads, I manually BLASTed putative viral reads against NCBI’s nt database using their online BLAST tool, then assessed the results by eye to determine if each read likely came from a human virus. This approach was effective, but slow and manual, and thus hard to scale to other datasets I wanted to analyze with a higher relative abundance of viral sequences. I thus investigated options for automating the process to generate custom tabular BLAST results on the command line, which could then be read and processed programmatically.\nAmong the several options I investigated, the most promising were (1) Elastic-BLAST, NCBI’s own cloud-based BLAST tool, and (2) running regular command-line blast with the -remote option enabled to query NCBI’s online databases. I don’t currently have the AWS permissions needed to run Elastic-BLAST on our AWS service, and blastn -remote initially seemed too slow to be useful when run on an EC2 instance. However, I discovered that the latter option appears to run much more quickly when run on my local machine, making this a much more attractive option, at least until I’m able to run Elastic-BLAST.\nTo evaluate the potential for this approach to semi-automate the process of HV read validation, I ran BLASTN on the 227 putative HV reads from my previous BMC entry, using the following command:\nblastn -query &lt;input_path&gt; -out &lt;output_path&gt; -db nt -remote -perc_identity 60 -max_hsps 5 -num_alignments 50 -qcov_hsp_perc 30 -outfmt \"6 qseqid sseqid sgi staxid qlen evalue bitscore qcovs length pident mismatch gapopen sstrand qstart qend sstart send\"\nTo begin with, I first needed to identify a list of taxids I could use to designate a read as human-viral. To do this, I took the list of HV taxids generated by my Nextflow workflow and expanded it to include any missing descendent taxids using the NCBI taxid tree structure. I also generated another list of taxids that included any taxid descended from any of these taxids or the overall virus taxid (10239); this latter approach decreases the risk of false negatives at the cost of potentially increasing the risk of false positives from phage sequences. The two approaches generated lists of 28,105 and 234,499 taxids, respectively.\n\nCode# Set file paths\ndata_dir &lt;- \"../data/2024-01-30_blast/\"\nreads_db_path &lt;- file.path(data_dir, \"bmc-hv-putative.tsv\")\nblast_results_path &lt;- file.path(data_dir, \"bmc-hv-putative.blast\")\nhv_taxids_path &lt;- file.path(data_dir, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir, \"nodes.dmp.gz\")\n\n# Import files for viral taxid search\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids_1 &lt;- expand_taxids(hv_taxids$taxid, tax_nodes)\nv_taxids_2 &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n\nI next imported the BLAST alignment results and performed exploratory data analysis. Following visual inspection, it appeared that the following process would generate good results:\n\nIf both the forward and reverse reads from a read pair align to the same viral taxid (given the specified filters on query coverage, percent identity, etc), classify that read pair as human viral.\nIf only one of the reads in a read pair aligns to any given viral taxid, flag the read for manual inspection.\nIf neither read aligns to a viral taxid, classify that read pair as non-human-viral.\n\n\nCode# Prepare and import BLAST results\nreads_db &lt;- read_tsv(reads_db_path, show_col_types = FALSE)\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Filter BLAST results by read ID\nreads_db_seqs &lt;- reads_db %&gt;% mutate(read_id = paste(sample, seq_num, sep=\"_\")) %&gt;% pull(read_id) %&gt;% c(paste(., \"1\", sep=\"_\"), paste(., \"2\", sep=\"_\"))\nblast_results_filtered &lt;- blast_results %&gt;% filter(qseqid %in% reads_db_seqs)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_paired &lt;- blast_results_filtered %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_hviral &lt;- blast_results_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_out &lt;- blast_results_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  inner_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_inspected &lt;- blast_results_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status &lt;- blast_results_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_1 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_2 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance &lt;- bind_rows(blast_performance_1, blast_performance_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance\n\n\n\n  \n\n\n\nUsing the inferred list of human-virus taxids results in high precision and specificity, but low sensitivity, missing many reads flagged as human-viral by manual inspection. Conversely, using a list of all viral taxids as the reference achieves perfect sensitivity and near-perfect precision and specificity, with only one false positive result, resulting in an F1 score of over 99%.\nThat one false positive turned out to be a sequence with a high-identity but low-query-coverage human-viral match (to human enterovirus 71) which was excluded during my previous manual assessment; in my opinion, it’s unclear whether this should really be classed as a false positive. If we want to class this sequence as non-human-viral, increasing the query coverage threshold from 30% to 35% successfully excludes it without losing any true positives, resulting in perfect precision relative to manual assignments:\n\nCode# Filter BLAST results by query coverage\nblast_results_qcov &lt;- blast_results_filtered %&gt;% filter(as.numeric(qcovs) &gt;= 35)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_qcov_paired &lt;- blast_results_qcov %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_qcov_hviral &lt;- blast_results_qcov_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_qcov_out &lt;- blast_results_qcov_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  full_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status_1 = replace_na(viral_status_1, \"FALSE\"),\n         viral_status_2 = replace_na(viral_status_2, \"FALSE\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_qcov_inspected &lt;- blast_results_qcov_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status_qcov &lt;- blast_results_qcov_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_qcov_1 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_qcov_2 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance_qcov &lt;- bind_rows(blast_performance_qcov_1, blast_performance_qcov_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance_qcov\n\n\n\n  \n\n\n\nIn summary, using command-line blastn -remote, combined with tabular processing in R against a reference class of all viral taxids, works well as a substitute for manual inspection of online BLAST results for validating human-viral read assignments. It’s too slow and failure-prone to be added to the pipeline as a replacement or supplement to the Bowtie/Kraken automated approach, but will be my go-to approach in future for manual validation and refinement of human-viral assignment criteria – at least until I’m able to try out Elastic-BLAST."
  },
  {
    "objectID": "notebooks/2024-02-04_crits-christoph-1.html",
    "href": "notebooks/2024-02-04_crits-christoph-1.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 1",
    "section": "",
    "text": "Since my last entry, I’ve continued to work on the new Nextflow MGS pipeline. Changes since the last entry include:\n\nSuccessful modification of the pipeline to run using working and output directories on S3 rather than locally, and several other QOL improvements (e.g. implementation of a Nextflow config file).\nVarious bug fixes.\nModification of the human-viral identification sub-pipeline to include singly or discordantly aligned read pairs.\nImplementation of a “trial run” mode for the main workflow in which the raw read files are automatically subset prior to analysis to speed up testing.\nCreation of a separate workflow for generating large index and reference files, so that these don’t need to be stored locally or generated during every run of the main workflow.\n\n\n\n\n\nIn order to validate pipeline performance on datasets other than our own BMC data, I’ve begun running it on pre-existing datasets from the P2RA project, starting with Crits-Christoph et al. (2021):\n\nThis was first and foremost a SARS-CoV-2 sequencing project that took place in the San Francisco Bay Area in 2020. Our data consists of 18 samples, all of which were collected as 24-hour composite samples of raw sewage collected from wastewater interceptor lines in four different locations (Berkeley, Oakland, San Francisco, and Marin).\nSamples were concentrated using three different methods (Amicon ultrafiltration, silica columns, or milk of silica); see paper methods for more details.\nSix samples (the “unenriched” fraction) underwent bacterial ribodepletion with RiboZero Plus, followed by untargeted metagenomic RNA-sequencing. The rest (the “enriched” fraction) underwent panel enrichment for respiratory viruses using the Illumina Respiratory Virus Oligo Panel, and no ribodepletion.\n\nThe raw data\nThe raw data obtained from SRA comprised 18 pairs of FASTQ files corresponding to 18 different samples. Unlike for the BMC data, there was no need to concatenate multiple FASTQ files for a given sample. What remained was roughly 298M read pairs of non-RVP-enriched data and 9M read pairs of RVP-enriched data, corresponding to roughly 45 Gb and 1 Gb of sequencing data, respectively. Reads were typically 70-80bp in length. Read qualities were high, adapter levels were relatively low, and FASTQC-measured duplication levels were moderate at 50-70%.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-04_crits-christoph-1/\"\nlibraries_path &lt;- file.path(data_dir, \"cc-libraries.txt\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,80),\n                     breaks=seq(0,80,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() + \n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(20,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,80),\n                     breaks=seq(0,80,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() + \n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nBoth RVP-enriched and unenriched samples showed significant, but not overwhelming, reductions in read counts during preprocessing. In the case of unenriched samples, most reads were lost during the deduplication stage, with low losses observed at other stages. In enriched samples, a much larger fraction of reads were lost during the ribodepletion stages; this is consistent with all of the unenriched samples having undergone ribodepletion, while the unenriched samples did not.\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 44% and 66% of total reads in unenriched and enriched samples, respectively. Host depletion, livestock depletion and secondary ribodepletion removed a further 2% and 5%, respectively.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot bases over preprocessing\ng_bases_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_bases_approx,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Enrichment\") +\n  scale_y_continuous(\"# Bases (approx)\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_bases_stages\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing had little effect on sequence quality, but this is unsurprising since raw sequence quality was already high.\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the former and latter respectively having the greatest effect in unenriched vs enriched samples. In both cases, the average detected duplication level after both processes was roughly 15%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% select(sample, location, method, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"method\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location, method=method,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (remove_other-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = (ribo_initial-remove_human) + eukaryota,\n                       n_other = remove_human-remove_other)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads)) %&gt;%\n  mutate(loc_met = paste(location,method,sep=\"_\"))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(loc_met, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=loc_met, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=loc_met, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nIn the unenriched samples, about 80% of reads are low-quality, duplicates, ribosomal, or unassigned, leaving about 20% of other assignable reads. Of these, most (16-20%) were bacterial. The fraction of human reads ranged from 0.6 to 1.5%; that of livestock reads from 0.04 to 0.1%; and that of viral reads from 0.2 to 0.5% – about 2 orders of magnitude higher than observed in our BMC data.\nAs expected, the panel-enriched samples show significantly higher levels of viral reads than the unenriched samples, between 0.5 and 6.4% of all reads. Conversely, with the exception of one sample with 33% bacterial reads, most enriched samples showed much lower bacterial read abundance (0.8 to 4.4%). Human reads are also moderately reduced (0 to 1%), though not as dramatically as bacteria.\nIn my next entry, I’ll look into human-infecting virus reads in the Crits-Christoph data, and the changes I needed to make to the pipeline described above to make this analysis go well."
  },
  {
    "objectID": "notebooks/2024-02-08_crits-christoph-2.html",
    "href": "notebooks/2024-02-08_crits-christoph-2.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 2",
    "section": "",
    "text": "In my last entry, I described the process of analyzing Crits-Christoph et al. (2021) data with my Nextflow workflow, up to and including high-level taxonomic analysis. In this entry, I’ll look more deeply at the human-infecting-virus content of these data, and address some inadequacies in that pipeline that were causing issues in analyzing those reads.\nTo begin with, I analyzed the human-infecting virus reads in Crits-Christoph in an identical manner to my previous analysis of our BMC data, making use of the automated BLAST characterization approach developed in a previous entry.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir, \"cc-libraries.txt\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nhv_reads_all_path &lt;- file.path(data_dir, \"hv_hits_putative_all.tsv.gz\")\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\n\n# Import data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nhv_reads_all &lt;- read_tsv(hv_reads_all_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_reads_filtered_1 &lt;- hv_reads_all %&gt;% \n  filter(!classified | assigned_hv)\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Count\nn_hv_reads_all &lt;- hv_reads_all %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"remove_other\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_1 &lt;- hv_reads_filtered_1 %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% rename(n_filtered_1 = n) %&gt;% inner_join(n_hv_reads_all, by=c(\"sample\", \"location\", \"method\", \"enrichment\")) %&gt;% mutate(p_reads = n_filtered_1/n_total, pc_reads = p_reads*100)\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"remove_other\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\nThe process for identifying human-viral (HV) reads was similar to that described in my last two entries:\n\nAfter removing human and livestock reads with BBMap, the remaining reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters. This retained roughly 0.3% to 0.4% of surviving reads (57k to 130k reads) for unenriched samples and 1% to 67% (650 to 25k reads) for enriched samples.\nReads that aligned successfully with Bowtie2 were run through Kraken2 (using the standard 16GB database) and reads that were assigned to any non-human-virus taxon were excluded. This retained roughly 0.17% to 0.29% of surviving reads (30k to 81k reads) for unenriched samples, and made little difference to the enriched samples.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 7042 read pairs across all unenriched samples (0.004% of surviving reads) and &gt;110,000 read pairs across all enriched samples (4% of surviving reads):\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(kraken_label~enrichment, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nI previously assessed potential criteria for designating a read as human-viral based on \\(S\\) and Kraken assignment status, and found that a disjunctive threshold of \\(S \\geq 20\\) (i.e. passing a read pair if either the forward or reverse read had an adjusted alignment score of at least 20) achieved the best performance. To test whether this remains the case across datasets, I BLASTed the 7042 putative HV reads from the unenriched data against nt using blastn -remote as described previously. I then assigned “ground-truth” viral status to each read pair as follows:\n\nLet a BLAST alignment to a sequence be described as “strong” if it’s in the joint top 5 highest-scoring BLAST alignments for that sequence and passes filters on % identity, query coverage, etc.\nIf both the forward and reverse reads from a read pair exhibit strong alignments to the same viral taxid, classify that read pair as human viral.\nIf either read in a read pair exhibits a strong alignment to the same viral taxid as that identified by Kraken/Bowtie, classify that read as human viral.\nOtherwise, if either read in a read pair exhibits a strong alignment to any viral taxid, classify the read as ambiguously viral and flag it for manual inspection.\nIf neither read aligns to a viral taxid, classify that read pair as non-human-viral.\n\n\nCode# Import files for viral taxid search\nhv_taxids_path &lt;- file.path(data_dir, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir, \"nodes.dmp.gz\")\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"cat-cc-unenriched.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\n\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  full_join(blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n# Import Bowtie2/Kraken data and perform filtering steps\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\ng_mrg_v &lt;- mrg_unenriched_plot %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v\n\n\n\n\nOff the bat, it’s clear this approach isn’t working well. There are almost 1,000 “UNCLEAR” data points that need manual inspection; worse, there appear to be numerous nonviral sequences achieving very high normalized Bowtie2 alignment scores:\n\nCodemrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\ng_hist &lt;- ggplot(mrg_unenriched_debug, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0) +\n  facet_wrap(~viral_status_out) +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") +\n  theme_base\ng_hist\n\n\n\n\nInspecting these high-scoring non-viral reads, we see that they are dominated by a few specific genome IDs: the top five genome IDs account for over 75% of high-scoring false-positives, and all of the top 10 overwhelmingly produce false rather than true positives.\n\nCodeheader_path &lt;- file.path(data_dir, \"human-viral-headers.txt\")\nheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_unenriched_debug, header_db, by=\"genome_id\")\n\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\nInspecting these top genome IDs more closely, we notice something interesting: many of them are transgenic. When we try BLASTing read pairs mapping to these transgenic sequences against nt with NCBI’s online tool, we find that the best matches are all to bacterial plasmids, cloning vectors and synthetic constructs. This suggests a clear culprit for a large fraction of the false positives we are seeing here: contamination of the Bowtie2 reference database with non-viral sequences inserted into transgenic viruses.\n\nCodemrg_unenriched_fasta &lt;- mrg_unenriched_debug %&gt;% filter(adj_score_max &gt;= 20, viral_status == 0) %&gt;% group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir, \"cc-bad-gid.fasta\"))\n\n\nExcluding “transgenic” and “mutant” genome IDs is predicted to remove 77% of the high-scoring false positives observed above. This left a small enough number of sequences to characterize manually using NCBI BLAST. When we do this, we see the following breakdown of causes:\n\nCode# Separate transgenic sequences\nbad_genomes_tr &lt;- bad_genomes %&gt;% \n  mutate(transgenic = grepl(\"transgenic\", genome_name, ignore.case=TRUE)|grepl(\"mutant\", genome_name, ignore.case=TRUE))\nbad_genomes_notr &lt;- bad_genomes_tr %&gt;% filter(!transgenic) %&gt;% ungroup %&gt;% mutate(pr_FALSE = n_FALSE/sum(n_FALSE))\n\n# Import manually annotated causes\nbg_causes &lt;- read_tsv(file.path(data_dir, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nbg_causes_out &lt;- bg_causes %&gt;% group_by(cause) %&gt;% summarize(n = sum(n_FALSE)) %&gt;% arrange(desc(n))\nbg_causes_out\n\n\n\n  \n\n\n\nBy far the most important attributable causes for misidentification of viral reads are:\n\nMapping to the human genome or human-derived synthetic constructs\nMapping to the E. coli genome\nMapping to cow or pig genomes\n\nThis suggests (1) that the current filtering to remove human and cow/pig sequences is insufficiently stringent, and (2) that adding E. coli and possibly one or more eukaryotic synthetic constructs to the database of contaminants to screen for could go a long way toward removing remaining false positives.\nFinally, it’s worth noting that a significant fraction of these sequences appear to be Bowtie true-positives that were falsely annotated as negatives by my automated BLAST analysis above. I’m not yet sure what’s underlying this, but it gives me some confidence that, if I can fix the issues outlined above, the general approach of this pipeline is still viable.\n\nCodemrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% full_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2"
  },
  {
    "objectID": "notebooks/2024-02-13_crits-christoph-3.html",
    "href": "notebooks/2024-02-13_crits-christoph-3.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 3",
    "section": "",
    "text": "In my last entry, I documented my first attempt at analyzing the human-infecting-virus content of sequence data from Crits-Christoph et al. (2021). In this entry, I’ll attempt to address the issues that arose in that first analysis and analyze the resulting updated data.\nAs a reminder, I previously found that the analysis pipeline I developed for our BMC data led to numerous high-scoring false-positives when run on Crits-Christoph data:\n\nCode# Data input paths\ndata_dir_old &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir_old, \"cc-libraries.txt\")\nhv_reads_filtered_path &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_taxids_path &lt;- file.path(data_dir_old, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir_old, \"nodes.dmp.gz\")\nblast_results_path &lt;- file.path(data_dir_old, \"cat-cc-unenriched.blast.gz\")\nbg_causes &lt;- read_tsv(file.path(data_dir_old, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nheader_path &lt;- file.path(data_dir_old, \"human-viral-headers.txt\")\n\n# Import data\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Process Bowtie/Kraken data\nmrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n\n# Compare BLAST results to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid, seq_id)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num, seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  left_join(blast_results_out, by=c(\"sample\", \"seq_num\", \"seq_id\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\nmrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% left_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2\n\n\n\n\nIn total, 992 non-viral read pairs and 83 read pairs of unclear status achieved high length-adjusted alignment scores (\\(\\geq 20\\)) when mapped to viral Genbank with Bowtie2. For now, I will focus on the non-viral read pairs, and return to the unclear read pairs after these are in better shape.\nPass 1: excluding transgenic and bacterial sequences\nThe first major change I made to the pipeline was to curate the reference database for the Bowtie2 alignment to remove transgenic and other inappropriate viral sequences. To do this, I subset the collated FASTA file of viral genomes with seqtk to remove any sequence with “transgenic”, “mutant”, “recombinant”, “unverified” or “draft” in its sequence ID. This removed a total of 493 genomes from the reference DB, leaving a total of 41942 remaining for alignment with Bowtie2.\nAt the same time, I also added an E. coli genome to the set of contaminant genomes to screen against, joining cow, pig and human. I also moved the screening step to be downstream rather than upstream of the Bowtie2 filtering step, to save computation time and make it quicker to make further modifications downstream if needed. For now, I didn’t make any changes to the BBMap parameters for screening for contaminant genomes.\nAfter passing putative viral read pairs through the same filtering steps as last time, I got the following result:\n\nCode# Import and process new HV data\ndata_dir_new &lt;- \"../data/2024-02-13_crits-christoph-3/\"\nhv_reads_new_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_new_unenriched &lt;- filter(hv_reads_new, enrichment == \"Unenriched\")\nmrg_old_join &lt;- mrg_unenriched_plot_2 %&gt;% ungroup %&gt;% \n              select(seq_id, cause, viral_status, viral_status_out, genome_id_old=genome_id, taxid_old=taxid)\nmrg_new &lt;- hv_reads_new_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_new &lt;- mrg_new %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_new\n\n\n\n\n\nCodemrg_hist_new &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1))) \ng_hist_new &lt;- ggplot(mrg_hist_new, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_new\n\n\n\n\nThis is a clear improvement on the last round: the number of high-scoring non-viral sequences has fallen from 992 to 549, and the number of high-scoring unclear sequences from 83 to 56. The degree of improvement, however, is less than I’d hoped. Looking into the new alignments, this appears to be because many of the sequences mapping to the old transgenic sequences now map to non-transgenic (or at least, not labeled to be transgenic) strains of the same viruses. When BLASTed against nt, however, these sequences still map primarily to synthetic cloning vectors, suggesting these viral genomes may still be genetically modified despite not being labeled as such.\n\nCodeheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_new, header_db, by=\"genome_id\")\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta &lt;- mrg_new %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir_new, \"cc-bad-gid.fasta\"))\nbad_genomes_out &lt;- bad_genomes %&gt;%\n  left_join(mrg_new %&gt;% filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;%\n              group_by(genome_id, cause) %&gt;% summarize(.groups=\"drop\"), \n            by=\"genome_id\")\n\n\nPass 2: Additional “contaminant” screening\nIn my second attempt, I excluded further viral genomes (those with “recombinant” in the genome name) from the Bowtie database, and more importantly added several new sequences to the set of “contaminant” genomes to screen for during viral read identification. Specifically, I added one eukaryotic synthetic construct chromosome, three synthetic cloning vectors, and the Klebsiella pneumoniae genome, all of which came up during pass 1 as matches to false-positive viral sequences. Re-running the analysis with these changes, we see a large improvement:\n\nCode# Import and process new HV data\nhv_reads_newer_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_newer &lt;- read_tsv(hv_reads_newer_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newer_unenriched &lt;- filter(hv_reads_newer, enrichment == \"Unenriched\")\nmrg_newer &lt;- hv_reads_newer_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newer &lt;- mrg_newer %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newer\n\n\n\n\n\nCodemrg_hist_newer &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), mrg_newer %&gt;% mutate(attempt=2)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1,2))) \ng_hist_newer &lt;- ggplot(mrg_hist_newer, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_newer\n\n\n\n\n\nCodecounts_newer &lt;- mrg_hist_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newer) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newer\n\n\n\n  \n\n\n\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_old &lt;- range_f1(mrg_unenriched_plot_2, inc_special) %&gt;% mutate(attempt=0)\nstats_new &lt;- range_f1(mrg_new, inc_special) %&gt;% mutate(attempt=1)\nstats_newer &lt;- range_f1(mrg_newer, inc_special) %&gt;% mutate(attempt=2)\nstats_all &lt;- bind_rows(stats_old, stats_new, stats_newer) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_2 &lt;- ggplot(stats_all %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_2\n\n\n\n\nThe number of high-scoring false positives has been cut by over 400 (almost 75% compared to pass 1) and the optimal F1 score (excluding unclear read pairs for now) increased from 0.938 to 0.967. However, over 100 apparent false positives still remain, still primarily arising from cloning vectors and cow and pig sequences:\n\nCodemrg_ug_newer &lt;- left_join(mrg_newer, header_db, by=\"genome_id\")\nbad_genomes_newer &lt;- mrg_ug_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newer_caused &lt;- left_join(bad_genomes_newer, bg_causes %&gt;% select(genome_id, cause),\n                                      by=\"genome_id\")\nbad_genomes_newer_caused\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta_2 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_2 &lt;- do.call(paste, c(mrg_unenriched_fasta_2, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_2, file.path(data_dir_new, \"cc-bad-gid-2.fasta\"))\n\n\nPass 3: Unmasking “contaminant” genomes\nIn my previous attempts at this problem, I mapped reads against “contaminant” genomes having first masked the latter to remove repetitive and low-entropy sequences. This makes sense in many contexts, but may not make sense here: in particular, if the repeat sequences being masked include virus-like transposable elements, this may be responsible for the failure of my current contaminant screening approach to detect and remove some of the non-viral (especially mammalian) sequences being mistaken for viruses by Bowtie2.\nIn addition to adding a further cloning vector sequence to the contaminant sequence database for this third pass, therefore, I also tried tried re-running my analysis pipeline against an unmasked version of this database. The results looked like this:\n\nCode# Import and process new HV data\nhv_reads_newest_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_3.tsv.gz\")\nhv_reads_newest &lt;- read_tsv(hv_reads_newest_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newest_unenriched &lt;- filter(hv_reads_newest, enrichment == \"Unenriched\")\nmrg_newest &lt;- hv_reads_newest_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newest &lt;- mrg_newest %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newest\n\n\n\n\n\nCodemrg_hist_newest &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), \n                             mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), \n                             mrg_newer %&gt;% mutate(attempt=2),\n                             mrg_newest %&gt;% mutate(attempt=3)) %&gt;% \n  mutate(attempt = factor(attempt, levels=c(0,1,2,3))) \ng_hist_newest &lt;- ggplot(mrg_hist_newest, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_newest\n\n\n\n\n\nCodecounts_newest &lt;- mrg_hist_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newest) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newest\n\n\n\n  \n\n\n\n\nCodestats_newest &lt;- range_f1(mrg_newest, inc_special) %&gt;% mutate(attempt=3)\nstats_all_3 &lt;- bind_rows(stats_old, stats_new, stats_newer, stats_newest) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt_3 &lt;- stats_all_3 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_3 &lt;- ggplot(stats_all_3 %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_3\n\n\n\n\nThe number of high-scoring false positives has been cut by another 98, down to just 40, and the optimal F1 score (again excluding unclear read pairs for now) increased from to 0.98. Looking at the remaining false positives, it looks like this is entirely due to more successful removal of remaining cloning vector sequences; unmasking the cow and pig genomes seems to have had little effect:\n\nCodemrg_ug_newest &lt;- left_join(mrg_newest, header_db, by=\"genome_id\")\nbad_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newest_caused &lt;- left_join(bad_genomes_newest,\n                                       bg_causes %&gt;% select(genome_id, cause),\n                                       by=\"genome_id\")\nbad_genomes_newest_caused\n\n\n\n  \n\n\n\nWhile I would like to do better at removing these residual sequences, I think the F1 scores I’m getting are now high enough to consider this acceptable performance.\nTurning now to the “unclear” sequences, we see that, unlike the false-positive sequences, these are not concentrated in a few specific culprits, but rather spread fairly evenly over numerous viruses (44 sequences across 34 genome IDs). Inspecting these manually with NCBI BLAST, we find that most, but not all, of them appear to be real matches:\n\nCodeunclear_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_UNCLEAR &gt; 0) %&gt;% arrange(desc(n_UNCLEAR)) %&gt;%\n  select(genome_id, genome_name, n_UNCLEAR, n_FALSE, n_TRUE) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_FALSE = replace_na(n_FALSE, 0)) %&gt;%\n  mutate(p_UNCLEAR = n_UNCLEAR/(n_FALSE+n_TRUE+n_UNCLEAR))\nunclear_genomes_newest\n\n\n\n  \n\n\nCodewrite_tsv(unclear_genomes_newest, file.path(data_dir_new, \"gid-unclear-3-raw.tsv\"))\n\nmrg_unenriched_fasta_3 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_3 &lt;- do.call(paste, c(mrg_unenriched_fasta_3, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_3, file.path(data_dir_new, \"cc-unclear-gid-3.fasta\"))\n\n\n\nCodeunclear_genomes_caused &lt;- read_tsv(file.path(data_dir_new, \"gid-unclear-3.tsv\"), show_col_types = FALSE)\nmrg_newest_unclear &lt;- mrg_ug_newest %&gt;% select(-cause) %&gt;%\n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;%\n  left_join(unclear_genomes_caused %&gt;% select(genome_id, cause), by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, \n                               ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n    mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_newest_unclear %&gt;% group_by(cause) %&gt;% count\n\n\n\n  \n\n\n\n\nCodeg_unclear &lt;- mrg_newest_unclear %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nThe false matches don’t appear particularly differentiated from the true matches by alignment score, either using Bowtie2 (above) or BLAST (below):\n\nCodeblast_results_seqid &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  mutate(seq_num = as.integer(seq_num), read_pair = as.integer(read_pair), sample=fct_inorder(sample)) %&gt;%\n  left_join(mrg %&gt;% select(sample, seq_num, seq_id, taxid_bowtie = taxid), by = c(\"sample\", \"seq_num\")) \nblast_results_unclear &lt;- blast_results_seqid %&gt;%\n  inner_join(mrg_newest_unclear %&gt;% select(seq_id, viral_status, viral_status_out, cause), by = \"seq_id\") %&gt;%\n  filter(viral)\n# First check if any BLAST taxid is a descendent of the corresponding Bowtie taxid, or vice versa\ntaxid_dec_bowtie &lt;- blast_results_unclear %&gt;% pull(taxid_bowtie) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$taxid_bowtie))\ntaxid_dec_blast &lt;- blast_results_unclear %&gt;% pull(staxid) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$staxid))\nmatch_1 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$staxid[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$taxid_bowtie[n])]])\nmatch_2 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$taxid_bowtie[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$staxid[n])]])\nmatch_any &lt;- match_1 | match_2\nblast_results_matched &lt;- blast_results_unclear %&gt;% mutate(taxid_match = match_any)\n\n\n\nCode# Otherwise, take the highest-scoring, longest viral match\nblast_results_single &lt;- blast_results_matched %&gt;% group_by(sample, seq_num, read_pair) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), length = as.numeric(length)) %&gt;%\n  filter(taxid_match == max(taxid_match)) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nbrs_out &lt;- blast_results_single %&gt;% select(sample, seq_num, bitscore, viral_status_out) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"seq_num\", \"viral_status_out\"), names_from = \"read_pair\", \n              values_from = \"bitscore\", names_prefix = \"read_\") %&gt;%\n  mutate(read_1 = replace_na(read_1, 0), read_2 = replace_na(read_2, 0))\ng_brs &lt;- ggplot(brs_out, aes(x=read_1, y=read_2, color = viral_status_out)) + \n    geom_point(alpha=0.5, shape=16, size=3) + \n  scale_x_continuous(name=\"Best viral bitscore (forward read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_y_continuous(name=\"Best viral bitscore (reverse read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_brs\n\n\n\n\nOverall, I think pulling these out for manual inspection is currently the right call. But if we’re forced to classify them automatically, counting them as true viral matches is probably the better bet.\nHuman viral reads in Crits-Christoph (2021): Final assessment\nNow that I’ve settled on a pipeline and downstream analysis process I’m happy with, we can return to the question of overall human-viral abundance and composition in Crits-Christoph (2021). I’ll use a Bowtie2 alignment score cutoff of 20 here, as this is consistent with previous studies and gives good F1 scores in the tests above.\nUsing this cutoff, we find a total of 109868/8716917 human-viral reads in panel-enriched samples (\\(1.26 \\times 10^{-2}\\) , about 1 in 80), and 4064/297690777 in unenriched samples (\\(1.37 \\times 10^{-5}\\) , about 1 in 73,000). Unsurprisingly, enriched samples show much higher overall human-viral abundance than unenriched samples; however, even unenriched samples show much higher relative abundance than our previous BMC sludge sequences ( \\(\\sim 3 \\times 10^{-7}\\)):\n\nCode# Get raw read counts\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE)\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% select(sample, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nhv_reads_newest_cut &lt;- hv_reads_newest %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_newest_counts &lt;- hv_reads_newest_cut %&gt;% group_by(sample, enrichment) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_newest_counts, read_counts_raw, by=\"sample\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\ng_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, color=enrichment)) + geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  facet_wrap(~enrichment, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nIn comparison, the old pipeline returns an estimated overall relative abundance of human-infecting viruses in unenriched samples of roughly \\(3.1 \\times 10^{-6}\\), nearly 5 times lower.\nDigging into individual viruses, we see large but inconsistent differences in relative abundance between enriched and unenriched samples:\n\nCode# Import viral genera\nviral_taxids_path &lt;- file.path(data_dir_new, \"viral-taxids.tsv\")\nviral_taxids &lt;- read_tsv(viral_taxids_path, show_col_types = FALSE)\nviral_genera &lt;- viral_taxids %&gt;% filter(rank == \"genus\")\n\n# Get unique name for each viral genus\nviral_genera_unique &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() == 1)\nviral_genera_duplicate &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() &gt; 1)\nviral_genera_valid_1 &lt;- viral_genera_duplicate %&gt;% filter(grepl(\"virus$\", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_1 %&gt;% filter(n() == 1))\nviral_genera_valid_2 &lt;- viral_genera_valid_1 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(!grepl(\" \", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_2 %&gt;% filter(n() == 1))\nviral_genera_valid_3 &lt;- viral_genera_valid_2 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_3 %&gt;% filter(n() == 1))\nviral_genera_valid_4 &lt;- viral_genera_duplicate %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_4 %&gt;% filter(n() == 1))\nwrite_tsv(viral_genera_unique, file.path(data_dir_new, \"viral-genera-unique.tsv\"))\n\n# Discover viral genera for HV reads\nhigh_ranks &lt;- c(\"class\", \"family\", \"kingdom\", \"order\", \"phylum\", \"subfamily\", \"suborder\", \"subphylum\", \"superkingdom\")\nhv_read_db &lt;- hv_reads_newest_cut\ntax_nodes_cut &lt;- rename(tax_nodes, taxid = child_taxid) %&gt;% filter(taxid %in% v_taxids)\nhv_read_genus &lt;- hv_read_db %&gt;% inner_join(viral_genera_unique, by=\"taxid\")\nhv_read_nogenus &lt;- hv_read_db %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;%\n  inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n  filter(!is.na(taxid), !rank %in% high_ranks)\n#cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\nwhile(nrow(hv_read_nogenus) &gt; 0){\n  hv_read_genus &lt;- bind_rows(hv_read_genus, hv_read_nogenus %&gt;% inner_join(viral_genera_unique, by=\"taxid\"))\n  hv_read_nogenus &lt;- hv_read_nogenus %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;% select(-rank, -parent_taxid) %&gt;%\n    inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n    filter(!is.na(taxid), !rank %in% high_ranks)\n  #cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\n}\n\n# Get taxon names for higher-ranked assignments\nsmatch &lt;- hv_read_db$seq_id %in% hv_read_genus$seq_id\nhv_read_highrank &lt;- hv_read_db[!smatch,] %&gt;% inner_join(viral_taxids %&gt;% group_by(taxid) %&gt;% filter(row_number() == 1), by = \"taxid\")\n\n# Count viral genera (& unassigned viruses)\nhv_counts_wide &lt;- bind_rows(hv_read_genus, hv_read_highrank) %&gt;% group_by(name, enrichment) %&gt;% count %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"n\", values_fill = 0) \nhv_counts &lt;- hv_counts_wide %&gt;%\n  pivot_longer(-name, names_to = \"enrichment\", values_to = \"n_reads_virus\")\n\n\n\nCodehv_counts_fraction &lt;- hv_counts %&gt;%\n  inner_join(hv_reads_total %&gt;% select(enrichment, n_reads_hv, n_reads_raw), by=\"enrichment\") %&gt;%\n  mutate(p_reads_virus_all = n_reads_virus/n_reads_raw, p_reads_virus_hv = n_reads_virus/n_reads_hv)\ng_hv_counts &lt;- ggplot(hv_counts_fraction, aes(x=name, y=p_reads_virus_all, color=enrichment)) +\n  geom_point(shape=16) +\n  scale_y_log10(name = \"Relative abundance\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1), aspect.ratio = 1/5)\ng_hv_counts\n\n\n\n\nAs expected, viruses included in the respiratory virus panel see large increases in relative abundance in the enriched vs the unenriched samples, with the largest relative increases seen for Bocaparvovirus (Human bocavirus 1, 2c, 3), Betacoronavirus (SARS-CoV-2, OC43, HKU1), and Mastadenovirus (Human adenovirus B1, C2, E4). Confusingly, Orthopoxvirus, Cytomegalovirus and Gemygorvirus all also show substantially increased relative abundance, even though as far as I can tell there are no viruses from those genera in the Illumina panel. Numerous other viruses show weaker enrichment; even norovirus shows moderate (~4x) enrichment in the enriched vs the unenriched samples.\nConversely, a number of viruses are found in the unenriched samples that are absent in the enriched samples, including Rotavirus, Flavivirus, Parvovirus, and various papillomaviruses and polyomaviruses. One natural hypothesis for this is that these viruses were excluded by the enrichment panel and so had their relative abundance reduced; however, the enrichment observed for various other not-in-panel viruses calls this into question. An alternative hypothesis is that this difference is simply a consequence of the much deeper sequencing conducted on the unenriched samples (geometric mean of ~340k read pairs per enriched sample vs 42M read pairs per unenriched sample).\n\nCodehv_ra_wide &lt;- hv_counts_fraction %&gt;% select(name, enrichment, p_reads_virus_all) %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"p_reads_virus_all\", values_fill = 0) %&gt;%\n  rename(ra_enriched = Enriched, ra_unenriched = Unenriched) %&gt;%\n  mutate(relative_enrichment = log10(ra_enriched/ra_unenriched)) %&gt;%\n  arrange(desc(relative_enrichment), desc(ra_enriched), ra_unenriched)\nhv_ra_wide\n\n\n\n  \n\n\n\nAt this point, I’m satisfied with my workflow’s ability to produce usable results on the Crits-Christoph data. Next, I’ll apply this updated workflow to another previously-published WMGS dataset, likely Rothman et al. (2021)."
  },
  {
    "objectID": "notebooks/2024-02-15_crits-christoph-3.html",
    "href": "notebooks/2024-02-15_crits-christoph-3.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 3",
    "section": "",
    "text": "In my last entry, I documented my first attempt at analyzing the human-infecting-virus content of sequence data from Crits-Christoph et al. (2021). In this entry, I’ll attempt to address the issues that arose in that first analysis and analyze the resulting updated data.\nAs a reminder, I previously found that the analysis pipeline I developed for our BMC data led to numerous high-scoring false-positives when run on Crits-Christoph data:\n\nCode# Data input paths\ndata_dir_old &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir_old, \"cc-libraries.txt\")\nhv_reads_filtered_path &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_taxids_path &lt;- file.path(data_dir_old, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir_old, \"nodes.dmp.gz\")\nblast_results_path &lt;- file.path(data_dir_old, \"cat-cc-unenriched.blast.gz\")\nbg_causes &lt;- read_tsv(file.path(data_dir_old, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nheader_path &lt;- file.path(data_dir_old, \"human-viral-headers.txt\")\n\n# Import data\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Process Bowtie/Kraken data\nmrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n\n# Compare BLAST results to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid, seq_id)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num, seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  left_join(blast_results_out, by=c(\"sample\", \"seq_num\", \"seq_id\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\nmrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% left_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2\n\n\n\n\nIn total, 992 non-viral read pairs and 83 read pairs of unclear status achieved high length-adjusted alignment scores (\\(\\geq 20\\)) when mapped to viral Genbank with Bowtie2. For now, I will focus on the non-viral read pairs, and return to the unclear read pairs after these are in better shape.\nPass 1: excluding transgenic and bacterial sequences\nThe first major change I made to the pipeline was to curate the reference database for the Bowtie2 alignment to remove transgenic and other inappropriate viral sequences. To do this, I subset the collated FASTA file of viral genomes with seqtk to remove any sequence with “transgenic”, “mutant”, “recombinant”, “unverified” or “draft” in its sequence ID. This removed a total of 493 genomes from the reference DB, leaving a total of 41942 remaining for alignment with Bowtie2.\nAt the same time, I also added an E. coli genome to the set of contaminant genomes to screen against, joining cow, pig and human. I also moved the screening step to be downstream rather than upstream of the Bowtie2 filtering step, to save computation time and make it quicker to make further modifications downstream if needed. For now, I didn’t make any changes to the BBMap parameters for screening for contaminant genomes.\nAfter passing putative viral read pairs through the same filtering steps as last time, I got the following result:\n\nCode# Import and process new HV data\ndata_dir_new &lt;- \"../data/2024-02-15_crits-christoph-3/\"\nhv_reads_new_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_new_unenriched &lt;- filter(hv_reads_new, enrichment == \"Unenriched\")\nmrg_old_join &lt;- mrg_unenriched_plot_2 %&gt;% ungroup %&gt;% \n              select(seq_id, cause, viral_status, viral_status_out, genome_id_old=genome_id, taxid_old=taxid)\nmrg_new &lt;- hv_reads_new_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_new &lt;- mrg_new %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_new\n\n\n\n\n\nCodemrg_hist_new &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1))) \ng_hist_new &lt;- ggplot(mrg_hist_new, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_new\n\n\n\n\nThis is a clear improvement on the last round: the number of high-scoring non-viral sequences has fallen from 992 to 549, and the number of high-scoring unclear sequences from 83 to 56. The degree of improvement, however, is less than I’d hoped. Looking into the new alignments, this appears to be because many of the sequences mapping to the old transgenic sequences now map to non-transgenic (or at least, not labeled to be transgenic) strains of the same viruses. When BLASTed against nt, however, these sequences still map primarily to synthetic cloning vectors, suggesting these viral genomes may still be genetically modified despite not being labeled as such.\n\nCodeheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_new, header_db, by=\"genome_id\")\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta &lt;- mrg_new %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir_new, \"cc-bad-gid.fasta\"))\nbad_genomes_out &lt;- bad_genomes %&gt;%\n  left_join(mrg_new %&gt;% filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;%\n              group_by(genome_id, cause) %&gt;% summarize(.groups=\"drop\"), \n            by=\"genome_id\")\n\n\nPass 2: Additional “contaminant” screening\nIn my second attempt, I excluded further viral genomes (those with “recombinant” in the genome name) from the Bowtie database, and more importantly added several new sequences to the set of “contaminant” genomes to screen for during viral read identification. Specifically, I added one eukaryotic synthetic construct chromosome, three synthetic cloning vectors, and the Klebsiella pneumoniae genome, all of which came up during pass 1 as matches to false-positive viral sequences. Re-running the analysis with these changes, we see a large improvement:\n\nCode# Import and process new HV data\nhv_reads_newer_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_newer &lt;- read_tsv(hv_reads_newer_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newer_unenriched &lt;- filter(hv_reads_newer, enrichment == \"Unenriched\")\nmrg_newer &lt;- hv_reads_newer_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newer &lt;- mrg_newer %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newer\n\n\n\n\n\nCodemrg_hist_newer &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), mrg_newer %&gt;% mutate(attempt=2)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1,2))) \ng_hist_newer &lt;- ggplot(mrg_hist_newer, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_newer\n\n\n\n\n\nCodecounts_newer &lt;- mrg_hist_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newer) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newer\n\n\n\n  \n\n\n\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_old &lt;- range_f1(mrg_unenriched_plot_2, inc_special) %&gt;% mutate(attempt=0)\nstats_new &lt;- range_f1(mrg_new, inc_special) %&gt;% mutate(attempt=1)\nstats_newer &lt;- range_f1(mrg_newer, inc_special) %&gt;% mutate(attempt=2)\nstats_all &lt;- bind_rows(stats_old, stats_new, stats_newer) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_2 &lt;- ggplot(stats_all %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_2\n\n\n\n\nThe number of high-scoring false positives has been cut by over 400 (almost 75% compared to pass 1) and the optimal F1 score (excluding unclear read pairs for now) increased from 0.938 to 0.967. However, over 100 apparent false positives still remain, still primarily arising from cloning vectors and cow and pig sequences:\n\nCodemrg_ug_newer &lt;- left_join(mrg_newer, header_db, by=\"genome_id\")\nbad_genomes_newer &lt;- mrg_ug_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newer_caused &lt;- left_join(bad_genomes_newer, bg_causes %&gt;% select(genome_id, cause),\n                                      by=\"genome_id\")\nbad_genomes_newer_caused\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta_2 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_2 &lt;- do.call(paste, c(mrg_unenriched_fasta_2, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_2, file.path(data_dir_new, \"cc-bad-gid-2.fasta\"))\n\n\nPass 3: Unmasking “contaminant” genomes\nIn my previous attempts at this problem, I mapped reads against “contaminant” genomes having first masked the latter to remove repetitive and low-entropy sequences. This makes sense in many contexts, but may not make sense here: in particular, if the repeat sequences being masked include virus-like transposable elements, this may be responsible for the failure of my current contaminant screening approach to detect and remove some of the non-viral (especially mammalian) sequences being mistaken for viruses by Bowtie2.\nIn addition to adding a further cloning vector sequence to the contaminant sequence database for this third pass, therefore, I also tried tried re-running my analysis pipeline against an unmasked version of this database. The results looked like this:\n\nCode# Import and process new HV data\nhv_reads_newest_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_3.tsv.gz\")\nhv_reads_newest &lt;- read_tsv(hv_reads_newest_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newest_unenriched &lt;- filter(hv_reads_newest, enrichment == \"Unenriched\")\nmrg_newest &lt;- hv_reads_newest_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newest &lt;- mrg_newest %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newest\n\n\n\n\n\nCodemrg_hist_newest &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), \n                             mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), \n                             mrg_newer %&gt;% mutate(attempt=2),\n                             mrg_newest %&gt;% mutate(attempt=3)) %&gt;% \n  mutate(attempt = factor(attempt, levels=c(0,1,2,3))) \ng_hist_newest &lt;- ggplot(mrg_hist_newest, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_newest\n\n\n\n\n\nCodecounts_newest &lt;- mrg_hist_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newest) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newest\n\n\n\n  \n\n\n\n\nCodestats_newest &lt;- range_f1(mrg_newest, inc_special) %&gt;% mutate(attempt=3)\nstats_all_3 &lt;- bind_rows(stats_old, stats_new, stats_newer, stats_newest) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt_3 &lt;- stats_all_3 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_3 &lt;- ggplot(stats_all_3 %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_3\n\n\n\n\nThe number of high-scoring false positives has been cut by another 98, down to just 40, and the optimal F1 score (again excluding unclear read pairs for now) increased from to 0.98. Looking at the remaining false positives, it looks like this is entirely due to more successful removal of remaining cloning vector sequences; unmasking the cow and pig genomes seems to have had little effect:\n\nCodemrg_ug_newest &lt;- left_join(mrg_newest, header_db, by=\"genome_id\")\nbad_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newest_caused &lt;- left_join(bad_genomes_newest,\n                                       bg_causes %&gt;% select(genome_id, cause),\n                                       by=\"genome_id\")\nbad_genomes_newest_caused\n\n\n\n  \n\n\n\nWhile I would like to do better at removing these residual sequences, I think the F1 scores I’m getting are now high enough to consider this acceptable performance.\nTurning now to the “unclear” sequences, we see that, unlike the false-positive sequences, these are not concentrated in a few specific culprits, but rather spread fairly evenly over numerous viruses (44 sequences across 34 genome IDs). Inspecting these manually with NCBI BLAST, we find that most, but not all, of them appear to be real matches:\n\nCodeunclear_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_UNCLEAR &gt; 0) %&gt;% arrange(desc(n_UNCLEAR)) %&gt;%\n  select(genome_id, genome_name, n_UNCLEAR, n_FALSE, n_TRUE) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_FALSE = replace_na(n_FALSE, 0)) %&gt;%\n  mutate(p_UNCLEAR = n_UNCLEAR/(n_FALSE+n_TRUE+n_UNCLEAR))\nunclear_genomes_newest\n\n\n\n  \n\n\nCodewrite_tsv(unclear_genomes_newest, file.path(data_dir_new, \"gid-unclear-3-raw.tsv\"))\n\nmrg_unenriched_fasta_3 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_3 &lt;- do.call(paste, c(mrg_unenriched_fasta_3, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_3, file.path(data_dir_new, \"cc-unclear-gid-3.fasta\"))\n\n\n\nCodeunclear_genomes_caused &lt;- read_tsv(file.path(data_dir_new, \"gid-unclear-3.tsv\"), show_col_types = FALSE)\nmrg_newest_unclear &lt;- mrg_ug_newest %&gt;% select(-cause) %&gt;%\n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;%\n  left_join(unclear_genomes_caused %&gt;% select(genome_id, cause), by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, \n                               ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n    mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_newest_unclear %&gt;% group_by(cause) %&gt;% count\n\n\n\n  \n\n\n\n\nCodeg_unclear &lt;- mrg_newest_unclear %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nThe false matches don’t appear particularly differentiated from the true matches by alignment score, either using Bowtie2 (above) or BLAST (below):\n\nCodeblast_results_seqid &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  mutate(seq_num = as.integer(seq_num), read_pair = as.integer(read_pair), sample=fct_inorder(sample)) %&gt;%\n  left_join(mrg %&gt;% select(sample, seq_num, seq_id, taxid_bowtie = taxid), by = c(\"sample\", \"seq_num\")) \nblast_results_unclear &lt;- blast_results_seqid %&gt;%\n  inner_join(mrg_newest_unclear %&gt;% select(seq_id, viral_status, viral_status_out, cause), by = \"seq_id\") %&gt;%\n  filter(viral)\n# First check if any BLAST taxid is a descendent of the corresponding Bowtie taxid, or vice versa\ntaxid_dec_bowtie &lt;- blast_results_unclear %&gt;% pull(taxid_bowtie) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$taxid_bowtie))\ntaxid_dec_blast &lt;- blast_results_unclear %&gt;% pull(staxid) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$staxid))\nmatch_1 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$staxid[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$taxid_bowtie[n])]])\nmatch_2 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$taxid_bowtie[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$staxid[n])]])\nmatch_any &lt;- match_1 | match_2\nblast_results_matched &lt;- blast_results_unclear %&gt;% mutate(taxid_match = match_any)\n\n\n\nCode# Otherwise, take the highest-scoring, longest viral match\nblast_results_single &lt;- blast_results_matched %&gt;% group_by(sample, seq_num, read_pair) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), length = as.numeric(length)) %&gt;%\n  filter(taxid_match == max(taxid_match)) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nbrs_out &lt;- blast_results_single %&gt;% select(sample, seq_num, bitscore, viral_status_out) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"seq_num\", \"viral_status_out\"), names_from = \"read_pair\", \n              values_from = \"bitscore\", names_prefix = \"read_\") %&gt;%\n  mutate(read_1 = replace_na(read_1, 0), read_2 = replace_na(read_2, 0))\ng_brs &lt;- ggplot(brs_out, aes(x=read_1, y=read_2, color = viral_status_out)) + \n    geom_point(alpha=0.5, shape=16, size=3) + \n  scale_x_continuous(name=\"Best viral bitscore (forward read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_y_continuous(name=\"Best viral bitscore (reverse read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_brs\n\n\n\n\nOverall, I think pulling these out for manual inspection is currently the right call. But if we’re forced to classify them automatically, counting them as true viral matches is probably the better bet.\nHuman viral reads in Crits-Christoph (2021): Final assessment\nNow that I’ve settled on a pipeline and downstream analysis process I’m happy with, we can return to the question of overall human-viral abundance and composition in Crits-Christoph (2021). I’ll use a Bowtie2 alignment score cutoff of 20 here, as this is consistent with previous studies and gives good F1 scores in the tests above.\nUsing this cutoff, we find a total of 109868/8716917 human-viral reads in panel-enriched samples (\\(1.26 \\times 10^{-2}\\) , about 1 in 80), and 4064/297690777 in unenriched samples (\\(1.37 \\times 10^{-5}\\) , about 1 in 73,000). Unsurprisingly, enriched samples show much higher overall human-viral abundance than unenriched samples; however, even unenriched samples show much higher relative abundance than our previous BMC sludge sequences ( \\(\\sim 3 \\times 10^{-7}\\)):\n\nCode# Get raw read counts\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE)\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% select(sample, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nhv_reads_newest_cut &lt;- hv_reads_newest %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_newest_counts &lt;- hv_reads_newest_cut %&gt;% group_by(sample, enrichment) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_newest_counts, read_counts_raw, by=\"sample\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\ng_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, color=enrichment)) + geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  facet_wrap(~enrichment, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nIn comparison, the old pipeline returns an estimated overall relative abundance of human-infecting viruses in unenriched samples of roughly \\(3.1 \\times 10^{-6}\\), nearly 5 times lower.\nDigging into individual viruses, we see large but inconsistent differences in relative abundance between enriched and unenriched samples:\n\nCode# Import viral genera\nviral_taxids_path &lt;- file.path(data_dir_new, \"viral-taxids.tsv\")\nviral_taxids &lt;- read_tsv(viral_taxids_path, show_col_types = FALSE)\nviral_genera &lt;- viral_taxids %&gt;% filter(rank == \"genus\")\n\n# Get unique name for each viral genus\nviral_genera_unique &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() == 1)\nviral_genera_duplicate &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() &gt; 1)\nviral_genera_valid_1 &lt;- viral_genera_duplicate %&gt;% filter(grepl(\"virus$\", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_1 %&gt;% filter(n() == 1))\nviral_genera_valid_2 &lt;- viral_genera_valid_1 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(!grepl(\" \", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_2 %&gt;% filter(n() == 1))\nviral_genera_valid_3 &lt;- viral_genera_valid_2 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_3 %&gt;% filter(n() == 1))\nviral_genera_valid_4 &lt;- viral_genera_duplicate %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_4 %&gt;% filter(n() == 1))\nwrite_tsv(viral_genera_unique, file.path(data_dir_new, \"viral-genera-unique.tsv\"))\n\n# Discover viral genera for HV reads\nhigh_ranks &lt;- c(\"class\", \"family\", \"kingdom\", \"order\", \"phylum\", \"subfamily\", \"suborder\", \"subphylum\", \"superkingdom\")\nhv_read_db &lt;- hv_reads_newest_cut\ntax_nodes_cut &lt;- rename(tax_nodes, taxid = child_taxid) %&gt;% filter(taxid %in% v_taxids)\nhv_read_genus &lt;- hv_read_db %&gt;% inner_join(viral_genera_unique, by=\"taxid\")\nhv_read_nogenus &lt;- hv_read_db %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;%\n  inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n  filter(!is.na(taxid), !rank %in% high_ranks)\n#cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\nwhile(nrow(hv_read_nogenus) &gt; 0){\n  hv_read_genus &lt;- bind_rows(hv_read_genus, hv_read_nogenus %&gt;% inner_join(viral_genera_unique, by=\"taxid\"))\n  hv_read_nogenus &lt;- hv_read_nogenus %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;% select(-rank, -parent_taxid) %&gt;%\n    inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n    filter(!is.na(taxid), !rank %in% high_ranks)\n  #cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\n}\n\n# Get taxon names for higher-ranked assignments\nsmatch &lt;- hv_read_db$seq_id %in% hv_read_genus$seq_id\nhv_read_highrank &lt;- hv_read_db[!smatch,] %&gt;% inner_join(viral_taxids %&gt;% group_by(taxid) %&gt;% filter(row_number() == 1), by = \"taxid\")\n\n# Count viral genera (& unassigned viruses)\nhv_counts_wide &lt;- bind_rows(hv_read_genus, hv_read_highrank) %&gt;% group_by(name, enrichment) %&gt;% count %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"n\", values_fill = 0) \nhv_counts &lt;- hv_counts_wide %&gt;%\n  pivot_longer(-name, names_to = \"enrichment\", values_to = \"n_reads_virus\")\n\n\n\nCodehv_counts_fraction &lt;- hv_counts %&gt;%\n  inner_join(hv_reads_total %&gt;% select(enrichment, n_reads_hv, n_reads_raw), by=\"enrichment\") %&gt;%\n  mutate(p_reads_virus_all = n_reads_virus/n_reads_raw, p_reads_virus_hv = n_reads_virus/n_reads_hv)\ng_hv_counts &lt;- ggplot(hv_counts_fraction, aes(x=name, y=p_reads_virus_all, color=enrichment)) +\n  geom_point(shape=16) +\n  scale_y_log10(name = \"Relative abundance\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1), aspect.ratio = 1/5)\ng_hv_counts\n\n\n\n\nAs expected, viruses included in the respiratory virus panel see large increases in relative abundance in the enriched vs the unenriched samples, with the largest relative increases seen for Bocaparvovirus (Human bocavirus 1, 2c, 3), Betacoronavirus (SARS-CoV-2, OC43, HKU1), and Mastadenovirus (Human adenovirus B1, C2, E4). Confusingly, Orthopoxvirus, Cytomegalovirus and Gemygorvirus all also show substantially increased relative abundance, even though as far as I can tell there are no viruses from those genera in the Illumina panel. Numerous other viruses show weaker enrichment; even norovirus shows moderate (~4x) enrichment in the enriched vs the unenriched samples.\nConversely, a number of viruses are found in the unenriched samples that are absent in the enriched samples, including Rotavirus, Flavivirus, Parvovirus, and various papillomaviruses and polyomaviruses. One natural hypothesis for this is that these viruses were excluded by the enrichment panel and so had their relative abundance reduced; however, the enrichment observed for various other not-in-panel viruses calls this into question. An alternative hypothesis is that this difference is simply a consequence of the much deeper sequencing conducted on the unenriched samples (geometric mean of ~340k read pairs per enriched sample vs 42M read pairs per unenriched sample).\n\nCodehv_ra_wide &lt;- hv_counts_fraction %&gt;% select(name, enrichment, p_reads_virus_all) %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"p_reads_virus_all\", values_fill = 0) %&gt;%\n  rename(ra_enriched = Enriched, ra_unenriched = Unenriched) %&gt;%\n  mutate(relative_enrichment = log10(ra_enriched/ra_unenriched)) %&gt;%\n  arrange(desc(relative_enrichment), desc(ra_enriched), ra_unenriched)\nhv_ra_wide\n\n\n\n  \n\n\n\nAt this point, I’m satisfied with my workflow’s ability to produce usable results on the Crits-Christoph data. Next, I’ll apply this updated workflow to another previously-published WMGS dataset, likely Rothman et al. (2021)."
  },
  {
    "objectID": "notebooks/2024-02-23_rothman-1.html",
    "href": "notebooks/2024-02-23_rothman-1.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 1",
    "section": "",
    "text": "In my last entry, I finished workflow analysis of Crits-Christoph et al. 2021 enriched and unenriched MGS data. In this entry, I apply the workflow to another pre-existing dataset from the P2RA project, namely Rothman et al. 2021.\nThis is a much bigger dataset than Crits-Christoph, with 6 billion reads spread out over 363 different samples. Interestingly, Nextflow seemed to struggle more with the number of samples than their size; when running the entire dataset through the pipeline I kept running into issues with caching that seemed to be due to the sheer number of jobs in the pipeline. I’ll keep working on these, but in the meantime I decided to separate the samples from the dataset into groups to make running them through the pipeline easier.\nIn this entry, I analyze the results of the pipeline on the 97 unenriched samples from the pipeline. These were collected from 9 California treatment plants between July 2020 and January 2021, pasteurized, filtered through a 0.22um filter and concentrated with 10-kDa Amicon filters. After this, they underwent RNA-seq library prep and were sequenced on an Illumina NovaSeq 6000.\nThe raw data\nThe Rothman unenriched samples totaled roughly 660M read pairs (151 gigabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 6.7 to 165.9M, with an average of 73.3M.\nWhile many samples had low levels of sequence duplication according to FASTQC, others had very high levels of duplication in excess of 75%. The median level of duplication according to FASTQ was roughly 56%. Adapter levels were high.\nRead qualities for most samples were consistently very high, but some samples showed a large drop in sequence quality around position 20, suggesting a serious need for preprocessing to remove low-quality bases.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-23_rothman-1/\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-unenriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 60% of total reads from unenriched samples, with the largest reductions seen during deduplication. Secondary ribodepletion removing a further 6%. However, these summary figures conceal significant inter-sample variation. Unsurprisingly, samples that showed a higher initial level of duplication also showed greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples, but failed to correct the large mid-read drop in quality seen in many samples. Since this group actually accounted for over half of samples, I decided to retain it for now, but will investigate ways to trim or discard such reads in future analyses, including of this data set.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 14%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nOverall, in these unenriched samples, between 66% and 86% of reads (mean 74%) are low-quality, duplicates, or unassigned. Of the remainder, 8-21% (mean 15%) were identified as ribosomal, and 2-12% (mean 5%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.4-6.5% (mean 1.8%) and that of viral reads from 0.6-8.6% (mean 4.5%).\nThe latter of these is strikingly high, about an order of magnitude higher than that observed in Crits-Christoph. In the public dashboard, high levels of total virus reads are also observed, and attributed almost entirely to elevated levels of tobamoviruses, especially tomato brown rugose fruit virus. Digging into the Kraken reports for these samples, we find that for most samples, Tobamovirus indeed makes up the vast majority of viral reads:\n\nCodetoba_path &lt;- file.path(data_dir, \"tobamovirus.tsv\")\ntoba &lt;- read_tsv(toba_path, show_col_types = FALSE)\ng_toba &lt;- ggplot(toba, aes(x=p_viral_reads_Tobamovirus)) +\n  geom_histogram(binwidth = 0.01, boundary=0) +\n  scale_x_continuous(name = \"% Viral reads assigned to Tobamovirus\",\n                     limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0),\n                     labels = function(x) x*100) +\n  scale_y_continuous(\"# samples\", breaks = seq(0,100,10),\n                   expand = c(0,0)) +\n  theme_base\ng_toba\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a modified form of the pipeline used in my last entry:\n\nAfter initial ribodepletion, surviving reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters.\nReads that aligned successfully with Bowtie2 were filtered to remove human, livestock, cloning vector, and other potential contaminant sequences, then run through Kraken2 using the standard 16GB database.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 16034 read pairs across all unenriched samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nTo investigate these putative viral reads, I ran them through BLASTN and analyzed the results in a similar manner to Crits-Christoph:\n\nCodemrg_num &lt;- mrg %&gt;% group_by(sample) %&gt;% \n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"rothman-putative-viral.fasta\"))\n\n\n\nCode# Import viral taxids and relationships\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"rothman-putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis initially looks pretty good, with high-scoring true-positives massively outnumbering false-positives and unclear cases. However, there are also a lot of low-scoring true-positives, which are excluded when the alignment score threshold is placed high enough to exclude the majority of false-positives. This pulls down sensitivity at higher score thresholds; as a result, F1 score is lower than I’d like, peaking at 0.947 for a disjunctive cutoff of 17 and dropping to 0.921 for a cutoff of 20.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nImproving precision for high-scoring sequences will only have a marginal effect on F1 score here, as the limiting factor is sensitivity. The best way to improve this would be to find ways to better exclude low-scoring false-positives from the dataset, then lower the score threshold closer to 15. I’m wary about doing this, however, as I suspect low-scoring false-positives will be substantially harder to remove than high-scoring ones, and I’m somewhat less worried about false-negatives than false-positives here in any case. As such, I’m going to stick with my current pipeline for now.\n\nCodemrg_unclear &lt;- mrg_blast %&gt;% ungroup %&gt;% filter(viral_status_out == \"UNCLEAR\") %&gt;%\n  arrange(sample, seq_num)\nmrg_unclear_fasta &lt;- mrg_unclear  %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_unclear_fasta_out &lt;- do.call(paste, c(mrg_unclear_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_unclear_fasta_out, file.path(data_dir, \"rothman-unclear-viral.fasta\"))\n\n\nMoving on to the small number of “unclear” viral matches (155 total, 107 high-scoring), as with Crits-Christoph, the vast majority of these appear to be true positives when run on NCBI BLAST online:\n\nCodeunclear_viral_status &lt;- c(\n  rep(TRUE, 53), FALSE, TRUE,\n  rep(TRUE, 13), FALSE, rep(TRUE, 15), FALSE, rep(TRUE, 6), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 5),\n  rep(TRUE, 2), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 29)\n)\nmrg_unclear_out &lt;- mrg_unclear %&gt;% mutate(viral_status_true = unclear_viral_status)\ng_unclear &lt;- mrg_unclear_out %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_true)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nSince this is two large datasets for which I’ve now gotten this result, I’ll henceforth treat reads for which only one of two reads shows a BLAST viral match as true positives for validation purposes.\nHuman-infecting virus reads: relative abundance\nIn total, my pipeline identified 12305 reads as human-viral out of 660M total reads, for a relative HV abundance of \\(1.87 \\times 10^{-5}\\). This compares to \\(4.3 \\times 10^{-6}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a 4.4-fold increase, similar to that observed for Crits-Christoph. Excluding false-positives identified by BLAST above, the new estimate is reduced slightly, to \\(1.81 \\times 10^{-5}\\).\nAggregating by treatment plant location, we see substantial variation in HV relative abundance, from \\(7.5 \\times 10^{-7}\\) for location JWPCP to \\(5.91 \\times 10^{-5}\\) for location SB:\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_blast %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20,\n         hv_true = viral_status &gt;= 1)\nread_counts_hv_all &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_all\")\nread_counts_hv_val &lt;- mrg_hv %&gt;% filter(hv_status, hv_true) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_validated\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv_all, by=\"sample\") %&gt;%\n  left_join(read_counts_hv_val, by=\"sample\") %&gt;%\n  mutate(n_reads_hv_all = replace_na(n_reads_hv_all, 0),\n         n_reads_hv_validated = replace_na(n_reads_hv_validated, 0),\n         p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_melted &lt;- read_counts %&gt;% \n  select(sample, location, date, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group))) %&gt;%\n  arrange(location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\nread_counts_agg_melted &lt;- read_counts_agg %&gt;% \n  select(location, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group)))\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg_melted, aes(x=location, y=p, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  facet_wrap(~group, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))#\ng_phv_agg\n\n\n\n\nPlotting HV relative abundance over time for each plant, we see that while some plants remain roughly stable over time, several others exhibit a gradual increase in HV relative abundance over the course of the study, possibly related to the winter disease season:\n\nCode# Visualize\ng_phv_time &lt;- ggplot(read_counts_melted, aes(x=date, y=p, color=location, linetype=group)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_linetype_discrete(name = \"Estimator\") +\n  facet_grid(location~.) +\n  theme_base\ng_phv_time\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially noroviruses (e.g. Norwalk virus) and hudisaviruses, though betacoronaviruses including SARS-CoV-2 make a respectable showing in several samples:\n\nCode# Get viral taxon names for putative HV reads\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, name) %&gt;%\n    summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, name) %&gt;%\n  summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results make sense aren’t especially surprising. In my next entry, I’ll turn to the enriched samples from Rothman, and compare them to these unenriched samples."
  },
  {
    "objectID": "notebooks/2024-02-27_rothman-1.html",
    "href": "notebooks/2024-02-27_rothman-1.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 1",
    "section": "",
    "text": "In my last entry, I finished workflow analysis of Crits-Christoph et al. 2021 enriched and unenriched MGS data. In this entry, I apply the workflow to another pre-existing dataset from the P2RA project, namely Rothman et al. 2021.\nThis is a much bigger dataset than Crits-Christoph, with 6 billion reads spread out over 363 different samples. Interestingly, Nextflow seemed to struggle more with the number of samples than their size; when running the entire dataset through the pipeline I kept running into issues with caching that seemed to be due to the sheer number of jobs in the pipeline. I’ll keep working on these, but in the meantime I decided to separate the samples from the dataset into groups to make running them through the pipeline easier.\nIn this entry, I analyze the results of the pipeline on the 97 unenriched samples from the pipeline. These were collected from 9 California treatment plants between July 2020 and January 2021, pasteurized, filtered through a 0.22um filter and concentrated with 10-kDa Amicon filters. After this, they underwent RNA-seq library prep and were sequenced on an Illumina NovaSeq 6000.\nThe raw data\nThe Rothman unenriched samples totaled roughly 660M read pairs (151 gigabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 6.7 to 165.9M, with an average of 73.3M.\nWhile many samples had low levels of sequence duplication according to FASTQC, others had very high levels of duplication in excess of 75%. The median level of duplication according to FASTQ was roughly 56%. Adapter levels were high.\nRead qualities for most samples were consistently very high, but some samples showed a large drop in sequence quality around position 20, suggesting a serious need for preprocessing to remove low-quality bases.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-23_rothman-1/\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-unenriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 60% of total reads from unenriched samples, with the largest reductions seen during deduplication. Secondary ribodepletion removing a further 6%. However, these summary figures conceal significant inter-sample variation. Unsurprisingly, samples that showed a higher initial level of duplication also showed greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples, but failed to correct the large mid-read drop in quality seen in many samples. Since this group actually accounted for over half of samples, I decided to retain it for now, but will investigate ways to trim or discard such reads in future analyses, including of this data set.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 14%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nOverall, in these unenriched samples, between 66% and 86% of reads (mean 74%) are low-quality, duplicates, or unassigned. Of the remainder, 8-21% (mean 15%) were identified as ribosomal, and 2-12% (mean 5%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.4-6.5% (mean 1.8%) and that of viral reads from 0.6-8.6% (mean 4.5%).\nThe latter of these is strikingly high, about an order of magnitude higher than that observed in Crits-Christoph. In the public dashboard, high levels of total virus reads are also observed, and attributed almost entirely to elevated levels of tobamoviruses, especially tomato brown rugose fruit virus. Digging into the Kraken reports for these samples, we find that for most samples, Tobamovirus indeed makes up the vast majority of viral reads:\n\nCodetoba_path &lt;- file.path(data_dir, \"tobamovirus.tsv\")\ntoba &lt;- read_tsv(toba_path, show_col_types = FALSE)\ng_toba &lt;- ggplot(toba, aes(x=p_viral_reads_Tobamovirus)) +\n  geom_histogram(binwidth = 0.01, boundary=0) +\n  scale_x_continuous(name = \"% Viral reads assigned to Tobamovirus\",\n                     limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0),\n                     labels = function(x) x*100) +\n  scale_y_continuous(\"# samples\", breaks = seq(0,100,10),\n                   expand = c(0,0)) +\n  theme_base\ng_toba\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a modified form of the pipeline used in my last entry:\n\nAfter initial ribodepletion, surviving reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters.\nReads that aligned successfully with Bowtie2 were filtered to remove human, livestock, cloning vector, and other potential contaminant sequences, then run through Kraken2 using the standard 16GB database.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 16034 read pairs across all unenriched samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nTo investigate these putative viral reads, I ran them through BLASTN and analyzed the results in a similar manner to Crits-Christoph:\n\nCodemrg_num &lt;- mrg %&gt;% group_by(sample) %&gt;% \n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"rothman-putative-viral.fasta\"))\n\n\n\nCode# Import viral taxids and relationships\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"rothman-putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis initially looks pretty good, with high-scoring true-positives massively outnumbering false-positives and unclear cases. However, there are also a lot of low-scoring true-positives, which are excluded when the alignment score threshold is placed high enough to exclude the majority of false-positives. This pulls down sensitivity at higher score thresholds; as a result, F1 score is lower than I’d like, peaking at 0.947 for a disjunctive cutoff of 17 and dropping to 0.921 for a cutoff of 20.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nImproving precision for high-scoring sequences will only have a marginal effect on F1 score here, as the limiting factor is sensitivity. The best way to improve this would be to find ways to better exclude low-scoring false-positives from the dataset, then lower the score threshold closer to 15. I’m wary about doing this, however, as I suspect low-scoring false-positives will be substantially harder to remove than high-scoring ones, and I’m somewhat less worried about false-negatives than false-positives here in any case. As such, I’m going to stick with my current pipeline for now.\n\nCodemrg_unclear &lt;- mrg_blast %&gt;% ungroup %&gt;% filter(viral_status_out == \"UNCLEAR\") %&gt;%\n  arrange(sample, seq_num)\nmrg_unclear_fasta &lt;- mrg_unclear  %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_unclear_fasta_out &lt;- do.call(paste, c(mrg_unclear_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_unclear_fasta_out, file.path(data_dir, \"rothman-unclear-viral.fasta\"))\n\n\nMoving on to the small number of “unclear” viral matches (155 total, 107 high-scoring), as with Crits-Christoph, the vast majority of these appear to be true positives when run on NCBI BLAST online:\n\nCodeunclear_viral_status &lt;- c(\n  rep(TRUE, 53), FALSE, TRUE,\n  rep(TRUE, 13), FALSE, rep(TRUE, 15), FALSE, rep(TRUE, 6), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 5),\n  rep(TRUE, 2), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 29)\n)\nmrg_unclear_out &lt;- mrg_unclear %&gt;% mutate(viral_status_true = unclear_viral_status)\ng_unclear &lt;- mrg_unclear_out %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_true)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nSince this is two large datasets for which I’ve now gotten this result, I’ll henceforth treat reads for which only one of two reads shows a BLAST viral match as true positives for validation purposes.\nHuman-infecting virus reads: relative abundance\nIn total, my pipeline identified 12305 reads as human-viral out of 660M total reads, for a relative HV abundance of \\(1.87 \\times 10^{-5}\\). This compares to \\(4.3 \\times 10^{-6}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a 4.4-fold increase, similar to that observed for Crits-Christoph. Excluding false-positives identified by BLAST above, the new estimate is reduced slightly, to \\(1.81 \\times 10^{-5}\\).\nAggregating by treatment plant location, we see substantial variation in HV relative abundance, from \\(7.5 \\times 10^{-7}\\) for location JWPCP to \\(5.91 \\times 10^{-5}\\) for location SB:\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_blast %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20,\n         hv_true = viral_status &gt;= 1)\nread_counts_hv_all &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_all\")\nread_counts_hv_val &lt;- mrg_hv %&gt;% filter(hv_status, hv_true) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_validated\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv_all, by=\"sample\") %&gt;%\n  left_join(read_counts_hv_val, by=\"sample\") %&gt;%\n  mutate(n_reads_hv_all = replace_na(n_reads_hv_all, 0),\n         n_reads_hv_validated = replace_na(n_reads_hv_validated, 0),\n         p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_melted &lt;- read_counts %&gt;% \n  select(sample, location, date, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group))) %&gt;%\n  arrange(location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\nread_counts_agg_melted &lt;- read_counts_agg %&gt;% \n  select(location, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group)))\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg_melted, aes(x=location, y=p, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  facet_wrap(~group, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))#\ng_phv_agg\n\n\n\n\nPlotting HV relative abundance over time for each plant, we see that while some plants remain roughly stable over time, several others exhibit a gradual increase in HV relative abundance over the course of the study, possibly related to the winter disease season:\n\nCode# Visualize\ng_phv_time &lt;- ggplot(read_counts_melted, aes(x=date, y=p, color=location, linetype=group)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_linetype_discrete(name = \"Estimator\") +\n  facet_grid(location~.) +\n  theme_base\ng_phv_time\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially noroviruses (e.g. Norwalk virus) and hudisaviruses, though betacoronaviruses including SARS-CoV-2 make a respectable showing in several samples:\n\nCode# Get viral taxon names for putative HV reads\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, name) %&gt;%\n    summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, name) %&gt;%\n  summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results make sense aren’t especially surprising. In my next entry, I’ll turn to the enriched samples from Rothman, and compare them to these unenriched samples."
  },
  {
    "objectID": "notebooks/2024-02-29_rothman-2.html",
    "href": "notebooks/2024-02-29_rothman-2.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 2",
    "section": "",
    "text": "In my last entry, I analyzed the unenriched samples from Rothman et al. 2021. In this entry, I extend that analysis to the 266 samples that underwent panel enrichment using the Illumina respiratory virus panel prior to sequencing. (These were otherwise processed identially to the unenriched samples described in my last entry.)\nThe raw data\nThe Rothman panel-enriched samples totaled roughly 5.4B read pairs (1.1 terabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 0.8M to 89M, with an average of 20M. The great majority of reads came from three treatment plant locations: ESC, HTP, and PL. Duplication and adapter levels were similar to the unenriched samples, which a median FASTQC-measured duplication level of 57%. Read qualities were high, albeit with a dropoff towards the end of longer reads; the large mid-read drop seen in some unenriched samples was not observed here.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-29_rothman-2\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-enriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Enriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nDeduplication and conservative ribodepletion together removed about 57% of total reads on average, similar to unenriched samples. Secondary ribodepletion removing a further 4% on average. However, as before, these summary figures conceal significant inter-sample variation, with samples that showed a higher initial level of duplication also showing greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 11%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import data for unenriched samples\ndata_dir_old &lt;- \"../data/2024-02-23_rothman-1/\"\nbracken_path_old &lt;- file.path(data_dir_old, \"bracken_counts.tsv\")\nbasic_stats_path_old &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nlibraries_path_old &lt;- file.path(data_dir_old, \"rothman-libraries-unenriched.csv\")\n\nbracken_old &lt;- read_tsv(bracken_path_old, show_col_types = FALSE)\nlibraries_old &lt;- read_csv(libraries_path_old, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\nbasic_stats_old &lt;- read_tsv(basic_stats_path_old, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries_old, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\n\n# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\ntotal_assigned_old &lt;- bracken_old %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread_old &lt;- bracken_old %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\nread_counts_preproc_old &lt;- basic_stats_old %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts_old &lt;- read_counts_preproc_old %&gt;%\n  inner_join(total_assigned_old %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread_old, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\nread_comp_old &lt;- transmute(read_counts_old, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long_old &lt;- pivot_longer(read_comp_old, -(sample:enrichment), \n                                   names_to = \"classification\",\n                                   names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ_old &lt;- read_comp_long_old %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n# Merge data\nread_comp_mrg &lt;- bind_rows(read_comp_summ %&gt;% mutate(enrichment = \"Enriched\"),\n                           read_comp_summ_old %&gt;% mutate(enrichment = \"Unenriched\"))\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_mrg, aes(x=enrichment, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_mrg %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=enrichment, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nThe average fraction of low-quality, duplicate, and unassigned reads is slightly higher in enriched vs unenriched samples (mean 80% vs 75%), while the average fraction of ribosomal reads is lower (10% vs 15%). The fraction of bacterial reads is similar (7% vs 5%), while the overall fraction of viral reads is actually slightly lower (2% vs 5%). This latter finding is perhaps surprising, given that the enriched samples are enriched for a group of viruses; I suspect the observed results are due to non-human viruses (especially tobamoviruses) dominating human viruses in the overall counts.\nHuman-infecting virus reads\nNow we come to the main result of interest: the fraction of human-infecting virus reads in enriched vs unenriched samples.\n\nCode# New HV reads\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Old HV reads\nhv_reads_filtered_path_old &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered_old &lt;- read_tsv(hv_reads_filtered_path_old, show_col_types = FALSE) %&gt;%\n  inner_join(libraries_old, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Combined\nmrg &lt;- bind_rows(hv_reads_filtered, hv_reads_filtered_old) %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev)) %&gt;%\n  filter(assigned_hv | hit_hv | adj_score_max &gt;= 20)\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(kraken_label~enrichment, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nFollowing the same selection criteria used for the unenriched samples, we identify 177,246 HV reads across the panel-enriched samples, compared to 12,305 in the unenriched samples. This corresponds to an overall relative HV abundance estimate of \\(3.3 \\times 10^{-5}\\) - about double the estimate for unenriched samples of \\(1.87 \\times 10^{-5}\\). This is much lower than the HV relative abundance observed in Crits-Christoph’s enriched samples, which exceeded \\(10^{-2}\\).\nPanel-enriched relative abundance for individual treatment plants varied from \\(9.3 \\times 10^{-6}\\) to \\(1.6 \\times 10^{-4}\\), with enrichment factors compared to unenriched samples from the same plant ranging from \\(1\\times\\) (HTP) to \\(12\\times\\) (ESC). Overall, this seems like a relatively disappointing showing for panel-enrichment compared to some other datasets we’ve seen.\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, enrichment, date, n_reads_raw = n_read_pairs)\nread_counts_raw_old &lt;- basic_stats_old %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, enrichment, date, n_reads_raw = n_read_pairs)\nread_counts_raw_mrg &lt;- bind_rows(read_counts_raw, read_counts_raw_old)\n\n# Get HV read counts & RA\nread_counts_hv_mrg &lt;- mrg %&gt;% group_by(sample, location, enrichment) %&gt;%\n  count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw_mrg %&gt;%\n  left_join(read_counts_hv_mrg, by=c(\"sample\", \"location\", \"enrichment\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location, enrichment) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Calculate enrichment factors\nread_counts_enrichment &lt;- read_counts_agg %&gt;% select(location, enrichment, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = \"location\", names_from = enrichment, values_from = p_reads_hv) %&gt;%\n  mutate(enrichment = Enriched/Unenriched)\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=location, y=p_reads_hv, color=location, \n                                         shape=enrichment)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  scale_shape_discrete(name = \"Panel enrichment\") +\n  guides(shape = guide_legend(nrow=2, vjust=0.5), color=\"none\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv_agg\n\n\n\n\nDigging into specific viruses, the results are…odd. While the list of most highly enriched viruses encludes some included in the Illumina panel (e.g. SARS-CoVs, betapolyomaviruses), many are fecal-oral viruses with no obvious relationship to the panel (e.g. various astroviruses). The most highly enriched viral genus is Lentivirus, which includes HIV and is not included in the Illumina panel. I don’t know enough about the panel or these respective viruses to give a strong take on what’s going on here, but it certainly seems that the Illumina RVP is less effective at enriching for specific respiratory viruses in Rothman than in Crits-Christoph.\n\nCodeviral_taxa_path &lt;- file.path(data_dir_old, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get viral taxon names for putative HV reads\nmrg_named &lt;- mrg %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\n\nhv_reads_species &lt;- raise_rank(mrg_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, enrichment, name) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, enrichment, n_reads_raw), \n             by=c(\"location\", \"enrichment\"))\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name, enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, enrichment, name) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, enrichment, n_reads_raw), \n             by=c(\"location\", \"enrichment\"))\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name, enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCodemax_rank_species &lt;- 10\n\n# Compute species enrichment\nhv_species_enr &lt;- hv_species_counts_agg %&gt;% filter(location == \"All locations\") %&gt;%\n  select(location, enrichment, name, n_reads_raw, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = c(\"location\", \"name\"), names_from = \"enrichment\", \n              values_from = c(\"p_reads_hv\", \"n_reads_raw\")) %&gt;%\n  mutate(p_reads_hv_Enriched = replace_na(p_reads_hv_Enriched, 0),\n         p_reads_hv_Unenriched = replace_na(p_reads_hv_Unenriched, 0),\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  group_by(location) %&gt;%\n  mutate(n_reads_raw_Enriched = max(n_reads_raw_Enriched, na.rm = TRUE),\n         n_reads_raw_Unenriched = max(n_reads_raw_Unenriched, na.rm = TRUE))\n\n# Identify most enriched/de-enriched species\nhv_species_enr_ranked &lt;- hv_species_enr %&gt;% group_by(location) %&gt;%\n  filter(log_enrichment &lt; Inf, log_enrichment &gt; -Inf) %&gt;%\n  mutate(rank_enrichment_neg = row_number(log_enrichment),\n         rank_enrichment_pos = row_number(desc(log_enrichment)),\n         major = rank_enrichment_neg &lt;= max_rank_species | rank_enrichment_pos &lt;= max_rank_species)\n\n# Aggregate and visualize\nhv_species_enr_plot &lt;- hv_species_enr_ranked %&gt;%\n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(location, name_display, n_reads_raw_Unenriched, n_reads_raw_Enriched) %&gt;%\n  summarize(n_reads_hv_Enriched = sum(p_reads_hv_Enriched * n_reads_raw_Enriched),\n            n_reads_hv_Unenriched = sum(p_reads_hv_Unenriched * n_reads_raw_Unenriched),\n            .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv_Enriched = n_reads_hv_Enriched / n_reads_raw_Enriched,\n         p_reads_hv_Unenriched = n_reads_hv_Unenriched / n_reads_raw_Unenriched,\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  arrange(log_enrichment) %&gt;% mutate(name_display = fct_inorder(name_display))\ng_species_enr &lt;- ggplot(hv_species_enr_plot,\n                       aes(x=name_display, y=log_enrichment)) +\n  geom_hline(yintercept = 0, color=\"red\", linetype = \"dashed\") +\n  geom_point(shape=16) +\n  scale_y_continuous(name = \"Log10 enrichment in panel-enriched samples\",\n                     limits = c(-1.5,1.5), expand=c(0,0)) +\n  facet_wrap(location~., scales=\"free\", ncol=1) +\n  theme_kit + theme(plot.margin = margin(l=1, unit=\"cm\"))\ng_species_enr\n\n\n\nCode# Single-sample viruses\nhv_species_enr_solo &lt;- hv_species_enr %&gt;%\n  filter(log_enrichment == Inf) %&gt;%\n  arrange(desc(p_reads_hv_Enriched))\n\n\n\nCodemax_rank_genera &lt;- 10\n\n# Compute genus enrichment\nhv_genera_enr &lt;- hv_genera_counts_agg %&gt;% filter(location == \"All locations\") %&gt;%\n  select(location, enrichment, name, n_reads_raw, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = c(\"location\", \"name\"), names_from = \"enrichment\", \n              values_from = c(\"p_reads_hv\", \"n_reads_raw\")) %&gt;%\n  mutate(p_reads_hv_Enriched = replace_na(p_reads_hv_Enriched, 0),\n         p_reads_hv_Unenriched = replace_na(p_reads_hv_Unenriched, 0),\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  group_by(location) %&gt;%\n  mutate(n_reads_raw_Enriched = max(n_reads_raw_Enriched, na.rm = TRUE),\n         n_reads_raw_Unenriched = max(n_reads_raw_Unenriched, na.rm = TRUE))\n\n# Identify most enriched/de-enriched genera\nhv_genera_enr_ranked &lt;- hv_genera_enr %&gt;% group_by(location) %&gt;%\n  filter(log_enrichment &lt; Inf, log_enrichment &gt; -Inf) %&gt;%\n  mutate(rank_enrichment_neg = row_number(log_enrichment),\n         rank_enrichment_pos = row_number(desc(log_enrichment)),\n         major = rank_enrichment_neg &lt;= max_rank_genera | rank_enrichment_pos &lt;= max_rank_genera)\n\n# Aggregate and visualize\nhv_genera_enr_plot &lt;- hv_genera_enr_ranked %&gt;%\n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(location, name_display, n_reads_raw_Unenriched, n_reads_raw_Enriched) %&gt;%\n  summarize(n_reads_hv_Enriched = sum(p_reads_hv_Enriched * n_reads_raw_Enriched),\n            n_reads_hv_Unenriched = sum(p_reads_hv_Unenriched * n_reads_raw_Unenriched),\n            .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv_Enriched = n_reads_hv_Enriched / n_reads_raw_Enriched,\n         p_reads_hv_Unenriched = n_reads_hv_Unenriched / n_reads_raw_Unenriched,\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  arrange(log_enrichment) %&gt;% mutate(name_display = fct_inorder(name_display))\ng_genera_enr &lt;- ggplot(hv_genera_enr_plot,\n                       aes(x=name_display, y=log_enrichment)) +\n  geom_hline(yintercept = 0, color=\"red\", linetype = \"dashed\") +\n  geom_point(shape=16) +\n  scale_y_continuous(name = \"Log10 enrichment in panel-enriched samples\",\n                     limits = c(-3,3), expand=c(0,0)) +\n  facet_wrap(location~., scales=\"free\", ncol=1) +\n  theme_kit + theme(plot.margin = margin(l=1, unit=\"cm\"))\ng_genera_enr\n\n\n\nCode# Single-sample viruses\nhv_genera_enr_solo &lt;- hv_genera_enr %&gt;%\n  filter(log_enrichment == Inf) %&gt;%\n  arrange(desc(p_reads_hv_Enriched))"
  },
  {
    "objectID": "notebooks/2024-03-01_dedup.html",
    "href": "notebooks/2024-03-01_dedup.html",
    "title": "Improving read deduplication in the MGS workflow",
    "section": "",
    "text": "After investigating several options in previous entries, I settled on Clumpify for deduplication of reads for my MGS pipeline. Unfortunately, Clumpify as I’m currently running it has a key flaw, which it shares with most other deduplication tools that run on paired reads: it’s unable to detect “reverse-complement” duplicates in which the forward read of one pair matches the reverse read of another & vice-versa. Since the orientation of reads is random in many cases, this will leave a random subset of duplicates undetected.\nThis is tolerable in many instances, but is a problem for cases where we care a lot about getting accurate read counts, such as when estimating the relative abundance of human-infecting viruses. As such, it would be great to find a solution that lets us remove these extra duplicates prior to estimating viral relative abundance.\nClumpify does have a configuration where it unpairs reads prior to deduplication, allowing them to be deduplicated as individual reads. This solves the problem above, but (a) can lead to over-removal in cases where only one read in a pair is a duplicate, and (b) typically breaks on large datasets, I think due to memory issues. Despite looking at quite a number of possible options, I was unable to find a deduplication tool that met my desiderata of (i) running on paired reads, (ii) identifying duplicates in a sensible, error-tolerant way, and (iii) handling reverse-complement duplicates.\nAs a result, I turned to an alternative approach, which was to restrict deduplication of the whole read set to RC-insensitive Clumpify, then apply additional, more stringent deduplication to putative human-viral reads. During the process of HV read identification, downstream of Bowtie2 but upstream of Kraken2, putative viral read pairs are merged & concatenated to produce a single sequence per read pair. This makes deduplication much easier as I can run deduplication tools on the result without needing them to specifically handle paired reads. In particular, Clumpify and its sister program Dedupe should both work well on these sequences, removing duplicates in either orientation while correctly handling errors and “containments” (partial duplicates in which one sequence is completely contained within another).\nTo investigate this, I downloaded the putative human-viral reads for each sample in Crits-Christoph 2021, after identification with Bowtie2 and screening against potential contaminants but before taxonomic assignment with Kraken. I ran three deduplication tools on these sequences:\n\nClumpify in single-end mode\n\nclumpify.sh in=reads/${s}_bowtie2_mjc.fastq.gz out=clumpify/${s}.fastq.gz dedupe containment\n\nDedupe in single-end mode\n\ndedupe.sh in=reads/${s}_bowtie2_mjc.fastq.gz out=dedupe/${s}.fastq.gz\n\nrmdup from the seqkit package\n\nseqkit rmdup -so seqkit/${s}.fastq.gz reads/${s}_bowtie2_mjc.fastq.gz\nI then quantified the number of surviving sequences in each case with FASTQC and MultiQC.\n\nCode# Paths to data\ndata_dir &lt;- \"../data/2024-02-29_dedup/\"\nraw_path &lt;- file.path(data_dir, \"multiqc/multiqc_raw_fastqc.txt\")\nclumpify_path &lt;- file.path(data_dir, \"multiqc/multiqc_clumpify_fastqc.txt\")\ndedupe_path &lt;- file.path(data_dir, \"multiqc/multiqc_dedupe_fastqc.txt\")\nseqkit_path &lt;- file.path(data_dir, \"multiqc/multiqc_seqkit_fastqc.txt\")\n\n# Import data\nraw &lt;- read_tsv(raw_path, show_col_types = FALSE) %&gt;%\n  mutate(Sample = sub(\"_bowtie2_mjc\", \"\", Sample),\n         Method = \"none\")\nclumpify &lt;- read_tsv(clumpify_path, show_col_types = FALSE) %&gt;% mutate(Method = \"clumpify\")\ndedupe &lt;- read_tsv(dedupe_path, show_col_types = FALSE) %&gt;% mutate(Method = \"dedupe\")\nseqkit &lt;- read_tsv(seqkit_path, show_col_types = FALSE) %&gt;% mutate(Method = \"seqkit\")\nprocessed &lt;- bind_rows(raw, clumpify, dedupe, seqkit) %&gt;%\n  mutate(Method = fct_inorder(Method),\n         seqs_abs = `Total Sequences`) %&gt;%\n  group_by(Sample) %&gt;%\n  mutate(seqs_rel = seqs_abs/max(seqs_abs))\n\n# Visualize\ng_dedup_abs &lt;- ggplot(processed, aes(x=Sample, y=seqs_abs, fill=Method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_y_continuous(name=\"# Surviving Reads\", expand = c(0,0), limits = c(0,140000), breaks = seq(0,200000,40000)) +\n  theme_kit\ng_dedup_abs\n\n\n\nCodeg_dedup_rel &lt;- ggplot(processed, aes(x=Sample, y=seqs_rel, fill=Method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_y_continuous(name=\"% Surviving Reads\", breaks = seq(0,1,0.2), labels = function(x) x*100, expand = c(0,0)) +\n  theme_kit\ng_dedup_rel\n\n\n\n\nClumpify consistently removed the most reads, with Dedupe close behind and seqkit’s rmdup performing much less well. As such, I decided to implement Clumpify to remove duplicates at this point in the pipeline, then look at the effect on predicted relative abundance of viruses in one previously-analyzed dataset.\nBrief re-analysis of Crits-Christoph 2021\nOn average, adding in RC-sensitive deduplication reduced overall HV relative abundance measurements by about 4% in unenriched samples and about 10% in panel-enriched samples in Crits-Christoph 2021:\n\nCodecc_dir_new &lt;- file.path(data_dir, \"cc-rerun\")\ncc_dir_old &lt;- file.path(data_dir, \"cc-prev\")\nbasic_stats_new_path &lt;- file.path(cc_dir_new, \"qc_basic_stats.tsv\")\nbasic_stats_old_path &lt;- file.path(cc_dir_old, \"qc_basic_stats.tsv\")\nhv_reads_new_path &lt;- file.path(cc_dir_new, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_old_path &lt;- file.path(cc_dir_old, \"hv_hits_putative_filtered.tsv\")\nlibraries_path &lt;- file.path(cc_dir_new, \"cc-libraries.txt\")\n\n# Get raw read counts\nbasic_stats_new &lt;- read_tsv(basic_stats_new_path, show_col_types = FALSE)\nbasic_stats_old &lt;- read_tsv(basic_stats_old_path, show_col_types = FALSE)\nread_counts_raw_new &lt;- basic_stats_new %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, n_reads_raw = n_read_pairs) %&gt;%\n  mutate(dedup_rc = TRUE)\nread_counts_raw_old &lt;- basic_stats_old %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, n_reads_raw = n_read_pairs) %&gt;%\n  mutate(dedup_rc = FALSE)\nread_counts_raw &lt;- bind_rows(read_counts_raw_new, read_counts_raw_old)\n\n# Get HV read counts\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         dedup_rc = TRUE)\nhv_reads_old &lt;- read_tsv(hv_reads_old_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         dedup_rc = FALSE)\nhv_reads &lt;- bind_rows(hv_reads_new, hv_reads_old)\nhv_reads_cut &lt;- hv_reads %&gt;%\n    mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_counts &lt;- hv_reads_cut %&gt;% group_by(sample, enrichment, dedup_rc) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_counts, read_counts_raw, by=c(\"sample\", \"dedup_rc\")) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment, dedup_rc) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw), .groups=\"drop\") %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment, dedup_rc)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\n\n# Calculate effect of dedup on RA\nra_rel &lt;- hv_reads_bound %&gt;% ungroup %&gt;% select(sample, enrichment, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"dedup_rc\", values_from = \"p_reads_hv\", names_prefix = \"dedup_\") %&gt;%\n  mutate(rel_ra = dedup_TRUE/dedup_FALSE)\n\n\n\nCodeg_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, \n                                    color=enrichment, shape=dedup_rc)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  guides(color=\"none\") +\n  facet_wrap(~enrichment, scales = \"free\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nTurning to specific viruses, just plotting out the relative abundances with and without deduplication isn’t terribly informative due to the wide log scales involved:\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get viral taxon names for putative HV reads\nhv_named &lt;- hv_reads_cut %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\n\nhv_reads_genera &lt;- raise_rank(hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% \n  group_by(sample, enrichment, name, dedup_rc) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_raw, by=c(\"sample\", \"dedup_rc\")) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name, enrichment, dedup_rc) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(sample = \"All samples\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCodehv_genera_counts_plot &lt;- hv_genera_counts_agg %&gt;%\n  filter(sample == \"All samples\", !is.na(name))\ng_genera &lt;- ggplot(hv_genera_counts_plot,\n                   aes(x=name, y=p_reads_hv, shape=dedup_rc, color=enrichment)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_genera\n\n\n\n\n\nCodehv_genera_counts_wide &lt;- hv_genera_counts_agg %&gt;%\n  select(sample, enrichment, name, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"enrichment\", values_from=\"p_reads_hv\") %&gt;%\n  mutate(Enriched = replace_na(Enriched, 0),\n         Unenriched = replace_na(Unenriched, 0),\n         rel_enrichment = Enriched/Unenriched,\n         log_enrichment = log10(rel_enrichment))\nhv_genera_counts_rel_plot &lt;- hv_genera_counts_wide %&gt;%\n  filter(sample == \"All samples\", log_enrichment &lt; \"Inf\", log_enrichment &gt; \"-Inf\")\ng_rel_ra &lt;- ggplot(hv_genera_counts_rel_plot,\n                   aes(x=name, y=log_enrichment, shape=dedup_rc)) +\n  geom_point() +\n  scale_y_continuous(name=\"Log10 enrichment in panel-enriched samples\",\n                     limits = c(0,5), breaks = seq(0,10,1), expand=c(0,0)) +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_rel_ra\n\n\n\n\nHowever, if we specifically look at the change in relative abundance with vs without deduplication, we do see some variation, with many viruses showing no change and others showing reductions in measured RA of up to 20%:\n\nCodehv_genera_counts_rel_dedup &lt;- hv_genera_counts_agg %&gt;%\n  filter(sample == \"All samples\", !is.na(name)) %&gt;%\n  select(sample, enrichment, name, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"dedup_rc\", names_prefix=\"dedup_\", \n              values_from=\"p_reads_hv\", values_fill=0) %&gt;%\n  mutate(dedup_rel = dedup_TRUE/dedup_FALSE,\n         dedup_rel_log = log10(dedup_rel))\ng_genera_rel_dedup &lt;- ggplot(hv_genera_counts_rel_dedup,\n                             aes(x=name, y=dedup_rel, color=enrichment)) +\n  geom_point(alpha=0.5) +\n  scale_y_continuous(\"Viral relative abundance in deduplicated vs\\nnon-deduplicated samples\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_genera_rel_dedup\n\n\n\n\nGoing forward, I’ll include this additional deduplication step in analysis of future data."
  },
  {
    "objectID": "notebooks/2024-03-16_yang.html",
    "href": "notebooks/2024-03-16_yang.html",
    "title": "Workflow analysis of Yang et al. (2020)",
    "section": "",
    "text": "As we work on expanding and updating the P2RA preprint for publication, I’m working on running the relevant datasets through my pipeline to compare the results to those from the original Kraken-based approach. I began by processing Yang et al. (2020), a study that carried out RNA viral metagenomics on raw sewage samples from Xinjiang, China.\nThe study collected 7 1L grab samples of raw sewage from the inlets of three WWTPs in Xinjiang, and processed them using an “anion filter membrane absorption” method quite distinct from the other studies I’ve looked at so far:\n\nInitially, samples were centrifuged to pellet cells and debris.\nThe supernatant was treated with magnesium chloride and HCl, then filtered through a mixed cellulose ester membrane filter, discarding the filtrate.\nMaterial was then eluted from the filters by ultrasonication with 3% beef extract, before going a second filtering step using a 0.22um filter, this time retaining the filtrate.\nNext, the samples underwent RNA extraction using the QIAamp viral RNA Mini Kit.\nFinally, sequencing libraries were generated using KAPA Hyper Prep Kit. No mention is made of rRNA depletion or panel enrichment.\n\nThis study is of particular interest because previous analysis found that the MGS data it generated contains a particularly high fraction of both total viruses and human-infecting viruses. It would be good to know whether this is attributable to the fact that the authors used an unusual method, or if it’s the result of genuine differences on the ground (e.g. an elevated prevalence of enteric disease compared to the USA).\nThe raw data\nThe Yang data is unusual in that it contains only a small number of samples (7 samples from 3 different locations) but each of these samples is sequenced quite deeply: the number of reads per sample varied from 162M to 283M, with an average of 203M. Taken together, the samples totaled roughly 1.4B read pairs (425 gigabases of sequence).\nRead qualities were consistently very high, but adapter levels were also elevated. The level of sequence duplication levels was very high according to FASTQC, with a minimum of 89% across all samples, suggesting a high degree of oversequencing; perhaps unsurprising given the sheer depth of these samples.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-03-15_yang/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=n_read_pairs, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=n_bases_approx, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=percent_duplicates, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, location, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location) %&gt;% arrange(sample, location, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\n\n\nOn average, cleaning & deduplication together removed about 86% of total reads from each sample, with the largest losses seen during deduplication. Only a few reads (low single-digit percentages at most) were lost during subsequent ribodepletion – a surprising finding given the apparent lack of rRNA depletion in the study methods. This makes me suspect that the methods did include rRNA depletion without mentioning it; if this isn’t the case, it suggests that the methods used by the authors might be particularly effective at removing bacteria from the sample.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost_abs_marginal,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with very few adapter sequences found by FASTQC at any stage after the raw data. Improvements in quality were unsurprisingly minimal, given the high initial quality observed.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~., scales=\"free_y\") + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nAccording to FASTQC, deduplication was only moderately effective at reducing measured duplicate levels, despite the high read losses observed. After deduplication, FASTQC-measured duplicate levels fell from an average of 92% to one of 58%:\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:location), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~location, scales = \"free_x\") +\n  theme_kit\ng_comp_minor\n\n\n\n\n\nCodep_reads_summ &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification))) %&gt;%\n  group_by(classification, sample, location) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, pc_mean = mean(p_reads)*100)\n\n\nOverall, in these unenriched samples, between 89% and 95% of reads (mean 93%) are low-quality, duplicates, or unassigned. Of the remainder, 0.4-3.9% (mean 1.6%) were identified as ribosomal, and 0.06-0.22% (mean 0.13%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.006-0.198% (mean 0.049%). Strikingly, those of viral reads ranged from 0.6% all the way up to 10.2% (mean 5.5%).\nThe total fraction of viral reads is strikingly high, even higher on average than for Rothman unenriched samples. As in Rothman, this is primarily accounted for by very high levels of tobamoviruses (viral family Virgaviridae), which make up 74-98% (mean 84%) of viral reads. Most of the remainder is accounted for by Tombusviridae and Solemoviridae, two other families of plant viruses. Only one family of vertebrate viruses, Astroviridae, accounted for more than 1% of viral reads in any sample.\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\nsamples &lt;- as.character(basic_stats_raw$sample)\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\", \"rank\", \"taxid\", \"name\")\nreport_paths &lt;- paste0(data_dir, \"kraken/\", samples, \".report.gz\")\nkraken_reports &lt;- lapply(1:length(samples), \n                         function(n) read_tsv(report_paths[n], col_names = col_names,\n                                              show_col_types = FALSE) %&gt;%\n                           mutate(sample = samples[n])) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nviral_families &lt;- kraken_reports_viral %&gt;% filter(rank == \"F\")\nviral_families_major_list &lt;- viral_families %&gt;% group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= 0.01) %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral)) %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- bind_rows(viral_families_major, viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\")))\ng_families &lt;- ggplot(viral_families_display, aes(x=sample, y=p_reads_viral, fill=name)) +\n  geom_col(position=\"stack\") +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Viral family\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  theme_kit\ng_families\n\n\n\n\nAlmost as striking as the high fraction of viral reads in these data is the extremely low prevalence of bacterial reads. For comparison, non-ribosomal bacterial reads in unenriched Crits-Christoph samples ranged from 16-20%, and in unenriched Rothman samples it ranged from 2-12%. Yang et al. are thus somehow achieving a greater than 10-fold reduction in the fraction of bacterial reads compared to other studies. Unlike the increased prevalence of viruses, this seems harder to explain away via differences in conditions on the ground. Together with the elevated viral fraction, these results suggest to me that Yang’s ideosyncratic methods are worth investigating further.\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a pipeline identical to that described in my entry on unenriched Rothman samples. This process identified a total of 532,583 read pairs across all samples (0.3% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(location) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis is a far larger number of putative viral reads than I previously needed to analyze, and is far too many to realistically process with BLASTN. To address this problem, I selected 1% of putative viral sequences (5,326) and ran them through BLASTN in a similar manner to past datasets:\n\nCodeset.seed(83745)\nmrg_sample &lt;- sample_frac(mrg, 0.01)\nmrg_num &lt;- mrg_sample %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n    mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"yang-putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"yang-putative-viral-all.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\nHigh-scoring true-positives massively outnumber false-positives, while low-scoring true-positives are fairly rare. My usual score cutoff, a disjunctive threshold at S=20, achieves a sensitivity of ~97% and an F1 score of ~98%. As such, I think this looks pretty good, and feel comfortable moving on to analyzing the entire HV dataset.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nHuman-infecting virus reads: analysis\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, location, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate\nread_counts_total &lt;- read_counts %&gt;% ungroup %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         sample= \"All samples\", location = \"All locations\")\nread_counts_agg &lt;- read_counts %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         location = fct_inorder(location))\n\n\nApplying a disjunctive cutoff at S=20 identifies 513,259 reads as human-viral out of 1.42B total reads, for a relative HV abundance of \\(3.62 \\times 10^{-4}\\). This compares to \\(2.0 \\times 10^{-4}\\) on the public dashboard, corresponding to the results for Kraken-only identification: an 80% increase, smaller than the 4-5x increases seen for Crits-Christoph and Rothman. Relative HV abundances for individual samples ranged from \\(6.36 \\times 10^{-5}\\) to \\(1.19 \\times 10^{-3}\\):\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=sample, y=p_reads_hv, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  theme_kit\ng_phv_agg\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially Mamastrovirus, Rotavirus, Salivirus, and fecal-oral Enterovirus species (especially Enterovirus C, which includes poliovirus):\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n\n\nCode# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(sample, location, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(sample, n_reads_raw), by=\"sample\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = \"All samples\", location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(sample, location, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(sample, n_reads_raw), by=\"sample\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = \"All samples\", location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(sample) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(sample) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=sample, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=sample, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results are consistent with what’s been observed before, e.g. in the P2RA preprint and public dashboard.\nConclusion\nI analyzed Yang et al. for two reasons: first, because it’s included in the P2RA dataset that we’re currently reanalyzing, and second, because it has the highest relative abundance of both total and human-infecting viruses of any wastewater study we’ve looked at so far. Having done this analysis, the key question to ask is: is this elevated viral abundance a consequence of the unusual sample-processing methods employed by the authors, or the region in which samples were collected? Both are plausible: the processing methods used in this paper are distinct from any other paper we’ve looked at, which makes it possible that they are significantly superior; on the other hand, it’s also plausible that the methods are comparable in efficacy to more standard approaches, and the difference arises from a genuinely elevated level of viruses in Xinjiang compared to e.g. California (for Rothman and Crits-Christoph).\nThe analyses conducted here aren’t able to give a dispositive answer to that question, but they are suggestive. The fact that human-infecting viruses are strongly dominated by fecal-oral species is consistent with the elevated-infection theory, as these are the kinds of viruses I’d most expect to be elevated in a poorer region of the world compared to a rich one. On the other hand, the fact that total viruses (including plant viruses) are also elevated points in the other direction; I don’t see any obvious reason why we’d expect more tobamoviruses in Xinjiang than California.\nMost compellingly, from my perspective, is the very low fraction of bacterial and ribosomal reads found in the Yang data compared to other datasets I’ve analyzed. The methods make no mention of rRNA depletion prior to sequencing; even if this was conducted without being mentioned, that still doesn’t explain the very low level of non-ribosomal bacterial reads. In general, I’d expect a region with elevated enteric viral pathogens to also show elevated enteric bacterial pathogens, so this absence is difficult to explain with a catchment-based theory. Instead, it points towards the viral enrichment protocols used by this study as potentially achieving genuinely better results than other methods we’ve tried.\nThis certainly isn’t conclusive evidence, or close to it. However, based on these results, I’d be very interested in learning more about the details of Yang et al’s approach to viral enrichment, and to see it tried by another group in a different context to see if these impressive results can be replicated."
  },
  {
    "objectID": "notebooks/2024-03-19_yang-2.html",
    "href": "notebooks/2024-03-19_yang-2.html",
    "title": "Followup analysis of Yang et al. (2020)",
    "section": "",
    "text": "Some people had some helpful questions about my previous analysis of Yang et al. (2020), and the finding that the Yang data contains an unusually high relative abundance of viral sequences, together with an unusually low relative abundance of bacterial sequences. Here are the Bracken composition results I presented last time:\n\nCodedata_dir &lt;- \"../data/2024-03-18_yang-2\"\ndata_dir_old &lt;- file.path(data_dir, \"yang-1\")\nlibraries_path &lt;- file.path(data_dir_old, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\n\n# Import Bracken data\nbracken_path &lt;- file.path(data_dir_old, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:location), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\n\nMultiple people noted that almost all of the reads in all samples were either removed during filtering or flagged as duplicates, and wondered whether the results would stand if we looked at the pre-deduplicated composition instead. Put another way, they wondered whether deduplication might be disproportionately removing ribosomal or other bacterial sequences, and thus giving a misleadingly optimistic picture of the results.\nTo address this, I re-ran the taxonomic composition analysis on a 1% subset of the pre-deduplication Yang data, then compared the effect of deduplication on the inferred taxonomic makeup (excluding filtered and deduplicated reads). In running this analysis, I made the assumption that all of the post-deduplication reads identified as ribosomal were from bacterial ribosomes; this seems to be generally true in our experience of wastewater data and allows us to make a more like-for-like comparison (as we don’t have ribosomal status for the pre-deduplication reads).\nThe results were as follows:\n\nCodeclass_levels &lt;- c(\"Unassigned\", \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# 1. Remove filtered & duplicate reads from original Bracken output & renormalize\nread_comp_renorm &lt;- read_comp_long %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads),\n         classification = classification %&gt;% as.character %&gt;%\n           ifelse(. == \"Ribosomal\", \"Bacterial\", .)) %&gt;%\n  group_by(sample, location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  mutate(classification = factor(classification, levels = class_levels))\n  \n# 2. Import pre-deduplicated \nbracken_path_predup &lt;- file.path(data_dir, \"bracken_counts_subset.tsv\")\nbracken_predup &lt;- read_tsv(bracken_path_predup, show_col_types = FALSE)\ntotal_assigned_predup &lt;- bracken_predup %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread_predup &lt;- bracken_predup %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_predup &lt;- read_counts_preproc %&gt;%\n  select(sample, location, raw_concat, cleaned) %&gt;%\n  mutate(raw_concat = raw_concat * 0.01, cleaned = cleaned * 0.01) %&gt;%\n  inner_join(total_assigned_predup %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread_predup, by=\"sample\")\n# Assess composition\nread_comp_predup &lt;- transmute(read_counts_predup, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_unassigned = cleaned-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_predup_long &lt;- pivot_longer(read_comp_predup, -(sample:location), \n                                      names_to = \"classification\",\n                                      names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads)) %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads))\n\n# 3. Combine\nread_comp_comb &lt;- bind_rows(read_comp_predup_long %&gt;% mutate(deduplicated = FALSE),\n                            read_comp_renorm %&gt;% mutate(deduplicated = TRUE)) %&gt;%\n  mutate(label = ifelse(deduplicated, \"Post-dedup\", \"Pre-dedup\") %&gt;% fct_inorder)\n\n# Plot overall composition\ng_comp_predup &lt;- ggplot(read_comp_comb, aes(x=label, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~sample, scales=\"free\", ncol=4) +\n  theme_kit\ng_comp_predup\n\n\n\n\nIn most cases, deduplication made little-to-no difference to the inferred taxonomic composition, with bacterial and viral read fractions remaining roughly the same in both cases. In a few cases, and especially in sample SRR12204735, deduplication substantially reduced the inferred relative abundance of bacterial reads, probably due to collapsing genuine biological duplicates of common (e.g. ribosomal) sequences. In 5/7 samples, the viral fraction substantially exceeded the bacterial fraction both pre- and post-deduplication.\nOverall, then, it appears that the finding of unusually high viral and low bacterial abundance in the Yang data is not an artefact of deduplication. This is a relief, and strengthens my confidence that their methods are worth investigating further."
  },
  {
    "objectID": "notebooks/2024-04-01_spurbeck.html",
    "href": "notebooks/2024-04-01_spurbeck.html",
    "title": "Workflow analysis of Spurbeck et al. (2023)",
    "section": "",
    "text": "Continuing my analysis of datasets from the P2RA preprint, I analyzed the data from Spurbeck et al. (2023), a wastewater RNA-sequencing study from Ohio. Samples for this study underwent a variety of processing protocols at different research centers across the state. Along with Rothman and Crits-Christoph, Spurbeck is one of three RNA-sequencing studies that underwent full in-depth analysis in the P2RA study, so is worth looking at closely here.\nThis one turned out to be a bit of a saga. As we’ll see in the section on human-virus identification, it took multiple tries and several substantial changes to the pipeline to get things to a state I was happy with. Still, I am happy with the outcome and think the changes will improve analysis of future datasets.\nThe raw data\nThe Spurbeck data comprises 55 samples from 8 processing groups, with 4 to 6 samples per group. The number of sequencing read pairs per sample varied widely from 4.5M-106.7M (mean 33.4M). Taken together, the samples totaled roughly 1.8B read pairs (425 gigabases of sequence). Read qualities were generally high but in need of some light preprocessing. Adapter levels were moderate. Inferred duplication levels were fairly high: 14-91% with a mean of 55%.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-04-01_spurbeck/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats_1.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n\n\nCode# Visualize basic stats\nscale_fill_grp &lt;- purrr::partial(scale_fill_brewer, palette=\"Set3\", name=\"Processing group\")\ng_basic &lt;- ggplot(basic_stats_raw, aes(x=group, fill=group, group=sample)) +\n  scale_fill_grp() + theme_kit\ng_nreads_raw &lt;- g_basic + geom_col(aes(y=n_read_pairs), position = \"dodge\") +\n  scale_y_continuous(name=\"# Read pairs\", expand=c(0,0))\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\nlegend_group &lt;- get_legend(g_nreads_raw)\n\n\ng_nbases_raw &lt;- g_basic + geom_col(aes(y=n_bases_approx), position = \"dodge\") +\n  scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + \n  theme(legend.position = \"none\")\ng_ndup_raw &lt;- g_basic + geom_col(aes(y=percent_duplicates), position = \"dodge\") +\n  scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) +\n  theme(legend.position = \"none\")\n\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_group, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\n\n\n\n\n\nCodescale_color_grp &lt;- purrr::partial(scale_color_brewer,palette=\"Set3\",name=\"Processing group\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_grp() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_wrap(~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. Significant numbers of reads were lost at each stage in the preprocessing pipeline. In total, cleaning, deduplication and initial ribodepletion removed 17-96% (mean 72%) of input reads, while secondary ribodepletion removed an additional 0-11% (mean 6%).\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, group, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, group) %&gt;% arrange(sample, group, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% rename(Stage=stage) %&gt;% group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\")) %&gt;% tail(-1) %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=group,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost_abs_marginal,fill=group,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning with FASTP was mostly successful at removing adapters; however, detectable levels of Illumina Universal Adapter sequences persisted through the preprocessing pipeline. FASTP was successful at improving read quality.\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_grp() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, deduplication was quite effective at reducing measured duplicate levels, with FASTQC-measured levels falling from an average of 55% to one of 22%:\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=group, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=group, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, group, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"group\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, group=group,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:group), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(group, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\\n\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~group, scales=\"free\", ncol=5) +\n  theme_kit\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\\n\",\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~group, scales = \"free_x\", ncol=5) +\n  theme_kit\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, group) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, group) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_total &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n                  classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, group) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")  %&gt;%\n  mutate(group = \"All groups\")\np_reads_summ_prep &lt;- bind_rows(p_reads_summ_total, p_reads_summ_group) %&gt;%\n  mutate(group = fct_inorder(group),\n         classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(classification, group, display) %&gt;%\n  pivot_wider(names_from=group, values_from = display)\np_reads_summ\n\n\n  \n\n\n\nAverage composition varied substantially between groups. Four groups (A, B, I, J) showed high within-group consistency, with high levels of ribosomal reads and very low levels of assigned reads. Other groups showed much more variability between samples, with higher average levels of assigned reads.\nOverall, the average relative abundance of viral reads was 0.04%; however, groups F, G & H showed substantially higher average abundance of around 0.1%. Even these elevated groups, however, still show lower total viral abundance than Rothman or Crits-Christoph, so this doesn’t seem particularly noteworthy.\nHuman-infecting virus reads: validation, round 1\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a pipeline identical to that described in my entry on unenriched Rothman samples. This process identified a total of 31,610 read pairs across all samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_1_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_filtered_1 &lt;- read_tsv(hv_reads_filtered_1_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_1 &lt;- hv_reads_filtered_1 %&gt;% group_by(sample, group) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_1_summ &lt;- n_hv_filtered_1 %&gt;% ungroup %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCodemrg_1 &lt;- hv_reads_filtered_1 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg_1 &lt;- ggplot(mrg_1, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_1\n\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_1, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\n\n\n\nTo analyze all these reads in a reasonable amount of time, I set up a dedicated EC2 instance, downloaded nt, and ran BLASTN locally there, otherwise using the same parameters I’ve used for past datasets:\n\nCodemrg_1_num &lt;- mrg_1 %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n    mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_1_fasta &lt;-  mrg_1_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_1_fasta_out &lt;- do.call(paste, c(mrg_1_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_1_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-1.fasta\"))\n\n\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# NB: Importing partially-processed BLAST results to save storage space\n# Import BLAST results\nblast_results_1_path &lt;- file.path(data_dir, \"putative-viral-blast-best-1.tsv.gz\")\nblast_results_1 &lt;- read_tsv(blast_results_1_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_1_best &lt;- blast_results_1 %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_1_ranked &lt;- blast_results_1_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_1_highrank &lt;- blast_results_1_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_1_paired &lt;- blast_results_1_highrank %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_1_viral &lt;- blast_results_1_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_1_assign &lt;- mrg_1_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_1_assign &lt;- full_join(blast_results_1_viral, mrg_1_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_1_out &lt;- blast_results_1_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_1_blast &lt;- full_join(mrg_1_num, blast_results_1_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_mrg_blast_1 &lt;- mrg_1_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_1\n\n\n\n\n\n\n\n\nCodeg_hist_blast_1 &lt;- ggplot(mrg_1_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_blast_1\n\n\n\n\n\n\n\nThere results were…okay. There were a lot of false positives, but nearly all of them were low-scoring and excluded by my usual score filters. Most true positives, meanwhile, had high enough scores to be retained by those filters. However, there were enough high-scoring false positives and low-scoring true positives to drag down my precision and sensitivity, resulting in an F1 score (at a disjunctive score threshold of 20) a little under 90% – quite a few percentage points lower than I usually aim for.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special=FALSE, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\nstats_1 &lt;- range_f1(mrg_1_blast)\nthreshold_opt_1 &lt;- stats_1 %&gt;% group_by(conj_label) %&gt;% \n  filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_1 &lt;- ggplot(stats_1, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_1, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_1\n\n\n\n\n\n\n\nDigging into taxonomic assignments more deeply, we find that low-scoring false positives are primarily mapped by Bowtie2 to SARS-CoV-2, while higher-scoring false positives are mainly mapped to a variety of parvoviruses and parvo-like viruses, as well as Orf virus (cows again?). High-scoring true positives map to a wide range of viruses, but low-scoring true-positives primarily map to human gammaherpesvirus 4. Given that the latter is a DNA virus, I find this quite suspicious.\n\nCodefp_1 &lt;- mrg_1_blast %&gt;% \n  group_by(viral_status_out, highscore = adj_score_max &gt;= 20, taxid) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name))\nfp_1_major_tab &lt;- fp_1 %&gt;% filter(p &gt; 0.05) %&gt;% arrange(desc(p))\nfp_1_major_list &lt;- fp_1_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_1_major &lt;- fp_1 %&gt;% mutate(major = p &gt; 0.1) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_1_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\ng_fp_1 &lt;- ggplot(fp_1_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_brewer(palette = \"Set3\", name = \"Viral\\ntaxon\") +\n  facet_wrap(~status_display) +\n  guides(fill=guide_legend(nrow=6)) +\n  theme_kit\ng_fp_1\n\n\n\n\n\n\n\nSince sensitivity is more of a problem here than precision, I decided to dig into into the “true positive” herpesvirus reads. There are 1,197 of these in total, 93% of which are low scoring. When I look into the top taxids that BLAST maps these reads to, we see the following:\n\nCode# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n# Get BLAST staxids\nherp_seqs_1 &lt;- mrg_1_blast %&gt;% filter(taxid == 10376, viral_status_out) %&gt;%\n  select(sample, seq_num, taxid, adj_score_max) %&gt;%\n  mutate(highscore = adj_score_max &gt;= 20)\nherp_seqs_1_total &lt;- herp_seqs_1 %&gt;% group_by(highscore) %&gt;% \n  count(name=\"n_seqs_total\")\nherp_blast_1_hits &lt;- herp_seqs_1 %&gt;% \n  left_join(blast_results_1_paired, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(staxid = as.integer(staxid))\nherp_blast_1_hits_top &lt;- herp_blast_1_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% \n  arrange(desc(n)) %&gt;% left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name=ifelse(staxid == 3004166, \n                     \"Caudopunctatus cichlid hepacivirus\", name)) %&gt;%\n  mutate(name = fct_inorder(name))\n\n# Plot\ng_herp_1 &lt;- ggplot(herp_blast_1_hits_top %&gt;% head(10), \n                   aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name=\"# Mapped Read Pairs\") + theme_base + \n  theme(axis.title.y = element_blank())\ng_herp_1\n\n\n\n\n\n\n\nTwo things strike me as notable about these results. First, human gammaherpesvirus 4 is only the second-most-common subject taxid, after SARS-CoV-2, an unrelated virus with a different nucleic-acid type. Hmm. Second, close behind these two viruses is a distinctly non-viral taxon, the common carp. This jumped out at me, because the common carp genome is somewhat notorious for being contaminated with Illumina adapter sequences. This makes me suspect that Illumina adapter contamination is playing a role in these results.\n\nCodecarp_hits_1 &lt;- herp_blast_1_hits %&gt;% filter(staxid == 7962) %&gt;% \n  select(sample, seq_num, taxid) %&gt;% \n  left_join(mrg_1_blast, by=c(\"sample\", \"seq_num\", \"taxid\")) %&gt;% \n  select(sample, seq_num, taxid, adj_score_max, query_seq_fwd, query_seq_rev)\ncarp_hits_1_out &lt;- carp_hits_1 %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\ncarp_hits_1_fasta &lt;- carp_hits_1_out %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\ncarp_hits_1_fasta_out &lt;- do.call(paste, c(carp_hits_1_fasta, sep=\"\\n\")) %&gt;% \n  paste(collapse=\"\\n\")\nwrite(carp_hits_1_fasta_out, file.path(data_dir, \"spurbeck-carp-hits-1.fasta\"))\n\n\nInspecting these reads manually, I find that a large fraction do indeed show substantial adapter content. In particular, of the 1658 individual reads queried, at least 1338 (81%) showed a strong match to the Illumina multiplexing primer. As such, better removal of these adapter sequences would likely remove most or all of these “false true positives”, potentially significantly improving my results for this dataset.\nHuman-infecting virus reads: validation, round 2\nTo address this problem, I added Cutadapt to the pipeline, using settings that allowed it to trim known adaptors internal to the read:\ncutadapt -b file:&lt;adapter_file&gt; -B file:&lt;adapter_file&gt; -m 15 -e 0.25 --action=trim -o &lt;fwd_reads_out&gt; -p &lt;rev_reads_out&gt; &lt;fwd_reads_in&gt; &lt;rev_reads_in&gt;\nRunning this additional preprocessing step reduced the total number of reads surviving cleaning by 10.4M, or an average of 189k reads per sample. The number of putative human-viral reads, meanwhile, was reduced from 31,610 to 19,799, a 37% decrease. This reduction is primarily due to a large decrease in the number of low-scoring Bowtie2-only putative HV hits.\n\nCodebasic_stats_2_path &lt;- file.path(data_dir, \"qc_basic_stats_2.tsv\")\nbasic_stats_2 &lt;- read_tsv(basic_stats_2_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nbasic_stats_2_sum &lt;- basic_stats_2 %&gt;% group_by(stage) %&gt;% summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx)) %&gt;% mutate(label = \"With Cutadapt\")\nbasic_stats_sum &lt;- basic_stats %&gt;% group_by(stage) %&gt;% summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx)) %&gt;% mutate(label = \"Without Cutadapt\")\nsum_cat &lt;- bind_rows(basic_stats_sum, basic_stats_2_sum) %&gt;% mutate(label=fct_inorder(label))\ng_sum_cat_reads &lt;- ggplot(sum_cat, aes(x=stage, y=n_read_pairs, fill=label)) +\n  geom_col(position=\"dodge\") + \n  scale_fill_brewer(palette = \"Set1\", name=\"Preprocessing\") + \n  scale_y_continuous(name=\"# Read Pairs\", expand=c(0,0)) +\n  theme_kit\ng_sum_cat_reads\n\n\n\n\n\n\n\n\nCodehv_reads_filtered_2_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_filtered_2 &lt;- read_tsv(hv_reads_filtered_2_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_2 &lt;- hv_reads_filtered_2 %&gt;% group_by(sample, group) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_2_summ &lt;- n_hv_filtered_2 %&gt;% ungroup %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Make new labelled HV dataset\nmrg_2 &lt;- hv_reads_filtered_2 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE))\n\n# Merge and plot histogram\nmrg_2_join &lt;- bind_rows(mrg_1 %&gt;% mutate(label=\"Without Cutadapt\"),\n                      mrg_2 %&gt;% mutate(label=\"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label))\n\n\ng_hist_2 &lt;- ggplot(mrg_2_join, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + \n  facet_grid(label~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_2\n\n\n\n\n\n\n\nComparing the lengths of the same sequences in the old versus new results, we see that many reads are reduced in length as a result of adding Cutadapt. As such, it made sense to repeat the BLAST analysis on the reprocessed reads rather than using the BLAST assignments from the previous analysis.\n\nCodemrg_num_comp &lt;- inner_join(mrg_2 %&gt;% ungroup %&gt;% select(-seq_num), \n                           mrg_1_num %&gt;% ungroup %&gt;% select(seq_id, seq_num), by=c(\"seq_id\"))\nmrg_num_diff &lt;- mrg_1_num %&gt;% select(sample, seq_num, query_len_fwd_old = query_len_fwd,\n                                   query_len_rev_old = query_len_rev) %&gt;% \n  inner_join(mrg_num_comp %&gt;% select(sample, seq_num, query_len_fwd_new = query_len_fwd, \n                                  query_len_rev_new = query_len_rev),\n             by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(query_len_fwd_diff = query_len_fwd_new - query_len_fwd_old,\n         query_len_rev_diff = query_len_rev_new - query_len_rev_old)\nmrg_num_diff_long &lt;- mrg_num_diff %&gt;% \n  select(sample, seq_num, Forward=query_len_fwd_diff, Reverse=query_len_rev_diff) %&gt;%\n  pivot_longer(-(sample:seq_num), names_to=\"read\", values_to=\"length_difference\")\ng_len_diff &lt;- ggplot(mrg_num_diff_long, aes(x=length_difference)) +\n  geom_histogram(binwidth=4, boundary=2) +\n  scale_x_continuous(name=\"Effect of Cutadapt on read length\") +\n  scale_y_continuous(name=\"# of Reads\", expand=c(0,0)) +\n  facet_wrap(~read) +\n  theme_base\ng_len_diff\n\n\n\n\n\n\n\n\nCodemrg_2_num &lt;- mrg_2 %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_2_fasta &lt;-  mrg_2_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_2_fasta_out &lt;- do.call(paste, c(mrg_1_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_2_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-2.fasta\"))\n\n\n\nCode# Import BLAST results (again, pre-filtered to save space)\nblast_results_2_path &lt;- file.path(data_dir, \"putative-viral-blast-best-2.tsv.gz\")\nblast_results_2 &lt;- read_tsv(blast_results_2_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_2_best &lt;- blast_results_2 %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_2_ranked &lt;- blast_results_2_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_2_highrank &lt;- blast_results_2_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_2_paired &lt;- blast_results_2_highrank %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_2_viral &lt;- blast_results_2_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_2_assign &lt;- mrg_2_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_2_assign &lt;- full_join(blast_results_2_viral, mrg_2_assign, \n                                    by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_2_out &lt;- blast_results_2_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n\nThe addition of Cutadapt results in a substantial decrease in low-scoring putative HV sequences, resulting in a large improvement in measured sensitivity and F1 score:\n\nCode# Merge BLAST results with unenriched read data\nmrg_2_blast &lt;- full_join(mrg_2_num, blast_results_2_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Combine and label\nhist_blast_2_prep &lt;- bind_rows(mrg_1_blast %&gt;% mutate(label=\"Without Cutadapt\"),\n                               mrg_2_blast %&gt;% mutate(label=\"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label))\n\n# Plot\ng_hist_blast_2 &lt;- ggplot(hist_blast_2_prep, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(label~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_blast_2\n\n\n\n\n\n\n\n\nCodestats_2 &lt;- range_f1(mrg_2_blast) %&gt;% mutate(label = \"With Cutadapt\")\nstats_comb &lt;- bind_rows(stats_1 %&gt;% mutate(label = \"Without Cutadapt\"), stats_2)\n\ng_stats_2 &lt;- ggplot(stats_comb, aes(x=threshold, y=value, color=label, linetype=conj_label)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(NA,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\", name = \"Configuration\") +\n  scale_linetype_discrete(name=\"Threshold type\") +\n  facet_wrap(~metric, nrow=2, scales = \"free_y\") +\n  theme_base\ng_stats_2\n\n\n\n\n\n\n\nAt a disjunctive threshold of 20, excluding “fake true positives” arising from adapter contamination improved measured sensitivity from 86% to 97%, bringing the measured F1 score up from 89% to 95%. I would feel much better about using these results for further downstream analyses.\nThat said, I think further improvement is possible. While addition of Cutadapt processing substantially improved measured sensitivity, measured precision only improved slightly. Looking at the apparent taxonomic makeup of putative HV reads, we see that, while low-scoring “true positives” mapping to herpesviruses have been mostly eliminated, high-scoring false positives still map primarily to parvoviruses and parvo-like viruses:\n\nCode# Calculate composition for 2nd attempt\nmajor_threshold &lt;- 0.1\nfp_2 &lt;- mrg_2_blast %&gt;% group_by(viral_status_out, highscore = adj_score_max &gt;= 20, taxid) %&gt;% \n  count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name),\n         major = p &gt; major_threshold)\nfp_2_major_tab &lt;- fp_2 %&gt;% filter(major) %&gt;% arrange(desc(p))\nfp_2_major_list &lt;- fp_2_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_2_major &lt;- fp_2 %&gt;% mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_2_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Combine composition\nfp_major_comb &lt;- bind_rows(fp_1_major %&gt;% mutate(label = \"Without Cutadapt\"),\n                           fp_2_major %&gt;% mutate(label = \"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label)) %&gt;%\n  arrange(as.character(name_display)) %&gt;% arrange(str_detect(name_display, \"Other\")) %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Palette\npalette_fp_2 &lt;- c(brewer.pal(12, \"Set3\"), \"#999999\")\ng_fp_2 &lt;- ggplot(fp_major_comb, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), breaks = seq(0,1,0.2), expand = c(0,0), label = function(y) y*100) +\n  scale_fill_manual(name = \"Viral\\ntaxon\", values=palette_fp_2) +\n  facet_grid(label~status_display) +\n  guides(fill=guide_legend(nrow=6)) +\n  theme_kit\ng_fp_2\n\n\n\n\n\n\n\nNot only are these parvoviruses and parvo-like viruses DNA viruses (and thus suspicious as abundant components of the RNA virome), they are also the subject of a famous controversy in early viral metagenomics, where viruses apparently widespread in Chinese hepatitis patients were found to arise from spin column contamination. In fact, these viruses seem not to be human-infecting at all; the authors of the study reporting the contamination suggested that they might be viruses of the diatom algae used as the source of silica for these columns. As such, it seems appropriate to remove these from the human-virus database used to identify putative HV reads.\nWe also see a significant number of false-positive reads mapped to Orf virus. Historically this has been a sign of contamination with bovine sequences, and indeed the top taxa mapped to these reads by BLAST include Bos taurus (cattle), Bos mutus (wild yak), and Cervus elaphus (red deer).\n\nCodeorf_taxid &lt;- 10258\norf_seqs &lt;- mrg_1_blast %&gt;% filter(taxid == 10258)\norf_matches &lt;- orf_seqs %&gt;% inner_join(blast_results_1_paired, by=c(\"sample\", \"seq_num\"))\norf_staxids &lt;- orf_matches %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;% mutate(staxid = as.integer(staxid)) %&gt;% left_join(tax_names, by=\"staxid\")\norf_fp_out &lt;- orf_seqs %&gt;% filter(!viral_status_out) %&gt;% arrange(desc(adj_score_max)) %&gt;%\n  ungroup %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", taxid, \"_\", row_number()))\norf_fp_fasta &lt;- orf_fp_out %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\norf_fp_fasta_out &lt;- do.call(paste, c(orf_fp_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(orf_fp_fasta_out, file.path(data_dir, \"spurbeck-orf-fp.fasta\"))\n\n\nThe ideal approach to removing these false positives would be to add the cow genome to the Kraken database we use for validation of Bowtie2 assignments, along with the pig genome and probably some other known contaminants causing issues. This is probably worth doing at some point, but represents a substantial amount of additional work; I’d rather use a simpler alternative right now if one is available. One option that comes to mind is to try replacing the BBMap aligner I’m using to detect and remove contaminants with Bowtie2; in quick experiments, running Bowtie2 (--sensitive) on the putative Orf virus sequences above, using the same dataset of contaminants for the index, successfully removed about two-thirds of them. While this doesn’t guarantee that Bowtie2 would perform as well on the whole population of contaminating cow sequences, it suggests that using Bowtie2 in place of BBMap is worth trying.\nAs such, I re-ran the HV detection pipeline again, having made two changes: first, removing Parvovirus NIH-CQV and the parvo-like viruses from the HV database used to identify putative HV reads, and secondly, replacing BBMap with Bowtie2 for contaminant removal.\nHuman-infecting virus reads: validation, round 3\nI tried re-running the HV detection pipeline with three different sets of Bowtie2 settings: (a) --sensitive, (b) --local --sensitive-local, and (c) --local --very-sensitive-local. In total, these find 26,370, 15,338 and 13,160 putative human-viral reads, respectively, compared to 31,610 for my initial attempt and 19,799 after the addition of Cutadapt:\n\nCodehv_reads_filtered_3_paths &lt;- paste0(data_dir, \"hv_hits_putative_filtered_3\", \n                                    c(\"a\", \"b\", \"c\"), \".tsv.gz\")\nhv_reads_filtered_3_raw &lt;- lapply(hv_reads_filtered_3_paths, read_tsv, show_col_types = FALSE)\nhv_reads_filtered_3 &lt;- lapply(1:3, function(n) hv_reads_filtered_3_raw[[n]] %&gt;% \n                                mutate(attempt=paste0(\"Bowtie2\", c(\"(a)\",\"(b)\",\"(c)\")[n]))) %&gt;% \n  bind_rows() %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_3 &lt;- hv_reads_filtered_3 %&gt;% group_by(sample, group, attempt) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_3_summ &lt;- n_hv_filtered_3 %&gt;% group_by(attempt) %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\nbind_rows(n_hv_filtered_2_summ %&gt;% mutate(attempt=\"BBMap\"), n_hv_filtered_3_summ) %&gt;% \n  select(attempt, everything())\n\n\n  \n\n\n\n\nCode# Make new labelled HV dataset\nmrg_3 &lt;- hv_reads_filtered_3 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE))\n\n# Merge and plot histogram\nmrg_join_3 &lt;- bind_rows(mrg_2 %&gt;% mutate(attempt=\"BBMap\"), mrg_3)\n\n\ng_hist_3 &lt;- ggplot(mrg_join_3, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + \n  facet_grid(attempt~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_3\n\n\n\n\n\n\n\nIn all cases, the great majority of false-positive sequences from previous attempts were successfully removed (85%/89%/90%, respectively), along with a small number of true-positives (0.19%/0.23%/0.23%, respectively). At the same time, a number of new putative HV sequences arose as a result of the change in filtering algorithm: 8,512/2,224/1,166 respectively. The great majority of these (97.6%/99.1%/98.8%) are low-scoring, suggesting that they are mostly or entirely false matches that are being let through by Bowtie2 that were previously being caught by BBMap.\n\nCodemrg_2_blast_prep &lt;- mrg_2_blast %&gt;% ungroup %&gt;% \n  select(seq_id, seq_num, viral_status, viral_status_out)\nmrg_3_blast_old &lt;- mrg_join_3 %&gt;% select(-seq_num) %&gt;% ungroup %&gt;% \n  left_join(mrg_2_blast_prep, by = \"seq_id\") %&gt;%\n  rename(viral_status_old = viral_status) %&gt;%\n  mutate(viral_status_old_out = replace_na(as.character(viral_status_out), \"UNKNOWN\")) %&gt;%\n  select(-viral_status_out)\nmrg_3_blast_old_cum &lt;- mrg_3_blast_old %&gt;%\n  mutate(vs_label = paste0(\"HV status:\\n\", viral_status_old_out),\n         attempt_label = paste(\"Attempt\", attempt))\nmrg_3_blast_old_summ &lt;- mrg_3_blast_old_cum %&gt;% \n  group_by(attempt, viral_status_old_out, highscore = adj_score_max &gt;= 20) %&gt;% count %&gt;%\n  mutate(score_label = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         vs_label = paste(\"HV status:\", viral_status_old_out))\ng_status_summ &lt;- ggplot(mrg_3_blast_old_summ, aes(x=attempt, y=n, fill=attempt)) +\n  geom_col(position=\"dodge\") +\n  facet_wrap(score_label~vs_label, scales = \"free_y\") +\n  scale_y_continuous(name=\"# Read pairs\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  theme_kit\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\nCodeg_status_summ\n\n\n\n\n\n\n\nChecking putative Orf virus reads specifically, all three Bowtie2-based attempts successfully remove most putative Orf reads from the previous attempt, while introducing some number of new putative hits (of which I assume the vast majority are fake hits arising from bovine contamination). In attempt (a), these new putative hits outnumber the old hits that were removed, resulting in a net increase in putative (but, I’m fairly confident, false) Orf-virus hits, whether total or high-scoring. Conversely, attempts (b) and (c) manage to remove even more old putative Orf hits while creating many fewer new ones, resulting in a large net decrease in total and high-scoring putative Orf hits:\n\nCodeorf_seqs_3 &lt;- mrg_3_blast_old_cum %&gt;% filter(taxid == 10258)\norf_seqs_3_summ &lt;- orf_seqs_3 %&gt;% \n  group_by(attempt, viral_status_old_out, highscore = adj_score_max &gt;= 20) %&gt;%\n  count\ng_orf_total &lt;- orf_seqs_3_summ %&gt;% group_by(attempt) %&gt;% summarize(n=sum(n)) %&gt;%\n  ggplot(aes(x=attempt, y=n, fill=attempt)) + geom_col() +\n  scale_y_continuous(name=\"Total reads mapped to Orf virus\", limits=c(0,250), expand=c(0,0)) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  theme_kit\ng_orf_high &lt;- orf_seqs_3_summ %&gt;% filter(highscore) %&gt;%\n  ggplot(aes(x=attempt, y=n, fill=attempt)) + geom_col() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  scale_y_continuous(name=\"High-scoring reads mapped to Orf virus\",\n                     limits=c(0,250), expand=c(0,0)) +\n  theme_kit\ng_orf_total + g_orf_high\n\n\n\n\n\n\n\nNext, I validated all new putative HV sequences with BLASTN as before, then evaluated each attempt’s performance against all the putative HV reads identified by any attempt:\n\nCodemrg_3_fasta_prep &lt;- mrg_join_3 %&gt;%\n  filter(! seq_id %in% mrg_2_blast$seq_id) %&gt;%\n  group_by(seq_id) %&gt;% filter(row_number() == 1) %&gt;% ungroup\nmrg_3_fasta &lt;- mrg_3_fasta_prep %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_3_fasta_out &lt;- do.call(paste, c(mrg_2_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_3_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-3.fasta\"))\n\n\n\nCode# Import BLAST results (again, pre-filtered to save space)\n# blast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\n# blast_results_3_path_0 &lt;- file.path(data_dir, \"spurbeck-putative-viral-3.blast.gz\")\n# blast_results_3 &lt;- read_tsv(blast_results_3_path_0, show_col_types = FALSE,\n#                           col_names = blast_cols, col_types = cols(.default=\"c\"))\n\nblast_results_3_path &lt;- file.path(data_dir, \"putative-viral-blast-best-3.tsv.gz\")\nblast_results_3 &lt;- read_tsv(blast_results_3_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_3_best &lt;- blast_results_3 %&gt;% group_by(qseqid, staxid) %&gt;%\n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_3_ranked &lt;- blast_results_3_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_3_highrank &lt;- blast_results_3_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=2), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_3_paired &lt;- blast_results_3_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_3_viral &lt;- blast_results_3_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_3_assign &lt;- mrg_3_fasta_prep %&gt;% select(seq_id, taxid, assigned_taxid)\nblast_results_3_assign &lt;- full_join(blast_results_3_viral, mrg_3_assign, by=c(\"seq_id\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_3_out &lt;- blast_results_3_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status_new = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status_new = replace_na(viral_status_new, 0))\n\n\n\nCodemrg_3_blast_new &lt;- mrg_3_blast_old %&gt;%\n  left_join(blast_results_3_out, by=\"seq_id\")\nmrg_3_blast &lt;- mrg_3_blast_new %&gt;%\n  mutate(viral_status = pmax(viral_status_old, viral_status_new, na.rm = TRUE),\n         viral_status = replace_na(viral_status, 0),\n         viral_status_out = viral_status &gt; 0)\nmrg_3_blast_fill_fwd &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, adj_score_fwd) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"adj_score_fwd\", values_fill=0) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"adj_score_fwd\")\nmrg_3_blast_fill_rev &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, adj_score_rev) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"adj_score_rev\", values_fill=0) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"adj_score_rev\")\nmrg_3_blast_fill_ass &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, assigned_hv) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"assigned_hv\", values_fill=FALSE) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"assigned_hv\")\nmrg_3_blast_fill_hit &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, hit_hv) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"hit_hv\", values_fill=FALSE) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"hit_hv\")\nmrg_3_blast_fill_ref &lt;- mrg_3_blast %&gt;% group_by(seq_id, sample, viral_status_out, taxid) %&gt;% summarize(.groups = \"drop\") %&gt;% group_by(seq_id, sample, viral_status_out) %&gt;% summarize(taxid = taxid[1], .groups = \"drop\")\nmrg_3_blast_fill &lt;- full_join(mrg_3_blast_fill_fwd, mrg_3_blast_fill_rev, \n                              by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_ass, by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_hit, by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_ref, by=\"seq_id\") %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n\n\n\nCodeattempts &lt;- mrg_3_blast_fill %&gt;% group_by(attempt) %&gt;% summarize %&gt;% pull(attempt)\nstats_3_raw &lt;- mrg_3_blast_fill %&gt;% group_by(attempt) %&gt;% group_split %&gt;%\n  lapply(range_f1)\nstats_3 &lt;- lapply(1:4, function(n) stats_3_raw[[n]] %&gt;% mutate(attempt=attempts[n])) %&gt;%\n  bind_rows\ng_stats_3 &lt;- ggplot(stats_3, aes(x=threshold, y=value, color=attempt, linetype=conj_label)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(NA,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\", name=\"Configuration\") +\n  scale_linetype_discrete(name=\"Threshold type\") +\n  facet_wrap(~metric, nrow=2, scales = \"free_y\") +\n  guides(color=guide_legend(nrow=2), linetype=guide_legend(nrow=2)) +\n  theme_base\ng_stats_3\n\n\n\n\n\n\n\nAt a disjunctive threshold of 20, Bowtie2(c) (--very-sensitive-local) performs the best, with an F1 score of 0.979; Bowtie2(b) (--sensitive-local) is very close behind. In both cases, precision (&gt;=0.988) is higher than sensitivity (~0.969), suggesting that any further improvements would come from investigating low-scoring true-positives:\n\nCodemajor_threshold &lt;- 0.1\nfp_3 &lt;- mrg_3_blast %&gt;% group_by(attempt, viral_status_out, \n                           highscore = adj_score_max &gt;= 20, taxid) %&gt;% \n  count %&gt;% \n  group_by(attempt, viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name),\n         major = p &gt; major_threshold)\nfp_3_major_tab &lt;- fp_3 %&gt;% filter(major) %&gt;% arrange(desc(p))\nfp_3_major_list &lt;- fp_3_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_3_major &lt;- fp_3 %&gt;% mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(attempt, viral_status_out, highscore, name_display) %&gt;% summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_3_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = paste(\"VS:\", viral_status_out)) #ifelse(viral_status_out, \"True positive\", \"False positive\"))\npalette_fp_3 &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(3, \"Dark2\"), \"#999999\")\ng_fp_3 &lt;- ggplot(fp_3_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values=palette_fp_3, name = \"Viral\\ntaxon\") +\n  facet_grid(attempt~status_display) +\n  guides(fill=guide_legend(nrow=8)) +\n  theme_kit\ng_fp_3\n\n\n\n\n\n\n\nI suspect that small additional gains are possible here, since I’m suspicious about the low-scoring “true-positives” mapping to human betaherpesvirus 5 and human mastadenovirus F. However, I’ve already spent a long time optimizing the results for this dataset, and the results are now good enough that I feel okay with leaving this here. Going forward, I’ll use Bowtie2(c) as my alignment strategy for the HV identification pipeline.\nHuman-infecting virus reads: analysis\nAfter several rounds of validation and refinement, we finally come to actually analyzing the human-infecting virus content of Spurbeck et al. This section might seem disappointingly short compared to all the effort expended to get here.\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, group, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_3 %&gt;% filter(attempt == \"Bowtie2(c)\") %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate\nread_counts_group &lt;- read_counts %&gt;% group_by(group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nread_counts_total &lt;- read_counts_group %&gt;% ungroup %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         group = \"All groups\")\nread_counts_agg &lt;- read_counts_group %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  arrange(group) %&gt;% arrange(str_detect(group, \"All groups\")) %&gt;%\n  mutate(group = fct_inorder(group))\n\n\nApplying a disjunctive cutoff at S=20 identifies 9,990 reads as human-viral out of 1.84B total reads, for a relative HV abundance of \\(5.44 \\times 10^{-6}\\). This compares to \\(2.8 \\times 10^{-4}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a roughly 2x increase, smaller than the 4-5x increases seen for Crits-Christoph and Rothman. Relative HV abundances for individual sample groups ranged from \\(9.19 \\times 10^{-8}\\) to \\(1.58 \\times 10^{-5}\\); as with total virus reads, groups F, G & H showed the highest relative abundance:\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=group, y=p_reads_hv, color=group)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette = \"Set3\", name = \"Group\") +\n  theme_kit\ng_phv_agg\n\n\n\n\n\n\n\nDigging into particular viruses, we see that Mamastrovirus, Mastadenovirus, Rotavirus and Betacoronavirus are the most abundant genera across samples:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 694009] &lt;- \"SARS-CoV\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 568715] &lt;- \"Astrovirus MLB1\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(group, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(group, n_reads_raw), by=\"group\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(group = \"All groups\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_genera &lt;- 5\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(group) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display)) %&gt;%\n  arrange(str_detect(group, \"Other\")) %&gt;%\n  mutate(group = fct_inorder(group))\n\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"), \"#888888\")\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=group, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(ncol=3)) +\n  theme_kit\ng_vcomp_genera\n\n\n\n\n\n\n\nMamastrovirus and Rotavirus are predominantly enteric RNA viruses whose prominence here makes sense, while the prevalence of Betacoronavirus is probably a result of the ongoing SARS-CoV-2 pandemic. As a DNA virus, the abundance of Mastadenovirus is a bit more surprising; however, this finding is consistent with the public dashboard, and is also borne out by the other BLASTN matches for these sequences, all of which are other adenovirus taxa:\n\nCodemasta_ids &lt;- hv_reads_genera %&gt;% filter(name==\"Mastadenovirus\") %&gt;% pull(seq_id)\nmasta_hits &lt;- bind_rows(blast_results_2_paired %&gt;% full_join(mrg_2_blast %&gt;% select(seq_id, sample, seq_num), by=c(\"sample\", \"seq_num\")),\n                        blast_results_3_paired) %&gt;%\n  select(-sample, -seq_num) %&gt;%\n  filter(seq_id %in% masta_ids) %&gt;%\n  mutate(staxid = as.integer(staxid))\nmasta_hits_sorted &lt;- masta_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name = fct_inorder(name))\nmasta_hits_sorted_head &lt;- masta_hits_sorted %&gt;% head(10) %&gt;%\n  mutate(name=fct_inorder(as.character(name)))\ng_masta &lt;- ggplot(masta_hits_sorted_head,\n                  aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name = \"# Mapped read pairs\") + theme_base +\n  theme(axis.title.y = element_blank())\ng_masta\n\n\n\n\n\n\n\nConclusion\nCompared to the last few datasets I analyzed, the analysis of Spurbeck took a long time, numerous attempts, and a lot of computational resources. However, as I said at the start of this post, I’m happy with the outcome and am confident it will improve analysis of future datasets. While the overall prevalence of human-infecting viruses is fairly low in Spurbeck compared to other wastewater datasets I’ve looked at, its inclusion as a core dataset for the P2RA analysis make it especially important to process in a reliable and high-quality manner.\nNext, I’ll turn my attention to more datasets included in the P2RA analysis, as well as some air-sampling datasets we’re interested in for another project."
  }
]