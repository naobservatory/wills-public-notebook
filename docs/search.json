[
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/280",
    "text": "Nanodrop 260/280\nThe standard measures of nucleic acid purity from Nanodrop measurements are the 260/280 and 260/230 absorption ratios. The standard advice is that, for a pure RNA sample, 260/280 should be around 2.0, while pure DNA should be around 1.8. Lower ratios are typically indicative of contamination with protein, or some specific contaminants such as phenol.\nThe 260/280 ratios measured for the DNase-treated samples are as follows:\n\nCodeg_260_280 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.8, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 1.9, ymax = 2.1, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_280), shape = 16) +\n  scale_y_continuous(limits = c(0.8, 2.3), breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/280nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_280\n\n\n\nFigure 5: 260/280 ratios for DNase-treated samples.\n\n\n\nNone of the kits returned 260/280 ratios at the expected level for pure RNA, though most were not drastically low. Several returned ratios close to (though below) 1.8, which might be indicative of significant residual DNA in the sample; it’s hard to distinguish between this and protein contamination without doing DNA Qubit measurements on the DNase-treated samples. Free dNTP nucleotides produced by the DNase treatment still absorb at DNA-like ratios, so DNA-like readings may not be indicative of poor quality here.\nAs with yield, the Qiagen AllPrep PowerViral kit does notably worse than the other kits."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/230",
    "text": "Nanodrop 260/230\nThe 260/230 absorption ratio gives a useful indicator of the presence of various contaminants, including phenol, carbohydrates, guanidine, and various salts. The standard advice is that a pure sample should have a 260/230 of around 2.0-2.2, with lower ratios indicating contamination. Readings for our DNase-treated samples are as follows:\n\nCodeg_260_230 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.9, ymax = 2.3, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 2.0, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_230), shape = 16) +\n  scale_y_continuous(#limits = c(0.8, 2.3),\n                     breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/230nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_230\n\n\n\nFigure 6: 260/230 ratios for DNase-treated samples.\n\n\n\nAs we can see, many kits have significantly lower 260/230 ratios than the recommended range, indicating contamination – most likely with guanidinium salts involved in nucleic acid extraction. Combined with the 260/280 results above, this suggests that many kits would benefit from an additional cleanup step. As with other metrics, the Qiagen AllPrep PowerViral kit comes out looking worst.\nTwo kits (NucleoSpin Virus and Zymo quick-RNA) have at least one replicate with significantly higher 260/230 ratios than the recommended values. It’s not very clear what could cause this, but one resource I found suggested misblanking as the likely culprit. I’d recommend repeating the nanodrop measurements for these samples to see if the pattern persists."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop/Qubit ratio",
    "text": "Nanodrop/Qubit ratio\nAn informal heuristic measurement of quality is to compare the measured RNA concentration with Qubit (which is highly specific) to Nanodrop (which is not); the higher the latter compared to the former, the more other material is likely contributing to the Nanodrop reading. The ratios for our DNase-treated samples are as follows:\n\nCodeg_ratio &lt;- ggplot(data, aes(x=kit,y=qubit_nanodrop_ratio)) +\n  geom_point(shape = 16) + \n  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), \n                     name = \"RNA concentration ratio (Qubit/Nanodrop)\", expand = c(0,0)) +\n  scale_x_discrete(name = \"Extraction Kit\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_ratio\n\n\n\nFigure 7: Nanodrop/Qubit ratios for DNase-treated samples.\n\n\n\nAccording to this metric, the Invitrogen PureLink RNA, QIAamp MinElute Virus, and Zymo quick-RNA kits come out looking relatively good, while the Qiagen AllPrep PowerViral kit comes out looking worst."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will's Public NAO Notebook",
    "section": "",
    "text": "Investigating the v2 pipeline’s human-virus assignment behavior\n\n\nChecking treatment of partially-human-infecting virus taxa\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up AWS Batch to work with the NAO’s MGS workflow\n\n\nA hopefully-simple guide to an unfortunately-complicated service.\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Munk et al. (2022)\n\n\nA global wastewater study.\n\n\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Ng et al. (2019)\n\n\nWastewater from Singapore.\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Bengtsson-Palme et al. (2016)\n\n\nWastewater grab samples from Sweden.\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Maritz et al. (2019)\n\n\nWastewater from NYC.\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Brinch et al. (2020)\n\n\nWastewater from Copenhagen.\n\n\n\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Leung et al. (2021)\n\n\nAir sampling from urban public transit systems.\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Rosario et al. (2018)\n\n\nAir sampling from a student dorm in Colorado.\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Prussin et al. (2019)\n\n\nAir filters from a daycare in Virginia.\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Brumfield et al. (2022)\n\n\nWastewater from a manhole in Maryland.\n\n\n\n\n\nApr 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Spurbeck et al. (2023)\n\n\nCave carpa.\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFollowup analysis of Yang et al. (2020)\n\n\nDigging into deduplication.\n\n\n\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Yang et al. (2020)\n\n\nWastewater from Xinjiang.\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImproving read deduplication in the MGS workflow\n\n\nRemoving reverse-complement duplicates of human-viral reads.\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Rothman et al. (2021), part 2\n\n\nPanel-enriched samples.\n\n\n\n\n\nFeb 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Rothman et al. (2021), part 1\n\n\nUnenriched samples.\n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 3\n\n\nFixing the virus pipeline.\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 2\n\n\nAbundance and composition of human-infecting viruses.\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Crits-Christoph et al. (2021), part 1\n\n\nPreprocessing and composition.\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating BLAST validation of human viral read assignment\n\n\nExperiments with BLASTN remote mode\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProject Runway RNA-seq testing data: removing livestock reads\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow analysis of Project Runway RNA-seq testing data\n\n\nApplying a new workflow to some oldish data.\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the effect of read depth on duplication rate for Project Runway DNA data\n\n\nHow deep can we go?\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing viral read assignments between pipelines on Project Runway data\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInitial analysis of Project Runway protocol testing data\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing options for read deduplication\n\n\nClumpify vs fastp\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Ribodetector and bbduk for rRNA detection\n\n\nIn search of quick rRNA filtering.\n\n\n\n\n\nOct 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\n\n\nTwo tools – how do they perform?\n\n\n\n\n\nOct 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow does Element AVITI sequencing work?\n\n\nFindings of a shallow investigation\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExtraction experiment 2: high-level results & interpretation\n\n\nComparing RNA yields and quality across extraction kits for settled solids\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "href": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "title": "Comparing FASTP and AdapterRemoval for MGS pre-processing",
    "section": "",
    "text": "The first major step in our current MGS pipeline uses AdapterRemoval to automatically identify and remove sequencing adapters, as well as trimming low-quality bases and collapsing overlapping read pairs (it can also discard low-quality reads entirely, but our current pipeline doesn’t use this). An alternative tool, that can do all of this as well as read deduplication, is fastp. I asked the pipeline’s current primary maintainer if there was a good reason we were using one tool instead of the other, and he said that there wasn’t. So I decided to do a shallow investigation of their relative behavior on some example MGS datasets to see how they compare.\nThe data\nTo carry out this test, I selected three pairs of raw Illumina FASTQC files, corresponding to one sample each from two different published studies as well as one dataset provided to us by Marc Johnson:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR19607374\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nFor each sample, I generated FASTQC report files for the raw data, then ran FASTP and AdapterRemoval independently on the FASTQ files and tabulated the results\nThe commands\nFor processing with FASTP, I ran the following command:\nfastp -i &lt;raw-reads-1&gt; -I &lt;raw-reads-2&gt; -o &lt;output-path-1&gt; -O &lt;output-path-2&gt; --failed_out &lt;output-path-failed-reads&gt; --cut_tail --correction\n(I didn’t run deduplication for this test, as AdapterRemoval doesn’t have that functionality.)\nFor processing with AdapterRemoval, I first ran the following command to identify adapters:\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --identify-adapters --threads 4 &gt; adapter_report.txt\nI then ran the following command to actually carry out pre-processing, using the adapter sequences identified in the previous step (NB the minlength and maxns values are chosen to match the FASTP defaults):\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --basename &lt;output-prefix&gt; --adapter1 &lt;adapter1&gt; --adapter2 &lt;adapter2&gt; --gzip --trimns --trimqualities --minlength 15 --maxns 5\n1. Rothman et al. (SRR19607374)\nThis sample from Rothman et al. contains 11.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 39 seconds.\nFASTP detected and trimmed adapters on 3.88M reads (note: not read pairs).\nA total of 133 Mb of sequence was trimmed due to adapter trimming, and 55 Mb due to other trimming processes, for a total of 188 Mb of trimmed sequence.\nA total of 367,938 read pairs were discarded due to failing various filters, leaving 11.21M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 323.9 seconds (a bit under 5.5 minutes).\nAdapterRemoval detected and trimmed adapters on 3.96M reads (note: again, not read pairs).\nA total of 135 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 2,347 read pairs were discarded due to failing various filters, leaving the final read number almost unchanged.\n\n\nCode# Calculate read allocations for Rothman\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed = c(1748896043+1748896043,1627794563+1627794563,1681118328+1681115431)\nbp_discarded = c(0,54014538,276107+271850)\nbp_trimmed = c(0,bp_passed[1]-bp_passed[2]-bp_discarded[2],bp_passed[1]-bp_passed[3]-bp_discarded[3])\n# Tabulate\ntab_rothman &lt;- tibble(status = status, bp_passed = bp_passed, bp_discarded = bp_discarded, bp_trimmed = bp_trimmed)\ntab_rothman\n\n\n\n  \n\n\nCode# Visualize\ntab_rothman_gathered &lt;- gather(tab_rothman, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_rothman &lt;- ggplot(tab_rothman_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_rothman\n\n\n\n\nFASTQC results:\n\nPrior to adapter removal with either tool, the sequencing reads appear good quality, with a consistent average quality score of 30 across all bases in the forward read and ~29 in the reverse read. FASTP successfully raises the average quality score in the reverse read to 30 through trimming and read filtering, while AdapterRemoval leaves it unchanged.\nFASTQC judges the data to have iffy sequence composition (%A/C/G/T); neither tool affects this much.\nAll reads in the raw data are 151bp long; unsurprisingly, trimming by both tools results in a left tail in the sequencing length distribution that was absent in the raw data.\nAs previously observed, the raw data has very high duplicate levels, with only ~26% of sequences estimated by FASTQC to remain after deduplication. Increasing the comparison window to 100bp (from a default of 50bp) increases this to ~35%. Neither tool has much effect on this number – unsurprisingly, since neither carried out deduplication.\nFinally, adapter removal. Unsurprisingly, the raw data shows substantial adapter content. AdapterRemoval does a good job of removing adapters, resulting in a “pass” grade from FASTQC. Surprisingly, despite trimming adapters from fewer reads, fastp does even better (according to FASTQC) at removing adapters.\n\nThe images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n2. Crits-Christoph et al. (SRR23998357)\nThis sample from Rothman et al. contains 48.46M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 99 seconds.\nFASTP detected and trimmed adapters on 13.41M reads (note: not read pairs).\nA total of 270 Mb of sequence was trimmed due to adapter trimming, and 43 Mb due to other trimming processes, for a total of 313 Mb of trimmed sequence.\nA total of 1.99M read pairs were discarded due to failing various filters, leaving 47.47M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 1041.3 seconds (a bit over 17 minutes).\nAdapterRemoval detected and trimmed adapters on 8.22M reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 32,381 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\n\nCode# Calculate read allocations for CritsCristoph\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_cc = c(3683175308+3683175308,3465517525+3467624847,3634186441+3634143668)\nbp_discarded_cc = c(0,120701257,2057612+2224529)\nbp_trimmed_cc = c(0,bp_passed_cc[1]-bp_passed_cc[2]-bp_discarded_cc[2],bp_passed_cc[1]-bp_passed_cc[3]-bp_discarded_cc[3])\n# Tabulate\ntab_cc &lt;- tibble(status = status, bp_passed = bp_passed_cc, bp_discarded = bp_discarded_cc, bp_trimmed = bp_trimmed_cc)\ntab_cc\n\n\n\n  \n\n\nCode# Visualize\ntab_cc_gathered &lt;- gather(tab_cc, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_cc_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nFASTQC results:\n\nAs with Rothman, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp in this case) and high numbers of duplicates. They also, unsurprisingly, have high adaptor content.\nAs with Rothman, fastp successfully improves read quality scores, while AdapterRemoval has little effect. Also as with Rothman, neither tool (as configured) has much effect on sequence composition or duplicates.\n\nIn this case, fastp is highly effective at removing adapter sequences, while AdapterRemoval is only weakly effective. I wonder if I misconfigured AR somehow, because I’m surprised at how many adapter sequences remain in this case. The images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n3. Johnson (COMO4)\nThis sample from Johnson contains 15.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 33 seconds.\nFASTP detected and trimmed adapters on 158,114 reads (note: not read pairs).\nA total of 1.3 Mb of sequence was trimmed due to adapter trimming, and 14.6 Mb due to other trimming processes, for a total of 15.9 Mb of trimmed sequence.\nA total of 0.33M read pairs were discarded due to failing various filters, leaving 15.25M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 311.4 seconds (a bit over 5 minutes).\nAdapterRemoval detected and trimmed adapters on 155,360 reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 5,512 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\nFASTQC results:\n\nAs with previous samples, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp again) and high numbers of duplicates.\nUnlike previous samples, the raw data for this sample shows very low adapter content – plausibly they underwent adapter trimming before they were sent to us?\nNeither tool achieves much visible improvement on adapter content – unsurprisingly, given the very low levels in the raw data.\n\n\nCode# Calculate read allocations for Johnson\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_como = c(1183987584+1183987584,1157539804+1157540364,1182999562+1182970312)\nbp_discarded_como = c(0,36950446,418230+257734)\nbp_trimmed_como = c(0,bp_passed_como[1]-bp_passed_como[2]-bp_discarded_como[2],bp_passed_como[1]-bp_passed_como[3]-bp_discarded_como[3])\n# Tabulate\ntab_como &lt;- tibble(status = status, bp_passed = bp_passed_como, bp_discarded = bp_discarded_como, bp_trimmed = bp_trimmed_como)\ntab_como\n\n\n\n  \n\n\nCode# Visualize\ntab_como_gathered &lt;- gather(tab_como, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_como_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nDeduplication with fastp\nGiven that all three of these samples contain high levels of sequence duplicates, I was curious to see to what degree fastp was able to improve on this metric. To test this, I reran fastp on all three samples, with the --dedup option enabled. I observed the following:\n\nRuntimes were consistently very slightly longer than without deduplication.\nThe number of successful output reads declined from 11.21M to 9.29M for the Rothman sample, from 47.47M to 31.47M, and from 15.25M to 11.03M for the Johnson sample.\nRelative to the raw data, and using the default FASTQC settings, the predicted fraction of reads surviving deduplication rose from 26% to 29% for the Rothman sample, from 45% to 64% for the Crits-Cristoph sample, and from 26% to 32% for the Johnson sample, following fastp deduplication. That is to say, by this metric, deduplication was mildly but not very effective.\nThis relative lack of efficacy may simply be because FASTP identifies duplicates as read pairs that are entirely identical in sequence, while FASTQC only looks at the first 50 base pairs of each read in isolation.\nI think I need to learn more about read duplicates and deduplication before I have strong takeaways here.\nConclusions\nTaken together, I think these data make a decent case for using FASTP, rather than AdapterRemoval, for pre-processing and adapter trimming.\n\nFASTP is much faster than AdapterRemoval.\nFor those samples with high adapter content, FASTP appeared more effective than AdapterRemoval at removing adapters, at least for those adapter sequences that could be detected by FASTQC.\nFASTP appears to be more aggressive at quality trimming reads than AdapterRemoval, resulting in better read quality distributions in FASTQC.\nFASTP provides substantially more functionality than AdapterRemoval, making it easier for us to add additional preprocessing steps like read filtering and (some) deduplication down the line.\n\nHowever, one important caveat is that it’s unclear how well either tool will perform on Element sequencing data – or how well FASTQC will be able to detect Element adapters that remain after preprocessing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "title": "How does Element AVITI sequencing work?",
    "section": "",
    "text": "In September 2023, the NAO team sent several samples to the MIT BioMicro Center, for library preparation and sequencing using their new Element AVITI sequencer. This machine works on quite different principles from Illumina sequencing, but also produces high-volume, paired-end, high-accuracy short reads. Since it looks like we might be using this machine quite a lot in the future, it pays to understand what it's doing. However, I found most quick explanations of Element sequencing much harder to follow than equivalent explanations of Illumina's sequencing technology (e.g. here).\nTo try and understand this better, I dug deeper, using a combination of talks by Element staff on YouTube, their core methods paper, and aggressive interrogation of Claude 2. Given my difficulty understanding this, I figured others on the team might also benefit from a quick-ish write-up of my current best understanding, presented here. Note that this does not go into the performance of Element sequencing, only the underlying mechanisms. Note also that, given the lack of very detailed documentation about many aspects of the process, my understanding here is inevitably more high-level than it would be for e.g. Illumina sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "title": "How does Element AVITI sequencing work?",
    "section": "4a. Background and justification",
    "text": "4a. Background and justification\n\nWhen a polymerase binds a DNA strand, it first positions itself over the boundary between the double-stranded primer region and the single-stranded template region. It then recruits and positions a nucleotide complementary to the first base of the template region, using a combination of base pairing and direct interactions between the nucleotide and the polymerase enzyme itself. Finally, it incorporates the new nucleotide into the elongating daughter strand by connecting it to the end of that strand via a new phosphodiester bond.\n\nUsually, the polymerase then repeats the cycle by recruiting and incorporating a nucleotide complementary to the next base of the template strand; however, if the incorporated nucleotide is a chain terminator, it is unable to do this, and stalls.\n\nIn Illumina sequencing, the terminator nucleotide incorporated by the polymerase is fluorescently labeled, and is imaged following incorporation. The fluorophore is then cleaved off along with the terminator group, and the cycle repeats. As a result, the process of daughter strand elongation and base calling are closely bound together.\nIn Element sequencing, the goal is to separate the processes of daughter strand elongation (above) and base calling, so that the two can be optimized separately. To achieve this, the aim is to call the next unincorporated position in the template sequence, rather than (as in Illumina sequencing) the most recently incorporated position.\nOne theoretical way to do this would be to use an engineered polymerase that is able to recruit complementary nucleotides but not incorporate them. One could supply this polymerase with fluorescent nucleotides, and it would recruit the one complementary to the next position on the template strand. This would occur simultaneously at many different locations on each polony, corresponding to the different copies of the library sequence produced by RNA. One could then image the flow cell to identify the nucleotide type recruited at each polony.\nThe problem with the above approach is low signal persistence. Without incorporation, recruitment of nucleotides by the polymerase is weak and transient: the nucleotide binds its complementary base and the polymerase, remains for a short time, then dissociates. The result is that, for any given polony, too few nucleotides are recruited at any one time to give a sufficient signal for imaging.\nIn order for an approach like this to work, then, we need a way to improve signal persistence without relying on covalent incorporation of nucleotides. Enter avidity sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "title": "How does Element AVITI sequencing work?",
    "section": "4b. Base calling by avidity",
    "text": "4b. Base calling by avidity\n\nThe avidity of a molecular interaction is the accumulated strength of that interaction across multiple separate noncovalent bonds. Even if any single one of these bonds is weak and transient, the overall interaction can be strong and stable if the two molecules interact at many different points.\nIn Element avidity sequencing, the avidite is a large molecular construct, comprising a fluorescently labeled protein core connected to some number of (identical) nucleotides via flexible linker regions. Each of these nucleotide groups can be independently recruited by a polymerase bound to a polony, and positioned based on base-pairing interactions. While each of these nucleotide:template:polymerase interactions is too weak and transient to sustain a strong signal, the avidite as a whole is bound to the polony via multiple such interactions, producing a strong and stable interaction overall.\n\n\nExample avidite structure from the avidity sequencing paper. The core of the molecule consists of fluorescently labeled streptavidin, bound to linker regions via streptavidin:biotin interactions. Three of the four linkers shown here end in nucleotides (specifically, adenosine); the fourth mediates core:core interactions to produce an even larger avidite complex.\n\nExample avidite arm structure, with biotin at one end (top-left) and adenosine at the other (bottom-right).\n\nThe base-calling phase of the avidity sequencing cycle thus proceeds as follows:\n\nPrior to the base-calling phase, the polymerase and nucleotides involved in the elongation phase are detached and washed away.\nThe flow cell is then washed with a mixture containing an engineered polymerase as well as four fluorescently-labeled avidites (one each for A, C, G and T). The engineered polymerase (henceforth the avidite-binding polymerase, or ABP) is distinct from that used for elongation, and is capable of binding a template strand and recruiting a complementary nucleotide, but not capable of incorporation.\nThe ABPs bind to the double-stranded regions of each polony and position themselves at the boundary with the single-stranded template region. They then attempt to recruit nucleotides complementary to the next position on the template strand. The only nucleotides available are those attached to the avidites, which are thus recruited. \nSince each copy of the template sequence in each polony is synchronized, each polymerase bound to each polony attempts to recruit the same nucleotide type, and thus interacts with the same type of avidite. Each avidite molecule is thus recruited to multiple points on the polony, producing a stable overall interaction.\n\n\n\nMultiple copies of the same avidite molecule are thus recruited to each polony, producing a strong and uniform fluorescent signal.\n\n\n\nThe flow cell is then imaged to identify the avidite bound to each polony, and thus the next nucleotide in each read. After this, the ABPs and avidites are detached and washed away, and the cycle proceeds to the next elongation phase (see above)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html",
    "href": "notebooks/2023-10-13_rrna-removal.html",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "",
    "text": "See also:\nA useful step in processing wastewater MGS data is the removal of (primarily bacterial) ribosomal RNA sequences. These often make up a substantial fraction of all reads in the dataset, and their removal can both speed up downstream processing and potentially improve certain downstream metrics (e.g. library complexity). Our current pipeline counts and lists rRNA reads using Ribodetector, a deep-learning-based tool that is sensitive and specific, but slow. This slowness makes it annoying to work Ribodetector into our broader pipeline – for example, the time taken to classify reads with Ribodetector is much more than the time saved on downstream steps by excluding rRNA reads from our pipeline.\nI wanted to see if there were alternative rRNA detection methods that gave good-enough results while being fast enough to include in the preprocessing phase of the pipeline. To that end, I investigated bbduk, a k-mer based contamination-detection approach suggested in this biostars thread1.\nTo compare these tools, I applied bbduk and Ribodetector to three samples from pre-existing wastewater metagenomics datasets. Two of these, from Johnson and Crits-Christoph, were the same as in my last methods comparison post. I initially also used the same sample from the third dataset, Rothman et al. (2021), but switched to a different sample when I realised the first had undergone panel enrichment for respiratory viruses and so wasn’t truly untargeted.\nFor each sample, I ran rRNA removal on the FASTQ files that had been preprocessed with FASTP (without deduplication. For SortMeRNA and bbduk, I used reference files (1,2) downloaded from the SILVA database.\nThe commands I ran were as follows:"
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "href": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "High-level output",
    "text": "High-level output\nRunning Ribodetector in high-MCC mode set took 1266 seconds (just over 21 minutes). The tool identified 6.87M out of 15.25M read pairs as ribosomal: 45.0%. Re-running in high-recall mode took a similar amount of time (1264 seconds) and identified 7.76M reads as ribosomal: 50.9%.\nRunning bbduk took a total of 36 seconds. The tool identified 7.57M out of 15.25M read pairs as ribosomal: 49.63%, a little under the result for Ribodetector’ high-recall mode.\nThe default bbduk command given above uses the default k-mer size of 27. Increasing k will increase the precision and decrease the recall of the bbduk algorithm, potentially moving the results closer to the high-MCC version of Ribodetector. After some experimentation, I found that a k-mer length of 43 returned a ribosomal fraction of 45.39%, only slightly above high-MCC ribodetector."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "href": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Overlap between tools",
    "text": "Overlap between tools\nTo compare the output of Ribodetector (high-MCC and high-recall) and bbduk (low and high k) in more detail, I extracted and downloaded the read IDs from their respective rRNA files and compared the overlap between the lists of IDs:\n\nCodedata_dir &lt;- \"../data/2023-10-16_ribodetection/\"\njohnson_bbduk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_failed_ids.txt.gz\")\njohnson_bbduk_highk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_highk_failed_ids.txt.gz\")\njohnson_rd_precision_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_failed_ids_1.txt.gz\")\njohnson_rd_recall_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_recall_failed_ids_1.txt.gz\")\njohnson_bbduk_reads &lt;- readLines(johnson_bbduk_reads_path)\njohnson_bbduk_highk_reads &lt;- readLines(johnson_bbduk_highk_reads_path)\njohnson_rd_precision_reads &lt;- readLines(johnson_rd_precision_reads_path)\njohnson_rd_recall_reads &lt;- readLines(johnson_rd_recall_reads_path)\njohnson_read_ids &lt;- list(bbduk = johnson_bbduk_reads,\n                 bbduk_highk = johnson_bbduk_highk_reads,\n                 rd_precision = johnson_rd_precision_reads,\n                 rd_recall = johnson_rd_recall_reads)\n\n\n\nCodeg_venn_johnson &lt;- ggVennDiagram(johnson_read_ids, label_alpha=0, edge_size = 0) +\n  scale_fill_gradient(low = \"#FFFFFF\", high = \"#4981BF\")\ng_venn_johnson\n\n\n\n\nSome observations:\n\nAs expected, the rRNA reads returned by bbduk with a high k-value are a strict subset of those returned with a low k-value, and those returned by Ribodetector in high-precision mode are a strict subset of those returned in high-recall mode.\n\nAcross all read IDs identified as ribosomal under any of the four conditions, 82% (6.42M read pairs) were identified as such by all four conditions. Of those that remain:\n\n5% are identified by all conditions except high-k bbduk;\n6% are identified by all conditions except high-precision Ribodetector;\n2% are identified by both high-recall conditions, but neither high-precision condition;\n3% are identified by high-recall Ribodetector only;\n1% are identified by low-k bbduk only;\n1% by some other combination of conditions\n\n\nOverall, it seems that, while the two high-recall methods exhibit quite good agreement (sharing &gt;95% of identified sequences), the two high-precision methods agree less well (with &gt;10% of all identified sequences identified by one but not the other)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "href": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Footnotes",
    "text": "Footnotes\n\nI also began investigating SortMeRNA, a published tool based on heuristic alignment, which generally appeared to perform second-best on the quality metrics from the Ribodetector paper. However, I quickly dropped SortMeRNA, as it was substantially slower than Ribodetector.↩︎\nAt least as far as Kraken2 is concerned. I don’t entirely trust Kraken2’s assignments, but am going with them for now. I might come back and dig more into this aspect of things later if it seems useful.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-19_deduplication.html",
    "href": "notebooks/2023-10-19_deduplication.html",
    "title": "Comparing options for read deduplication",
    "section": "",
    "text": "See also:\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\nComparing Ribodetector and bbduk for rRNA detection\n\nDuplicate read pairs can arise in sequencing data via several mechanisms.\n\nBiological duplicates are sequences that arise from different source nucleic-acid molecules that genuinely have the same sequence; these tend to arise when a particular gene or taxon is both extremely abundant and has low sequence diversity. In our case, the most likely cause of biological duplicates are ribosomal RNAs.\nTechnical duplicates, meanwhile, arise when the same input molecule produces multiple reads. Subgroups of technical duplicates include PCR duplicates arising from amplification of a single input sequence into many library molecules, and several forms of sequencing duplicates arising from errors in the sequencing process. For example, Illumina sequencing on unpatterned flow cells can give rise to optical duplicates, where a single cluster on the flow cell is falsely identified as two by the base-calling algorithm.\n\nIn general, we want to remove or collapse technical duplicates, while retaining biological duplicates. Unfortunately, in the absence of UMIs, there’s generally no way to distinguish biological and PCR duplicates; however, many forms of sequencing duplicates can often be identified from the sequence metadata provided in the FASTQ file.\nA number of tools are available that attempt to remove some or all of the duplicate sequences in a file. Some of these use cluster positioning information to distinguish sequencing duplicates from other duplicates, while others identify duplicates based purely on their base sequence. In the latter case, the tricky part is identifying the correct threshold for duplicate identification. Due to sequencing errors, requiring perfect base identity between two reads in order to designate them as duplicates often results in true duplicates surviving the deduplication process. On the other hand, designating two reads as duplicates based on too low a sequence identity (or too short a subsequence) will result in spurious deduplications that needlessly throw away useful data.\nAt the time of writing, our standard sequencing pipeline carries out deduplication very late in the process, during generation of JSON files for the dashboard (i.e. after generating clade counts). Read pairs are identified as duplicates if they are identical in the first 25 bases of both the forward and the reverse read, requiring 50nt of matches overall.\nI wanted to see if there was a widely-used read deduplication tool that we could apply to our pipeline, ideally early on as part of sample preprocessing. I started out comparing four approaches, before fairly quickly cutting down to two:\n\nfastp is a FASTQ pre-processing tool previously investigated here. If run with the --dedup`flag, it will remove read pairs that are exact duplicates of one another. As far as I know, fastp doesn’t have the ability to identify or remove inexact duplicates, or to distinguish sequencing duplicates from other duplicates. It thus represents a fast but relatively unsophisticated option.\nClumpify is a tool that was originally developed to reduce space and improve processing speed by rearranging fastq.gz files. It identifies duplicates by looking for reads (or read pairs) that match exactly in sequence, except for a specified number of permitted substitutions. This, to me, is the obvious way to detect duplicates, and this is the only deduplication tool I’ve found that does it this way. It also allows distinguishing of optical vs other duplicates and specific removal of only optical duplicates if desired.\nGATK Picard’s MarkDuplicates and samtools markdup are two functions for removing duplicate reads in SAM/BAM files. I tried both, but found them to be slow, confusing, and apparently unable to actually detect any duplicates in the files I ran them on. It’s possible that both of these tools only work on mapped reads (which would make sense given their demand for SAM/BAM files); it’s also possible that I could make them work given more time and effort, but I didn’t want to put this in unless the other tools I found proved inadequate.\n\nTo test fastp and Clumpify, I ran them on the same samples I used for looking into ribodetection:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR14530880\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nIn each case, I tested deduplication at two points in the pipeline: immediately after preprocessing (with FASTP, without deduplication), and following segregation of rRNA reads using bbduk.\n1. Johnson (COMO4)\nFollowing fastp preprocessing, the Johnson sample contains 15.25M read pairs, 74.36% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (as identified by bbduk), this number fell to 69.72%, while for ribosomal reads it rose to 79.13%. Needless to say, all of these fail FASTQC’s read-duplication QC test.\nRunning clumpify on the fastp output took 16 seconds and removed 5.04M read pairs (33.0%) as duplicates, 11576 of which were optical. Running it on the bbduk non-ribosomal output took 9 seconds and removed 2.71M (35.3%) reads as duplicates, 7107 of which were optical. After deduplication with clumpify, FASTQC identifies 24.19% of sequences as duplicates in the post-fastp dataset, and 18.38% in the post-bbduk dataset. In both cases, this is a dramatic reduction, sufficient for FASTQC to now mark the duplication level QC as passing where it was previously failing.\nRunning fastp with deduplication enabled on the fastp output took 33 seconds and removed 4.21M read pairs (27.6%) as duplicates. Running it on the bbduk non-ribosomal output took 17 seconds and removed 2.33M read pairs (30.4%) as duplicates. In both cases, the number of reads removed is lower than that removed by clumpify, consistent with the fact that fastp requires complete identity between duplicates while clumpify allows a small number of mismatches. For whatever reason, this difference resulted in a dramatic difference in FASTQC quality metrics: after deduplication with fastp, FASTQC identifies 68.09% of reads as duplicates in the full dataset, and 59.78% in the post-bbduk dataset. Both of these are high enough to result in a failure for the corresponding QC test.\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n24.19\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n68.09\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n18.38\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n59.78\nFailed\n\n\n2. Rothman (SRR14530880)\nFollowing fastp preprocessing, the Rothman sample contains 13.61M read pairs, 81.27% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (6.20M read pairs, as identified by bbduk), this number fell to 78.79%. Both of these failed QC.\nRunning clumpify on the fastp output took 14 seconds and removed 7.41M read pairs (54.5%) as duplicates. Running it on the bbduk non-ribosomal output took 7 seconds and removed 2.96M read pairs (47.8%) as duplicates. In both cases, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 26 seconds and removed 6.87M read pairs (50.5%) as duplicates. Running it on the bbduk non-ribosomal output took 14 seconds and removed 2.71M read pairs (43.6%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n27.9\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n62.9\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n33.0\nWarning\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n63.2\nFailed\n\n\n\nAs before, the difference is large, with Clumpify performing dramatically better.\n3. Crits-Christoph et al. (SRR23998357)\nFinally, we have the sample from Crits-Christoph et al. (2021). Following fastp processing, this sample contained 47.47M read pairs, 55.9% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (42.45M read pairs, as identified by bbduk), this number fell to 53.1%. Both of these failed QC.\nRunning clumpify on the fastp output took 90 seconds and removed 19.25M read pairs (40.6%) as duplicates. Running it on the bbduk non-ribosomal output took 65 seconds and removed 16.49M read pairs (38.8%) as duplicates. As with Rothman, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 94 seconds and removed 16.58M read pairs (34.9%) as duplicates. Running it on the bbduk non-ribosomal output took 85 seconds and removed 14.12M read pairs (33.3%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\nBoth programs perform much better here than for previous samples, with even fastp reducing levels of duplicates low enough to avoid failing. Nevertheless, clumpify continues to perform substantially better.\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n10.64\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n35.22\nWarning\n\n\nRibodepleted (BBDUK)\nClumpify\n9.89\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n32.07\nWarning\n\n\n\n\nCodeduplicate_data_path &lt;- \"../data/2023-10-19_deduplication/duplicate-data.csv\"\nduplicate_data &lt;- read_csv(duplicate_data_path, show_col_types = FALSE) %&gt;%\n  mutate(sample_display = paste0(sample, \" (\", dataset, \")\"),\n         dedup_method = fct_inorder(dedup_method),\n         sample_display = fct_inorder(sample_display))\ng_duplicate &lt;- ggplot(duplicate_data, aes(x=dedup_method, y=fastqc_pc_dup, fill=sample_display)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(name = \"% Duplicates (FASTQC)\", limits = c(0,100),\n                     breaks = seq(0,100,20), expand = c(0,0)) +\n  scale_x_discrete(name = \"Deduplication method\") +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Sample\") +\n  facet_grid(. ~ processing_stage) +\n  theme_base\ng_duplicate\n\n\n\n\n4. Conclusion\nI was originally planning to do more validation here, but I honestly don’t think it’s required. Of the two methods I managed to successfully apply to these samples, Clumpify is faster; applies an intuitively more appealing deduplication method; removes more sequences; and achieves much better FASTQC results. It’s also very easy to use and configure. I recommend it as our deduplication tool going forward.\nIn terms of when to apply the tool, I think it probably makes most sense to apply deduplication downstream of counting (if we use Ribodetector) or detecting and filtering (if we use bbduk) ribosomal reads, to avoid spurious filtering of rRNA biological duplicates."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html",
    "href": "notebooks/2023-10-24_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-01_project-runway-comparison.html",
    "href": "notebooks/2023-11-01_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication, at least on its own, though it might still be successful if applied twice (after reverse-complementing one of the FASTQ files) or in combination with another tool.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I’ve updated negatively about Clumpify, which appears to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates). I’m not sure yet how to handle this; possibly it makes sense to run reads through multiple fast deduplication tools in order to catch as many duplicates as possible.\n\nFASTP deduplication is also unable to catch these cases.\nI’m current unaware of a public tool that will catch these. We might need to build our own."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html",
    "href": "notebooks/2023-10-31_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-comparison.html",
    "href": "notebooks/2023-11-02_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication. UPDATE: I found an option for Clumpify that seems to solve this problem, at least in this case. See Conclusions for more.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I initially updated negatively about Clumpify, which appeared to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates).\n\nHowever, I found an option for Clumpify which addresses this issue, at least in this case. Specifically, one can configure Clumpify to unpair reads, perform deduplication on the forward and reverse reads all together, and then restore pairing. This successfully removes this class of duplicates.\nI’m a little worried that this approach might sometimes cause complete loss of all duplicate reads (rather than all-but-one-pair) when the best-quality duplicate differs between forward and reverse reads. I tried this out by artificially modifying the quality scores for the duplicate reads from D23-13405-2, and this doesn’t seem to be the case at least in this instance: when quality across read pairs was concordant, I was able to control which read pair survived as expected, and when it was discordant, one read pair survived anyway. Still, this remains a niggling doubt."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "",
    "text": "One relevant question for both Project Runway and other NAO sequencing is: what is the maximum read depth at which we can sequence a given sample while retaining an acceptable level of sequence duplication?\nAs discussed in a previous entry, duplicate reads can arise in sequencing data from a variety of processes, including true biological duplicates present in the raw sample; processing duplicates arising from amplification and other processes during sample and library prep; and sequencing duplicates arising from various processes on the actual flow cell.\nAs we sequence more deeply, we expect the fraction of biological and processing duplicates (but not, I think, sequencing duplicates) in our read data to increase. In the former case, this is because we are capturing a larger fraction of all the input molecules in our sample; in the latter, because we are sequencing copies of the same sequence over and over again. Intuitively, I expect the increase in processing duplicates to swamp that in biological duplicates at high read depth, at least for library prep protocols that involve amplification1.\nOne simple approach to investigate the overall effect of read depth on duplication levels in the sample is rarefaction: downsampling a library to different numbers of reads and seeing how the duplication rate changes as a function of read count. In this notebook entry, I apply this approach to sequencing data from the Project Runway initial DNA dataset, to see how duplication rate behaves in this case."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt might be worth explicitly modeling the difference in behavior between different kinds of duplicates as sequencing depth increases, to see if these intuitions are borne out.↩︎\nProbably the biggest two improvements that could be made to this model in future are (i) introducing biological duplicates, and (ii) introducing sequence-specific bias in PCR amplification and cluster formation.↩︎\nI’d appreciate it if someone else on the team can check my math here.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "href": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "title": "Project Runway RNA-seq testing data: removing livestock reads",
    "section": "",
    "text": "In my last entry, I presented my Nextflow workflow for analyzing viral MGS data, as well as the results of that workflow applied to our recent BMC RNA-seq dataset. One surprising thing I observed in those data was the presence of bovine and porcine sequences confounding my pipeline for identifying human-infecting-virus reads. To address this problem, I added a step to the pipeline to remove mammalian livestock sequences in a manner similar to the pre-existing human-removal step, by screening reads against cow and pig genomes using BBMap. In this short entry, I present the results of that change.\nAs expected, the bulk of the pipeline performed identically to the last analysis. Mammalian read depletion removed between 8k and 18k reads per protocol (0.007% to 0.017%):\n\nCode# Import stats\nkits &lt;- tibble(sample = c(\"1A\", \"1C\", \"2A\", \"2C\", \"6A\", \"6C\", \"SS1\", \"SS2\"),\n               kit = c(rep(\"Zymo quick-RNA kit\", 2),\n                       rep(\"Zymo quick-DNA/RNA kit\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (new)\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (old)\", 2))) %&gt;%\n  mutate(kit = fct_inorder(kit))\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\ndata_dir_1 &lt;- \"../data/2023-12-19_rna-seq-workflow/\"\ndata_dir_2 &lt;- \"../data/2023-12-22_bmc-cow-depletion/\"\nbasic_stats_path &lt;- file.path(data_dir_2, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir_2, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir_2, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir_2, \"qc_quality_sequence_stats.tsv\")\n# Extract stats\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\n# Plot stages\nbasic_stats_stages &lt;- basic_stats %&gt;% group_by(kit,stage) %&gt;% \n  summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx), .groups = \"drop\", mean_seq_len = mean(mean_seq_len), percent_duplicates = mean(percent_duplicates))\ng_reads_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_read_pairs, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_bases_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_bases_approx, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Bases (approx)\", expand=c(0,0)) +\n  theme_kit\nlegend &lt;- get_legend(g_reads_stages)\ntnl &lt;- theme(legend.position = \"none\")\nplot_grid((g_reads_stages + tnl) + (g_bases_stages + tnl), legend, nrow = 2,\n          rel_heights = c(4,1))\n\n\n\n\n\nCode# Import composition data\ncomp_path &lt;- file.path(data_dir_2, \"taxonomic_composition.tsv\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  mutate(classification = sub(\"Other_filtered\", \"Other filtered\", classification)) %&gt;%\n  arrange(desc(p_reads)) %&gt;% mutate(classification = fct_inorder(classification))\ncomp_kits &lt;- inner_join(comp, kits, by=\"sample\") %&gt;%\n  group_by(kit, classification) %&gt;%\n  summarize(t_reads = sum(n_reads/p_reads), n_reads = sum(n_reads), .groups = \"drop\") %&gt;%\n  mutate(p_reads = n_reads/t_reads) %&gt;% ungroup\n# Plot overall composition\ng_comp &lt;- ggplot(comp_kits, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- comp_kits %&gt;% filter(p_reads &lt; 0.1)\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,0.02), breaks = seq(0,0.02,0.004),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp_minor\n\n\n\n\nTo identify human-viral reads, I used the multi-stage process specified in the last entry. Specifically, after removing human reads with BBmap, the remaining reads went through the following steps:\n\nAlign reads to a database of human-infecting virus genomes with Bowtie2, with permissive parameters, & retain reads with at least one match. (Roughly 20k read pairs per kit, or 0.25% of all surviving non-host reads.)\nRun reads that successfully align with Bowtie2 through Kraken2 (using the standard 16GB database) and exclude reads assigned by Kraken2 to any non-human-infecting-virus taxon. (Roughly 5500 surviving read pairs per kit.)\nCalculate length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\). Filter out reads that don’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\n\nCode# Import Bowtie2/Kraken data and perform filtering steps\nmrg_path &lt;- file.path(data_dir_2, \"hv_hits_putative_all.tsv\")\nmrg0 &lt;- read_tsv(mrg_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev))\nmrg1 &lt;- mrg0 %&gt;% filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2)\nmrg2 &lt;- mrg1 %&gt;% \n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\")))) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nmrg_all &lt;- bind_rows(mrg0, mrg1, mrg2)\n# Visualize\ng_mrg &lt;- ggplot(mrg_all, aes(x=adj_score_fwd, y=adj_score_rev, color=assigned_hv)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby Kraken2\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(filter_step~kit, labeller = labeller(filter_step=function(x) paste(\"Step\", x), kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nApplying all of these filtering steps left a total of 227 read pairs across all kits: 14 fewer than in the previous analysis. The 14 excluded read pairs included all 11 cow and pig reads, as well as three other read pairs classified as non-viral by BLAST. After removing these reads, the distribution of read scores vs BLAST HV status looked like the following, with many fewer high-scoring non-HV reads:\n\nCodemrg_old_path &lt;- file.path(data_dir_1, \"hv_hits_putative.tsv\")\nmrg_old &lt;- read_tsv(mrg_old_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev)) %&gt;% \n  filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2) %&gt;%\n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\"))[1])) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nhv_blast_old &lt;- list(\n  `1A` = c(rep(TRUE, 40), TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE),\n  `1C` = c(rep(TRUE, 5), \"COWS\", TRUE, TRUE, FALSE, TRUE, rep(FALSE, 9)),\n  `2A` = c(rep(TRUE, 10), \"COWS\", TRUE, \"COWS\", \"COWS\", TRUE,\n           FALSE, \"COWS\", FALSE, TRUE, TRUE,\n           FALSE, FALSE, FALSE, FALSE, TRUE),\n  `2C` = c(rep(TRUE, 5), TRUE, \"COWS\", TRUE, TRUE, TRUE, \n           TRUE, TRUE, \"COWS\", TRUE, \"COWS\", \n           FALSE, FALSE, FALSE), \n  `6A` = c(rep(TRUE, 10), FALSE, TRUE, FALSE, FALSE, FALSE, \n           FALSE, TRUE, TRUE, FALSE), \n  `6C` = c(rep(TRUE, 5), \"PIGS\", TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, FALSE, FALSE, FALSE), \n  SS1  = c(rep(TRUE, 5), rep(FALSE, 10),\n           \"FALSE\", \"COWS\", \"FALSE\", \"FALSE\", \"FALSE\",\n           rep(FALSE, 15)\n           ),\n  SS2  = c(rep(TRUE, 25), TRUE, \"COWS\", TRUE, TRUE, TRUE,\n           TRUE, TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, TRUE, FALSE, FALSE,\n           FALSE, FALSE, TRUE, FALSE, FALSE,\n           rep(FALSE, 5),\n           FALSE, FALSE, FALSE, FALSE, TRUE,\n           FALSE, FALSE, TRUE, TRUE, FALSE)\n  )\nmrg_old_blast &lt;- mrg_old %&gt;% group_by(sample) %&gt;%\n  mutate(seq_num = row_number()) %&gt;% ungroup %&gt;%\n  mutate(hv_blast = unlist(hv_blast_old),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\nmrg_old_blast_filtered &lt;- mrg_old_blast %&gt;% filter(seq_id %in% mrg2$seq_id)\ng_mrg3_1 &lt;- mrg_old_blast_filtered %&gt;% mutate(hv_blast = hv_blast == \"TRUE\") %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=hv_blast)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby BLAST\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(kraken_label~kit, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg3_1\n\n\n\n\nRepeating the exercise from the last entry, in which different score thresholds are assessed along different performance metrics, we find a significant improvement in optimal F1 score compared to the pre-filtering dataset, with a maximum F1 of 0.958 for a conjunctive threshold and 0.966 for a disjunctive threshold (both at a threshold score value of 20):\n\nCode# Test sensitivity and specificity\ntest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, hv_blast %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(hv_blast, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nstats_conj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=TRUE) %&gt;% bind_rows\nstats_disj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=FALSE) %&gt;% bind_rows\nstats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n  pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n  mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats &lt;- ggplot(stats_all, aes(x=threshold, y=value, color=metric)) +\n  geom_line() +\n  geom_vline(data = threshold_opt, mapping=aes(xintercept=threshold), color=\"red\",\n             linetype = \"dashed\") +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats\n\n\n\n\nAs such, it appears that filtering mammalian livestock genomes is quite successful in improving our HV detection pipeline, at least for this dataset."
  },
  {
    "objectID": "notebooks/2024-01-30_blast-validation.html",
    "href": "notebooks/2024-01-30_blast-validation.html",
    "title": "Automating BLAST validation of human viral read assignment",
    "section": "",
    "text": "In previous entries, I presented the results of my Nextflow workflow on recent BMC RNA-seq data. To validate and calibrate my Bowtie2/Kraken2 pipeline for identifying human-viral reads, I manually BLASTed putative viral reads against NCBI’s nt database using their online BLAST tool, then assessed the results by eye to determine if each read likely came from a human virus. This approach was effective, but slow and manual, and thus hard to scale to other datasets I wanted to analyze with a higher relative abundance of viral sequences. I thus investigated options for automating the process to generate custom tabular BLAST results on the command line, which could then be read and processed programmatically.\nAmong the several options I investigated, the most promising were (1) Elastic-BLAST, NCBI’s own cloud-based BLAST tool, and (2) running regular command-line blast with the -remote option enabled to query NCBI’s online databases. I don’t currently have the AWS permissions needed to run Elastic-BLAST on our AWS service, and blastn -remote initially seemed too slow to be useful when run on an EC2 instance. However, I discovered that the latter option appears to run much more quickly when run on my local machine, making this a much more attractive option, at least until I’m able to run Elastic-BLAST.\nTo evaluate the potential for this approach to semi-automate the process of HV read validation, I ran BLASTN on the 227 putative HV reads from my previous BMC entry, using the following command:\nblastn -query &lt;input_path&gt; -out &lt;output_path&gt; -db nt -remote -perc_identity 60 -max_hsps 5 -num_alignments 50 -qcov_hsp_perc 30 -outfmt \"6 qseqid sseqid sgi staxid qlen evalue bitscore qcovs length pident mismatch gapopen sstrand qstart qend sstart send\"\nTo begin with, I first needed to identify a list of taxids I could use to designate a read as human-viral. To do this, I took the list of HV taxids generated by my Nextflow workflow and expanded it to include any missing descendent taxids using the NCBI taxid tree structure. I also generated another list of taxids that included any taxid descended from any of these taxids or the overall virus taxid (10239); this latter approach decreases the risk of false negatives at the cost of potentially increasing the risk of false positives from phage sequences. The two approaches generated lists of 28,105 and 234,499 taxids, respectively.\n\nCode# Set file paths\ndata_dir &lt;- \"../data/2024-01-30_blast/\"\nreads_db_path &lt;- file.path(data_dir, \"bmc-hv-putative.tsv\")\nblast_results_path &lt;- file.path(data_dir, \"bmc-hv-putative.blast\")\nhv_taxids_path &lt;- file.path(data_dir, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir, \"nodes.dmp.gz\")\n\n# Import files for viral taxid search\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids_1 &lt;- expand_taxids(hv_taxids$taxid, tax_nodes)\nv_taxids_2 &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n\nI next imported the BLAST alignment results and performed exploratory data analysis. Following visual inspection, it appeared that the following process would generate good results:\n\nIf both the forward and reverse reads from a read pair align to the same viral taxid (given the specified filters on query coverage, percent identity, etc), classify that read pair as human viral.\nIf only one of the reads in a read pair aligns to any given viral taxid, flag the read for manual inspection.\nIf neither read aligns to a viral taxid, classify that read pair as non-human-viral.\n\n\nCode# Prepare and import BLAST results\nreads_db &lt;- read_tsv(reads_db_path, show_col_types = FALSE)\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Filter BLAST results by read ID\nreads_db_seqs &lt;- reads_db %&gt;% mutate(read_id = paste(sample, seq_num, sep=\"_\")) %&gt;% pull(read_id) %&gt;% c(paste(., \"1\", sep=\"_\"), paste(., \"2\", sep=\"_\"))\nblast_results_filtered &lt;- blast_results %&gt;% filter(qseqid %in% reads_db_seqs)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_paired &lt;- blast_results_filtered %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_hviral &lt;- blast_results_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_out &lt;- blast_results_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  inner_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_inspected &lt;- blast_results_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status &lt;- blast_results_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_1 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_2 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance &lt;- bind_rows(blast_performance_1, blast_performance_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance\n\n\n\n  \n\n\n\nUsing the inferred list of human-virus taxids results in high precision and specificity, but low sensitivity, missing many reads flagged as human-viral by manual inspection. Conversely, using a list of all viral taxids as the reference achieves perfect sensitivity and near-perfect precision and specificity, with only one false positive result, resulting in an F1 score of over 99%.\nThat one false positive turned out to be a sequence with a high-identity but low-query-coverage human-viral match (to human enterovirus 71) which was excluded during my previous manual assessment; in my opinion, it’s unclear whether this should really be classed as a false positive. If we want to class this sequence as non-human-viral, increasing the query coverage threshold from 30% to 35% successfully excludes it without losing any true positives, resulting in perfect precision relative to manual assignments:\n\nCode# Filter BLAST results by query coverage\nblast_results_qcov &lt;- blast_results_filtered %&gt;% filter(as.numeric(qcovs) &gt;= 35)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_qcov_paired &lt;- blast_results_qcov %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_qcov_hviral &lt;- blast_results_qcov_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_qcov_out &lt;- blast_results_qcov_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  full_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status_1 = replace_na(viral_status_1, \"FALSE\"),\n         viral_status_2 = replace_na(viral_status_2, \"FALSE\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_qcov_inspected &lt;- blast_results_qcov_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status_qcov &lt;- blast_results_qcov_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_qcov_1 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_qcov_2 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance_qcov &lt;- bind_rows(blast_performance_qcov_1, blast_performance_qcov_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance_qcov\n\n\n\n  \n\n\n\nIn summary, using command-line blastn -remote, combined with tabular processing in R against a reference class of all viral taxids, works well as a substitute for manual inspection of online BLAST results for validating human-viral read assignments. It’s too slow and failure-prone to be added to the pipeline as a replacement or supplement to the Bowtie/Kraken automated approach, but will be my go-to approach in future for manual validation and refinement of human-viral assignment criteria – at least until I’m able to try out Elastic-BLAST."
  },
  {
    "objectID": "notebooks/2024-02-04_crits-christoph-1.html",
    "href": "notebooks/2024-02-04_crits-christoph-1.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 1",
    "section": "",
    "text": "Since my last entry, I’ve continued to work on the new Nextflow MGS pipeline. Changes since the last entry include:\n\nSuccessful modification of the pipeline to run using working and output directories on S3 rather than locally, and several other QOL improvements (e.g. implementation of a Nextflow config file).\nVarious bug fixes.\nModification of the human-viral identification sub-pipeline to include singly or discordantly aligned read pairs.\nImplementation of a “trial run” mode for the main workflow in which the raw read files are automatically subset prior to analysis to speed up testing.\nCreation of a separate workflow for generating large index and reference files, so that these don’t need to be stored locally or generated during every run of the main workflow.\n\n\n\n\n\nIn order to validate pipeline performance on datasets other than our own BMC data, I’ve begun running it on pre-existing datasets from the P2RA project, starting with Crits-Christoph et al. (2021):\n\nThis was first and foremost a SARS-CoV-2 sequencing project that took place in the San Francisco Bay Area in 2020. Our data consists of 18 samples, all of which were collected as 24-hour composite samples of raw sewage collected from wastewater interceptor lines in four different locations (Berkeley, Oakland, San Francisco, and Marin).\nSamples were concentrated using three different methods (Amicon ultrafiltration, silica columns, or milk of silica); see paper methods for more details.\nSix samples (the “unenriched” fraction) underwent bacterial ribodepletion with RiboZero Plus, followed by untargeted metagenomic RNA-sequencing. The rest (the “enriched” fraction) underwent panel enrichment for respiratory viruses using the Illumina Respiratory Virus Oligo Panel, and no ribodepletion.\n\nThe raw data\nThe raw data obtained from SRA comprised 18 pairs of FASTQ files corresponding to 18 different samples. Unlike for the BMC data, there was no need to concatenate multiple FASTQ files for a given sample. What remained was roughly 298M read pairs of non-RVP-enriched data and 9M read pairs of RVP-enriched data, corresponding to roughly 45 Gb and 1 Gb of sequencing data, respectively. Reads were typically 70-80bp in length. Read qualities were high, adapter levels were relatively low, and FASTQC-measured duplication levels were moderate at 50-70%.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-04_crits-christoph-1/\"\nlibraries_path &lt;- file.path(data_dir, \"cc-libraries.txt\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages), \n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,80),\n                     breaks=seq(0,80,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() + \n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(20,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,80),\n                     breaks=seq(0,80,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() + \n  scale_color_brewer(palette = \"Dark2\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nBoth RVP-enriched and unenriched samples showed significant, but not overwhelming, reductions in read counts during preprocessing. In the case of unenriched samples, most reads were lost during the deduplication stage, with low losses observed at other stages. In enriched samples, a much larger fraction of reads were lost during the ribodepletion stages; this is consistent with all of the unenriched samples having undergone ribodepletion, while the unenriched samples did not.\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 44% and 66% of total reads in unenriched and enriched samples, respectively. Host depletion, livestock depletion and secondary ribodepletion removed a further 2% and 5%, respectively.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot bases over preprocessing\ng_bases_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_bases_approx,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Dark2\", name=\"Enrichment\") +\n  scale_y_continuous(\"# Bases (approx)\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_bases_stages\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing had little effect on sequence quality, but this is unsurprising since raw sequence quality was already high.\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the former and latter respectively having the greatest effect in unenriched vs enriched samples. In both cases, the average detected duplication level after both processes was roughly 15%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Dark2\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  facet_wrap(.~enrichment, scales = \"free_y\") +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% select(sample, location, method, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"method\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location, method=method,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (remove_other-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = (ribo_initial-remove_human) + eukaryota,\n                       n_other = remove_human-remove_other)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads)) %&gt;%\n  mutate(loc_met = paste(location,method,sep=\"_\"))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(loc_met, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=loc_met, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=loc_met, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nIn the unenriched samples, about 80% of reads are low-quality, duplicates, ribosomal, or unassigned, leaving about 20% of other assignable reads. Of these, most (16-20%) were bacterial. The fraction of human reads ranged from 0.6 to 1.5%; that of livestock reads from 0.04 to 0.1%; and that of viral reads from 0.2 to 0.5% – about 2 orders of magnitude higher than observed in our BMC data.\nAs expected, the panel-enriched samples show significantly higher levels of viral reads than the unenriched samples, between 0.5 and 6.4% of all reads. Conversely, with the exception of one sample with 33% bacterial reads, most enriched samples showed much lower bacterial read abundance (0.8 to 4.4%). Human reads are also moderately reduced (0 to 1%), though not as dramatically as bacteria.\nIn my next entry, I’ll look into human-infecting virus reads in the Crits-Christoph data, and the changes I needed to make to the pipeline described above to make this analysis go well."
  },
  {
    "objectID": "notebooks/2024-02-08_crits-christoph-2.html",
    "href": "notebooks/2024-02-08_crits-christoph-2.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 2",
    "section": "",
    "text": "In my last entry, I described the process of analyzing Crits-Christoph et al. (2021) data with my Nextflow workflow, up to and including high-level taxonomic analysis. In this entry, I’ll look more deeply at the human-infecting-virus content of these data, and address some inadequacies in that pipeline that were causing issues in analyzing those reads.\nTo begin with, I analyzed the human-infecting virus reads in Crits-Christoph in an identical manner to my previous analysis of our BMC data, making use of the automated BLAST characterization approach developed in a previous entry.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir, \"cc-libraries.txt\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nhv_reads_all_path &lt;- file.path(data_dir, \"hv_hits_putative_all.tsv.gz\")\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\n\n# Import data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nhv_reads_all &lt;- read_tsv(hv_reads_all_path, show_col_types = FALSE) %&gt;% \n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_reads_filtered_1 &lt;- hv_reads_all %&gt;% \n  filter(!classified | assigned_hv)\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Count\nn_hv_reads_all &lt;- hv_reads_all %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"remove_other\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_1 &lt;- hv_reads_filtered_1 %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% rename(n_filtered_1 = n) %&gt;% inner_join(n_hv_reads_all, by=c(\"sample\", \"location\", \"method\", \"enrichment\")) %&gt;% mutate(p_reads = n_filtered_1/n_total, pc_reads = p_reads*100)\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, method, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"remove_other\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\nThe process for identifying human-viral (HV) reads was similar to that described in my last two entries:\n\nAfter removing human and livestock reads with BBMap, the remaining reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters. This retained roughly 0.3% to 0.4% of surviving reads (57k to 130k reads) for unenriched samples and 1% to 67% (650 to 25k reads) for enriched samples.\nReads that aligned successfully with Bowtie2 were run through Kraken2 (using the standard 16GB database) and reads that were assigned to any non-human-virus taxon were excluded. This retained roughly 0.17% to 0.29% of surviving reads (30k to 81k reads) for unenriched samples, and made little difference to the enriched samples.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 7042 read pairs across all unenriched samples (0.004% of surviving reads) and &gt;110,000 read pairs across all enriched samples (4% of surviving reads):\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(kraken_label~enrichment, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nI previously assessed potential criteria for designating a read as human-viral based on \\(S\\) and Kraken assignment status, and found that a disjunctive threshold of \\(S \\geq 20\\) (i.e. passing a read pair if either the forward or reverse read had an adjusted alignment score of at least 20) achieved the best performance. To test whether this remains the case across datasets, I BLASTed the 7042 putative HV reads from the unenriched data against nt using blastn -remote as described previously. I then assigned “ground-truth” viral status to each read pair as follows:\n\nLet a BLAST alignment to a sequence be described as “strong” if it’s in the joint top 5 highest-scoring BLAST alignments for that sequence and passes filters on % identity, query coverage, etc.\nIf both the forward and reverse reads from a read pair exhibit strong alignments to the same viral taxid, classify that read pair as human viral.\nIf either read in a read pair exhibits a strong alignment to the same viral taxid as that identified by Kraken/Bowtie, classify that read as human viral.\nOtherwise, if either read in a read pair exhibits a strong alignment to any viral taxid, classify the read as ambiguously viral and flag it for manual inspection.\nIf neither read aligns to a viral taxid, classify that read pair as non-human-viral.\n\n\nCode# Import files for viral taxid search\nhv_taxids_path &lt;- file.path(data_dir, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir, \"nodes.dmp.gz\")\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"cat-cc-unenriched.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\n\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  full_join(blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n# Import Bowtie2/Kraken data and perform filtering steps\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\ng_mrg_v &lt;- mrg_unenriched_plot %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v\n\n\n\n\nOff the bat, it’s clear this approach isn’t working well. There are almost 1,000 “UNCLEAR” data points that need manual inspection; worse, there appear to be numerous nonviral sequences achieving very high normalized Bowtie2 alignment scores:\n\nCodemrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\ng_hist &lt;- ggplot(mrg_unenriched_debug, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0) +\n  facet_wrap(~viral_status_out) +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") +\n  theme_base\ng_hist\n\n\n\n\nInspecting these high-scoring non-viral reads, we see that they are dominated by a few specific genome IDs: the top five genome IDs account for over 75% of high-scoring false-positives, and all of the top 10 overwhelmingly produce false rather than true positives.\n\nCodeheader_path &lt;- file.path(data_dir, \"human-viral-headers.txt\")\nheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_unenriched_debug, header_db, by=\"genome_id\")\n\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\nInspecting these top genome IDs more closely, we notice something interesting: many of them are transgenic. When we try BLASTing read pairs mapping to these transgenic sequences against nt with NCBI’s online tool, we find that the best matches are all to bacterial plasmids, cloning vectors and synthetic constructs. This suggests a clear culprit for a large fraction of the false positives we are seeing here: contamination of the Bowtie2 reference database with non-viral sequences inserted into transgenic viruses.\n\nCodemrg_unenriched_fasta &lt;- mrg_unenriched_debug %&gt;% filter(adj_score_max &gt;= 20, viral_status == 0) %&gt;% group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir, \"cc-bad-gid.fasta\"))\n\n\nExcluding “transgenic” and “mutant” genome IDs is predicted to remove 77% of the high-scoring false positives observed above. This left a small enough number of sequences to characterize manually using NCBI BLAST. When we do this, we see the following breakdown of causes:\n\nCode# Separate transgenic sequences\nbad_genomes_tr &lt;- bad_genomes %&gt;% \n  mutate(transgenic = grepl(\"transgenic\", genome_name, ignore.case=TRUE)|grepl(\"mutant\", genome_name, ignore.case=TRUE))\nbad_genomes_notr &lt;- bad_genomes_tr %&gt;% filter(!transgenic) %&gt;% ungroup %&gt;% mutate(pr_FALSE = n_FALSE/sum(n_FALSE))\n\n# Import manually annotated causes\nbg_causes &lt;- read_tsv(file.path(data_dir, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nbg_causes_out &lt;- bg_causes %&gt;% group_by(cause) %&gt;% summarize(n = sum(n_FALSE)) %&gt;% arrange(desc(n))\nbg_causes_out\n\n\n\n  \n\n\n\nBy far the most important attributable causes for misidentification of viral reads are:\n\nMapping to the human genome or human-derived synthetic constructs\nMapping to the E. coli genome\nMapping to cow or pig genomes\n\nThis suggests (1) that the current filtering to remove human and cow/pig sequences is insufficiently stringent, and (2) that adding E. coli and possibly one or more eukaryotic synthetic constructs to the database of contaminants to screen for could go a long way toward removing remaining false positives.\nFinally, it’s worth noting that a significant fraction of these sequences appear to be Bowtie true-positives that were falsely annotated as negatives by my automated BLAST analysis above. I’m not yet sure what’s underlying this, but it gives me some confidence that, if I can fix the issues outlined above, the general approach of this pipeline is still viable.\n\nCodemrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% full_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2"
  },
  {
    "objectID": "notebooks/2024-02-13_crits-christoph-3.html",
    "href": "notebooks/2024-02-13_crits-christoph-3.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 3",
    "section": "",
    "text": "In my last entry, I documented my first attempt at analyzing the human-infecting-virus content of sequence data from Crits-Christoph et al. (2021). In this entry, I’ll attempt to address the issues that arose in that first analysis and analyze the resulting updated data.\nAs a reminder, I previously found that the analysis pipeline I developed for our BMC data led to numerous high-scoring false-positives when run on Crits-Christoph data:\n\nCode# Data input paths\ndata_dir_old &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir_old, \"cc-libraries.txt\")\nhv_reads_filtered_path &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_taxids_path &lt;- file.path(data_dir_old, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir_old, \"nodes.dmp.gz\")\nblast_results_path &lt;- file.path(data_dir_old, \"cat-cc-unenriched.blast.gz\")\nbg_causes &lt;- read_tsv(file.path(data_dir_old, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nheader_path &lt;- file.path(data_dir_old, \"human-viral-headers.txt\")\n\n# Import data\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Process Bowtie/Kraken data\nmrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n\n# Compare BLAST results to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid, seq_id)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num, seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  left_join(blast_results_out, by=c(\"sample\", \"seq_num\", \"seq_id\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\nmrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% left_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2\n\n\n\n\nIn total, 992 non-viral read pairs and 83 read pairs of unclear status achieved high length-adjusted alignment scores (\\(\\geq 20\\)) when mapped to viral Genbank with Bowtie2. For now, I will focus on the non-viral read pairs, and return to the unclear read pairs after these are in better shape.\nPass 1: excluding transgenic and bacterial sequences\nThe first major change I made to the pipeline was to curate the reference database for the Bowtie2 alignment to remove transgenic and other inappropriate viral sequences. To do this, I subset the collated FASTA file of viral genomes with seqtk to remove any sequence with “transgenic”, “mutant”, “recombinant”, “unverified” or “draft” in its sequence ID. This removed a total of 493 genomes from the reference DB, leaving a total of 41942 remaining for alignment with Bowtie2.\nAt the same time, I also added an E. coli genome to the set of contaminant genomes to screen against, joining cow, pig and human. I also moved the screening step to be downstream rather than upstream of the Bowtie2 filtering step, to save computation time and make it quicker to make further modifications downstream if needed. For now, I didn’t make any changes to the BBMap parameters for screening for contaminant genomes.\nAfter passing putative viral read pairs through the same filtering steps as last time, I got the following result:\n\nCode# Import and process new HV data\ndata_dir_new &lt;- \"../data/2024-02-13_crits-christoph-3/\"\nhv_reads_new_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_new_unenriched &lt;- filter(hv_reads_new, enrichment == \"Unenriched\")\nmrg_old_join &lt;- mrg_unenriched_plot_2 %&gt;% ungroup %&gt;% \n              select(seq_id, cause, viral_status, viral_status_out, genome_id_old=genome_id, taxid_old=taxid)\nmrg_new &lt;- hv_reads_new_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_new &lt;- mrg_new %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_new\n\n\n\n\n\nCodemrg_hist_new &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1))) \ng_hist_new &lt;- ggplot(mrg_hist_new, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_new\n\n\n\n\nThis is a clear improvement on the last round: the number of high-scoring non-viral sequences has fallen from 992 to 549, and the number of high-scoring unclear sequences from 83 to 56. The degree of improvement, however, is less than I’d hoped. Looking into the new alignments, this appears to be because many of the sequences mapping to the old transgenic sequences now map to non-transgenic (or at least, not labeled to be transgenic) strains of the same viruses. When BLASTed against nt, however, these sequences still map primarily to synthetic cloning vectors, suggesting these viral genomes may still be genetically modified despite not being labeled as such.\n\nCodeheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_new, header_db, by=\"genome_id\")\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta &lt;- mrg_new %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir_new, \"cc-bad-gid.fasta\"))\nbad_genomes_out &lt;- bad_genomes %&gt;%\n  left_join(mrg_new %&gt;% filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;%\n              group_by(genome_id, cause) %&gt;% summarize(.groups=\"drop\"), \n            by=\"genome_id\")\n\n\nPass 2: Additional “contaminant” screening\nIn my second attempt, I excluded further viral genomes (those with “recombinant” in the genome name) from the Bowtie database, and more importantly added several new sequences to the set of “contaminant” genomes to screen for during viral read identification. Specifically, I added one eukaryotic synthetic construct chromosome, three synthetic cloning vectors, and the Klebsiella pneumoniae genome, all of which came up during pass 1 as matches to false-positive viral sequences. Re-running the analysis with these changes, we see a large improvement:\n\nCode# Import and process new HV data\nhv_reads_newer_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_newer &lt;- read_tsv(hv_reads_newer_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newer_unenriched &lt;- filter(hv_reads_newer, enrichment == \"Unenriched\")\nmrg_newer &lt;- hv_reads_newer_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newer &lt;- mrg_newer %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newer\n\n\n\n\n\nCodemrg_hist_newer &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), mrg_newer %&gt;% mutate(attempt=2)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1,2))) \ng_hist_newer &lt;- ggplot(mrg_hist_newer, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_newer\n\n\n\n\n\nCodecounts_newer &lt;- mrg_hist_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newer) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newer\n\n\n\n  \n\n\n\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_old &lt;- range_f1(mrg_unenriched_plot_2, inc_special) %&gt;% mutate(attempt=0)\nstats_new &lt;- range_f1(mrg_new, inc_special) %&gt;% mutate(attempt=1)\nstats_newer &lt;- range_f1(mrg_newer, inc_special) %&gt;% mutate(attempt=2)\nstats_all &lt;- bind_rows(stats_old, stats_new, stats_newer) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_2 &lt;- ggplot(stats_all %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_2\n\n\n\n\nThe number of high-scoring false positives has been cut by over 400 (almost 75% compared to pass 1) and the optimal F1 score (excluding unclear read pairs for now) increased from 0.938 to 0.967. However, over 100 apparent false positives still remain, still primarily arising from cloning vectors and cow and pig sequences:\n\nCodemrg_ug_newer &lt;- left_join(mrg_newer, header_db, by=\"genome_id\")\nbad_genomes_newer &lt;- mrg_ug_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newer_caused &lt;- left_join(bad_genomes_newer, bg_causes %&gt;% select(genome_id, cause),\n                                      by=\"genome_id\")\nbad_genomes_newer_caused\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta_2 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_2 &lt;- do.call(paste, c(mrg_unenriched_fasta_2, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_2, file.path(data_dir_new, \"cc-bad-gid-2.fasta\"))\n\n\nPass 3: Unmasking “contaminant” genomes\nIn my previous attempts at this problem, I mapped reads against “contaminant” genomes having first masked the latter to remove repetitive and low-entropy sequences. This makes sense in many contexts, but may not make sense here: in particular, if the repeat sequences being masked include virus-like transposable elements, this may be responsible for the failure of my current contaminant screening approach to detect and remove some of the non-viral (especially mammalian) sequences being mistaken for viruses by Bowtie2.\nIn addition to adding a further cloning vector sequence to the contaminant sequence database for this third pass, therefore, I also tried tried re-running my analysis pipeline against an unmasked version of this database. The results looked like this:\n\nCode# Import and process new HV data\nhv_reads_newest_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_3.tsv.gz\")\nhv_reads_newest &lt;- read_tsv(hv_reads_newest_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newest_unenriched &lt;- filter(hv_reads_newest, enrichment == \"Unenriched\")\nmrg_newest &lt;- hv_reads_newest_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newest &lt;- mrg_newest %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newest\n\n\n\n\n\nCodemrg_hist_newest &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), \n                             mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), \n                             mrg_newer %&gt;% mutate(attempt=2),\n                             mrg_newest %&gt;% mutate(attempt=3)) %&gt;% \n  mutate(attempt = factor(attempt, levels=c(0,1,2,3))) \ng_hist_newest &lt;- ggplot(mrg_hist_newest, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_newest\n\n\n\n\n\nCodecounts_newest &lt;- mrg_hist_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newest) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newest\n\n\n\n  \n\n\n\n\nCodestats_newest &lt;- range_f1(mrg_newest, inc_special) %&gt;% mutate(attempt=3)\nstats_all_3 &lt;- bind_rows(stats_old, stats_new, stats_newer, stats_newest) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt_3 &lt;- stats_all_3 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_3 &lt;- ggplot(stats_all_3 %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_3\n\n\n\n\nThe number of high-scoring false positives has been cut by another 98, down to just 40, and the optimal F1 score (again excluding unclear read pairs for now) increased from to 0.98. Looking at the remaining false positives, it looks like this is entirely due to more successful removal of remaining cloning vector sequences; unmasking the cow and pig genomes seems to have had little effect:\n\nCodemrg_ug_newest &lt;- left_join(mrg_newest, header_db, by=\"genome_id\")\nbad_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newest_caused &lt;- left_join(bad_genomes_newest,\n                                       bg_causes %&gt;% select(genome_id, cause),\n                                       by=\"genome_id\")\nbad_genomes_newest_caused\n\n\n\n  \n\n\n\nWhile I would like to do better at removing these residual sequences, I think the F1 scores I’m getting are now high enough to consider this acceptable performance.\nTurning now to the “unclear” sequences, we see that, unlike the false-positive sequences, these are not concentrated in a few specific culprits, but rather spread fairly evenly over numerous viruses (44 sequences across 34 genome IDs). Inspecting these manually with NCBI BLAST, we find that most, but not all, of them appear to be real matches:\n\nCodeunclear_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_UNCLEAR &gt; 0) %&gt;% arrange(desc(n_UNCLEAR)) %&gt;%\n  select(genome_id, genome_name, n_UNCLEAR, n_FALSE, n_TRUE) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_FALSE = replace_na(n_FALSE, 0)) %&gt;%\n  mutate(p_UNCLEAR = n_UNCLEAR/(n_FALSE+n_TRUE+n_UNCLEAR))\nunclear_genomes_newest\n\n\n\n  \n\n\nCodewrite_tsv(unclear_genomes_newest, file.path(data_dir_new, \"gid-unclear-3-raw.tsv\"))\n\nmrg_unenriched_fasta_3 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_3 &lt;- do.call(paste, c(mrg_unenriched_fasta_3, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_3, file.path(data_dir_new, \"cc-unclear-gid-3.fasta\"))\n\n\n\nCodeunclear_genomes_caused &lt;- read_tsv(file.path(data_dir_new, \"gid-unclear-3.tsv\"), show_col_types = FALSE)\nmrg_newest_unclear &lt;- mrg_ug_newest %&gt;% select(-cause) %&gt;%\n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;%\n  left_join(unclear_genomes_caused %&gt;% select(genome_id, cause), by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, \n                               ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n    mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_newest_unclear %&gt;% group_by(cause) %&gt;% count\n\n\n\n  \n\n\n\n\nCodeg_unclear &lt;- mrg_newest_unclear %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nThe false matches don’t appear particularly differentiated from the true matches by alignment score, either using Bowtie2 (above) or BLAST (below):\n\nCodeblast_results_seqid &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  mutate(seq_num = as.integer(seq_num), read_pair = as.integer(read_pair), sample=fct_inorder(sample)) %&gt;%\n  left_join(mrg %&gt;% select(sample, seq_num, seq_id, taxid_bowtie = taxid), by = c(\"sample\", \"seq_num\")) \nblast_results_unclear &lt;- blast_results_seqid %&gt;%\n  inner_join(mrg_newest_unclear %&gt;% select(seq_id, viral_status, viral_status_out, cause), by = \"seq_id\") %&gt;%\n  filter(viral)\n# First check if any BLAST taxid is a descendent of the corresponding Bowtie taxid, or vice versa\ntaxid_dec_bowtie &lt;- blast_results_unclear %&gt;% pull(taxid_bowtie) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$taxid_bowtie))\ntaxid_dec_blast &lt;- blast_results_unclear %&gt;% pull(staxid) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$staxid))\nmatch_1 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$staxid[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$taxid_bowtie[n])]])\nmatch_2 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$taxid_bowtie[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$staxid[n])]])\nmatch_any &lt;- match_1 | match_2\nblast_results_matched &lt;- blast_results_unclear %&gt;% mutate(taxid_match = match_any)\n\n\n\nCode# Otherwise, take the highest-scoring, longest viral match\nblast_results_single &lt;- blast_results_matched %&gt;% group_by(sample, seq_num, read_pair) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), length = as.numeric(length)) %&gt;%\n  filter(taxid_match == max(taxid_match)) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nbrs_out &lt;- blast_results_single %&gt;% select(sample, seq_num, bitscore, viral_status_out) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"seq_num\", \"viral_status_out\"), names_from = \"read_pair\", \n              values_from = \"bitscore\", names_prefix = \"read_\") %&gt;%\n  mutate(read_1 = replace_na(read_1, 0), read_2 = replace_na(read_2, 0))\ng_brs &lt;- ggplot(brs_out, aes(x=read_1, y=read_2, color = viral_status_out)) + \n    geom_point(alpha=0.5, shape=16, size=3) + \n  scale_x_continuous(name=\"Best viral bitscore (forward read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_y_continuous(name=\"Best viral bitscore (reverse read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_brs\n\n\n\n\nOverall, I think pulling these out for manual inspection is currently the right call. But if we’re forced to classify them automatically, counting them as true viral matches is probably the better bet.\nHuman viral reads in Crits-Christoph (2021): Final assessment\nNow that I’ve settled on a pipeline and downstream analysis process I’m happy with, we can return to the question of overall human-viral abundance and composition in Crits-Christoph (2021). I’ll use a Bowtie2 alignment score cutoff of 20 here, as this is consistent with previous studies and gives good F1 scores in the tests above.\nUsing this cutoff, we find a total of 109868/8716917 human-viral reads in panel-enriched samples (\\(1.26 \\times 10^{-2}\\) , about 1 in 80), and 4064/297690777 in unenriched samples (\\(1.37 \\times 10^{-5}\\) , about 1 in 73,000). Unsurprisingly, enriched samples show much higher overall human-viral abundance than unenriched samples; however, even unenriched samples show much higher relative abundance than our previous BMC sludge sequences ( \\(\\sim 3 \\times 10^{-7}\\)):\n\nCode# Get raw read counts\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE)\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% select(sample, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nhv_reads_newest_cut &lt;- hv_reads_newest %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_newest_counts &lt;- hv_reads_newest_cut %&gt;% group_by(sample, enrichment) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_newest_counts, read_counts_raw, by=\"sample\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\ng_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, color=enrichment)) + geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  facet_wrap(~enrichment, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nIn comparison, the old pipeline returns an estimated overall relative abundance of human-infecting viruses in unenriched samples of roughly \\(3.1 \\times 10^{-6}\\), nearly 5 times lower.\nDigging into individual viruses, we see large but inconsistent differences in relative abundance between enriched and unenriched samples:\n\nCode# Import viral genera\nviral_taxids_path &lt;- file.path(data_dir_new, \"viral-taxids.tsv\")\nviral_taxids &lt;- read_tsv(viral_taxids_path, show_col_types = FALSE)\nviral_genera &lt;- viral_taxids %&gt;% filter(rank == \"genus\")\n\n# Get unique name for each viral genus\nviral_genera_unique &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() == 1)\nviral_genera_duplicate &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() &gt; 1)\nviral_genera_valid_1 &lt;- viral_genera_duplicate %&gt;% filter(grepl(\"virus$\", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_1 %&gt;% filter(n() == 1))\nviral_genera_valid_2 &lt;- viral_genera_valid_1 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(!grepl(\" \", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_2 %&gt;% filter(n() == 1))\nviral_genera_valid_3 &lt;- viral_genera_valid_2 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_3 %&gt;% filter(n() == 1))\nviral_genera_valid_4 &lt;- viral_genera_duplicate %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_4 %&gt;% filter(n() == 1))\nwrite_tsv(viral_genera_unique, file.path(data_dir_new, \"viral-genera-unique.tsv\"))\n\n# Discover viral genera for HV reads\nhigh_ranks &lt;- c(\"class\", \"family\", \"kingdom\", \"order\", \"phylum\", \"subfamily\", \"suborder\", \"subphylum\", \"superkingdom\")\nhv_read_db &lt;- hv_reads_newest_cut\ntax_nodes_cut &lt;- rename(tax_nodes, taxid = child_taxid) %&gt;% filter(taxid %in% v_taxids)\nhv_read_genus &lt;- hv_read_db %&gt;% inner_join(viral_genera_unique, by=\"taxid\")\nhv_read_nogenus &lt;- hv_read_db %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;%\n  inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n  filter(!is.na(taxid), !rank %in% high_ranks)\n#cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\nwhile(nrow(hv_read_nogenus) &gt; 0){\n  hv_read_genus &lt;- bind_rows(hv_read_genus, hv_read_nogenus %&gt;% inner_join(viral_genera_unique, by=\"taxid\"))\n  hv_read_nogenus &lt;- hv_read_nogenus %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;% select(-rank, -parent_taxid) %&gt;%\n    inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n    filter(!is.na(taxid), !rank %in% high_ranks)\n  #cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\n}\n\n# Get taxon names for higher-ranked assignments\nsmatch &lt;- hv_read_db$seq_id %in% hv_read_genus$seq_id\nhv_read_highrank &lt;- hv_read_db[!smatch,] %&gt;% inner_join(viral_taxids %&gt;% group_by(taxid) %&gt;% filter(row_number() == 1), by = \"taxid\")\n\n# Count viral genera (& unassigned viruses)\nhv_counts_wide &lt;- bind_rows(hv_read_genus, hv_read_highrank) %&gt;% group_by(name, enrichment) %&gt;% count %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"n\", values_fill = 0) \nhv_counts &lt;- hv_counts_wide %&gt;%\n  pivot_longer(-name, names_to = \"enrichment\", values_to = \"n_reads_virus\")\n\n\n\nCodehv_counts_fraction &lt;- hv_counts %&gt;%\n  inner_join(hv_reads_total %&gt;% select(enrichment, n_reads_hv, n_reads_raw), by=\"enrichment\") %&gt;%\n  mutate(p_reads_virus_all = n_reads_virus/n_reads_raw, p_reads_virus_hv = n_reads_virus/n_reads_hv)\ng_hv_counts &lt;- ggplot(hv_counts_fraction, aes(x=name, y=p_reads_virus_all, color=enrichment)) +\n  geom_point(shape=16) +\n  scale_y_log10(name = \"Relative abundance\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1), aspect.ratio = 1/5)\ng_hv_counts\n\n\n\n\nAs expected, viruses included in the respiratory virus panel see large increases in relative abundance in the enriched vs the unenriched samples, with the largest relative increases seen for Bocaparvovirus (Human bocavirus 1, 2c, 3), Betacoronavirus (SARS-CoV-2, OC43, HKU1), and Mastadenovirus (Human adenovirus B1, C2, E4). Confusingly, Orthopoxvirus, Cytomegalovirus and Gemygorvirus all also show substantially increased relative abundance, even though as far as I can tell there are no viruses from those genera in the Illumina panel. Numerous other viruses show weaker enrichment; even norovirus shows moderate (~4x) enrichment in the enriched vs the unenriched samples.\nConversely, a number of viruses are found in the unenriched samples that are absent in the enriched samples, including Rotavirus, Flavivirus, Parvovirus, and various papillomaviruses and polyomaviruses. One natural hypothesis for this is that these viruses were excluded by the enrichment panel and so had their relative abundance reduced; however, the enrichment observed for various other not-in-panel viruses calls this into question. An alternative hypothesis is that this difference is simply a consequence of the much deeper sequencing conducted on the unenriched samples (geometric mean of ~340k read pairs per enriched sample vs 42M read pairs per unenriched sample).\n\nCodehv_ra_wide &lt;- hv_counts_fraction %&gt;% select(name, enrichment, p_reads_virus_all) %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"p_reads_virus_all\", values_fill = 0) %&gt;%\n  rename(ra_enriched = Enriched, ra_unenriched = Unenriched) %&gt;%\n  mutate(relative_enrichment = log10(ra_enriched/ra_unenriched)) %&gt;%\n  arrange(desc(relative_enrichment), desc(ra_enriched), ra_unenriched)\nhv_ra_wide\n\n\n\n  \n\n\n\nAt this point, I’m satisfied with my workflow’s ability to produce usable results on the Crits-Christoph data. Next, I’ll apply this updated workflow to another previously-published WMGS dataset, likely Rothman et al. (2021)."
  },
  {
    "objectID": "notebooks/2024-02-15_crits-christoph-3.html",
    "href": "notebooks/2024-02-15_crits-christoph-3.html",
    "title": "Workflow analysis of Crits-Christoph et al. (2021), part 3",
    "section": "",
    "text": "In my last entry, I documented my first attempt at analyzing the human-infecting-virus content of sequence data from Crits-Christoph et al. (2021). In this entry, I’ll attempt to address the issues that arose in that first analysis and analyze the resulting updated data.\nAs a reminder, I previously found that the analysis pipeline I developed for our BMC data led to numerous high-scoring false-positives when run on Crits-Christoph data:\n\nCode# Data input paths\ndata_dir_old &lt;- \"../data/2024-02-04_crits-christoph-2/\"\nlibraries_path &lt;- file.path(data_dir_old, \"cc-libraries.txt\")\nhv_reads_filtered_path &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_taxids_path &lt;- file.path(data_dir_old, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir_old, \"nodes.dmp.gz\")\nblast_results_path &lt;- file.path(data_dir_old, \"cat-cc-unenriched.blast.gz\")\nbg_causes &lt;- read_tsv(file.path(data_dir_old, \"cc-bad-notr.tsv\"), show_col_types = FALSE)\nheader_path &lt;- file.path(data_dir_old, \"human-viral-headers.txt\")\n\n# Import data\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n# Import BLAST results\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% v_taxids)\n\n# Filter for the best hit for each sequence and taxid\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each sequence\nblast_results_ranked &lt;- blast_results_best %&gt;% group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\n\n# Filter by rank\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5)\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Process Bowtie/Kraken data\nmrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number())\n\n# Compare BLAST results to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  select(sample, seq_num, taxid, assigned_taxid, seq_id)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num, seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge with unenriched read data\nmrg_unenriched &lt;- mrg %&gt;% filter(enrichment == \"Unenriched\") %&gt;%\n  left_join(blast_results_out, by=c(\"sample\", \"seq_num\", \"seq_id\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\nmrg_unenriched_plot &lt;- mrg_unenriched %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_unenriched_debug &lt;- mrg_unenriched_plot %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\nmrg_unenriched_plot_2 &lt;- mrg_unenriched_debug %&gt;% left_join(bg_causes, by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n  mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_v2 &lt;- mrg_unenriched_plot_2 %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_v2\n\n\n\n\nIn total, 992 non-viral read pairs and 83 read pairs of unclear status achieved high length-adjusted alignment scores (\\(\\geq 20\\)) when mapped to viral Genbank with Bowtie2. For now, I will focus on the non-viral read pairs, and return to the unclear read pairs after these are in better shape.\nPass 1: excluding transgenic and bacterial sequences\nThe first major change I made to the pipeline was to curate the reference database for the Bowtie2 alignment to remove transgenic and other inappropriate viral sequences. To do this, I subset the collated FASTA file of viral genomes with seqtk to remove any sequence with “transgenic”, “mutant”, “recombinant”, “unverified” or “draft” in its sequence ID. This removed a total of 493 genomes from the reference DB, leaving a total of 41942 remaining for alignment with Bowtie2.\nAt the same time, I also added an E. coli genome to the set of contaminant genomes to screen against, joining cow, pig and human. I also moved the screening step to be downstream rather than upstream of the Bowtie2 filtering step, to save computation time and make it quicker to make further modifications downstream if needed. For now, I didn’t make any changes to the BBMap parameters for screening for contaminant genomes.\nAfter passing putative viral read pairs through the same filtering steps as last time, I got the following result:\n\nCode# Import and process new HV data\ndata_dir_new &lt;- \"../data/2024-02-15_crits-christoph-3/\"\nhv_reads_new_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_new_unenriched &lt;- filter(hv_reads_new, enrichment == \"Unenriched\")\nmrg_old_join &lt;- mrg_unenriched_plot_2 %&gt;% ungroup %&gt;% \n              select(seq_id, cause, viral_status, viral_status_out, genome_id_old=genome_id, taxid_old=taxid)\nmrg_new &lt;- hv_reads_new_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_new &lt;- mrg_new %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_new\n\n\n\n\n\nCodemrg_hist_new &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1))) \ng_hist_new &lt;- ggplot(mrg_hist_new, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_new\n\n\n\n\nThis is a clear improvement on the last round: the number of high-scoring non-viral sequences has fallen from 992 to 549, and the number of high-scoring unclear sequences from 83 to 56. The degree of improvement, however, is less than I’d hoped. Looking into the new alignments, this appears to be because many of the sequences mapping to the old transgenic sequences now map to non-transgenic (or at least, not labeled to be transgenic) strains of the same viruses. When BLASTed against nt, however, these sequences still map primarily to synthetic cloning vectors, suggesting these viral genomes may still be genetically modified despite not being labeled as such.\n\nCodeheader_db &lt;- read_tsv(header_path, show_col_types = FALSE,\n                      col_names = c(\"genome_id\", \"genome_name\"))\nmrg_unenriched_genomes &lt;- full_join(mrg_new, header_db, by=\"genome_id\")\nbad_genomes &lt;- mrg_unenriched_genomes %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta &lt;- mrg_new %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out &lt;- do.call(paste, c(mrg_unenriched_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out, file.path(data_dir_new, \"cc-bad-gid.fasta\"))\nbad_genomes_out &lt;- bad_genomes %&gt;%\n  left_join(mrg_new %&gt;% filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;%\n              group_by(genome_id, cause) %&gt;% summarize(.groups=\"drop\"), \n            by=\"genome_id\")\n\n\nPass 2: Additional “contaminant” screening\nIn my second attempt, I excluded further viral genomes (those with “recombinant” in the genome name) from the Bowtie database, and more importantly added several new sequences to the set of “contaminant” genomes to screen for during viral read identification. Specifically, I added one eukaryotic synthetic construct chromosome, three synthetic cloning vectors, and the Klebsiella pneumoniae genome, all of which came up during pass 1 as matches to false-positive viral sequences. Re-running the analysis with these changes, we see a large improvement:\n\nCode# Import and process new HV data\nhv_reads_newer_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_newer &lt;- read_tsv(hv_reads_newer_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newer_unenriched &lt;- filter(hv_reads_newer, enrichment == \"Unenriched\")\nmrg_newer &lt;- hv_reads_newer_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newer &lt;- mrg_newer %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newer\n\n\n\n\n\nCodemrg_hist_newer &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), mrg_newer %&gt;% mutate(attempt=2)) %&gt;% mutate(attempt = factor(attempt, levels=c(0,1,2))) \ng_hist_newer &lt;- ggplot(mrg_hist_newer, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base\ng_hist_newer\n\n\n\n\n\nCodecounts_newer &lt;- mrg_hist_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newer) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newer\n\n\n\n  \n\n\n\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_old &lt;- range_f1(mrg_unenriched_plot_2, inc_special) %&gt;% mutate(attempt=0)\nstats_new &lt;- range_f1(mrg_new, inc_special) %&gt;% mutate(attempt=1)\nstats_newer &lt;- range_f1(mrg_newer, inc_special) %&gt;% mutate(attempt=2)\nstats_all &lt;- bind_rows(stats_old, stats_new, stats_newer) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_2 &lt;- ggplot(stats_all %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_2\n\n\n\n\nThe number of high-scoring false positives has been cut by over 400 (almost 75% compared to pass 1) and the optimal F1 score (excluding unclear read pairs for now) increased from 0.938 to 0.967. However, over 100 apparent false positives still remain, still primarily arising from cloning vectors and cow and pig sequences:\n\nCodemrg_ug_newer &lt;- left_join(mrg_newer, header_db, by=\"genome_id\")\nbad_genomes_newer &lt;- mrg_ug_newer %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newer_caused &lt;- left_join(bad_genomes_newer, bg_causes %&gt;% select(genome_id, cause),\n                                      by=\"genome_id\")\nbad_genomes_newer_caused\n\n\n\n  \n\n\n\n\nCodemrg_unenriched_fasta_2 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"FALSE\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_2 &lt;- do.call(paste, c(mrg_unenriched_fasta_2, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_2, file.path(data_dir_new, \"cc-bad-gid-2.fasta\"))\n\n\nPass 3: Unmasking “contaminant” genomes\nIn my previous attempts at this problem, I mapped reads against “contaminant” genomes having first masked the latter to remove repetitive and low-entropy sequences. This makes sense in many contexts, but may not make sense here: in particular, if the repeat sequences being masked include virus-like transposable elements, this may be responsible for the failure of my current contaminant screening approach to detect and remove some of the non-viral (especially mammalian) sequences being mistaken for viruses by Bowtie2.\nIn addition to adding a further cloning vector sequence to the contaminant sequence database for this third pass, therefore, I also tried tried re-running my analysis pipeline against an unmasked version of this database. The results looked like this:\n\nCode# Import and process new HV data\nhv_reads_newest_path &lt;- file.path(data_dir_new, \"hv_hits_putative_filtered_3.tsv.gz\")\nhv_reads_newest &lt;- read_tsv(hv_reads_newest_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         contained = seq_id %in% mrg$seq_id)\nhv_reads_newest_unenriched &lt;- filter(hv_reads_newest, enrichment == \"Unenriched\")\nmrg_newest &lt;- hv_reads_newest_unenriched %&gt;%\n  left_join(mrg_old_join, by=\"seq_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status_out = replace_na(viral_status_out, \"UNCLEAR\"),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\n\n# Make initial plot\ng_mrg_newest &lt;- mrg_newest %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_newest\n\n\n\n\n\nCodemrg_hist_newest &lt;- bind_rows(mrg_new %&gt;% mutate(attempt = 1), \n                             mrg_unenriched_plot_2 %&gt;% mutate(attempt = 0), \n                             mrg_newer %&gt;% mutate(attempt=2),\n                             mrg_newest %&gt;% mutate(attempt=3)) %&gt;% \n  mutate(attempt = factor(attempt, levels=c(0,1,2,3))) \ng_hist_newest &lt;- ggplot(mrg_hist_newest, aes(x=adj_score_max, fill=attempt, group=attempt)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_newest\n\n\n\n\n\nCodecounts_newest &lt;- mrg_hist_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% group_by(viral_status_out, attempt) %&gt;% count %&gt;% ungroup\nnames(counts_newest) &lt;- c(\"Viral Status\", \"Attempt\", \"High-Scoring Read Pairs\")\ncounts_newest\n\n\n\n  \n\n\n\n\nCodestats_newest &lt;- range_f1(mrg_newest, inc_special) %&gt;% mutate(attempt=3)\nstats_all_3 &lt;- bind_rows(stats_old, stats_new, stats_newer, stats_newest) %&gt;% \n  mutate(attempt = as.factor(attempt))\nthreshold_opt_3 &lt;- stats_all_3 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_3 &lt;- ggplot(stats_all_3 %&gt;% filter(metric == \"f1\"),\n                    aes(x=threshold, y=value, color=attempt)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set1\")+\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_3\n\n\n\n\nThe number of high-scoring false positives has been cut by another 98, down to just 40, and the optimal F1 score (again excluding unclear read pairs for now) increased from to 0.98. Looking at the remaining false positives, it looks like this is entirely due to more successful removal of remaining cloning vector sequences; unmasking the cow and pig genomes seems to have had little effect:\n\nCodemrg_ug_newest &lt;- left_join(mrg_newest, header_db, by=\"genome_id\")\nbad_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_FALSE &gt; 0) %&gt;% arrange(desc(n_FALSE)) %&gt;%\n  select(genome_id, genome_name, n_FALSE, n_TRUE, n_UNCLEAR) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_UNCLEAR = replace_na(n_UNCLEAR, 0)) %&gt;%\n  mutate(p_FALSE = n_FALSE/(n_FALSE+n_TRUE+n_UNCLEAR))\nbad_genomes_newest_caused &lt;- left_join(bad_genomes_newest,\n                                       bg_causes %&gt;% select(genome_id, cause),\n                                       by=\"genome_id\")\nbad_genomes_newest_caused\n\n\n\n  \n\n\n\nWhile I would like to do better at removing these residual sequences, I think the F1 scores I’m getting are now high enough to consider this acceptable performance.\nTurning now to the “unclear” sequences, we see that, unlike the false-positive sequences, these are not concentrated in a few specific culprits, but rather spread fairly evenly over numerous viruses (44 sequences across 34 genome IDs). Inspecting these manually with NCBI BLAST, we find that most, but not all, of them appear to be real matches:\n\nCodeunclear_genomes_newest &lt;- mrg_ug_newest %&gt;% filter(adj_score_max &gt;= 20) %&gt;% \n  group_by(genome_id, genome_name, viral_status_out) %&gt;% count() %&gt;% \n  pivot_wider(names_from=viral_status_out, values_from=n, names_prefix = \"n_\") %&gt;%\n  filter(n_UNCLEAR &gt; 0) %&gt;% arrange(desc(n_UNCLEAR)) %&gt;%\n  select(genome_id, genome_name, n_UNCLEAR, n_FALSE, n_TRUE) %&gt;%\n  mutate(n_TRUE = replace_na(n_TRUE, 0), n_FALSE = replace_na(n_FALSE, 0)) %&gt;%\n  mutate(p_UNCLEAR = n_UNCLEAR/(n_FALSE+n_TRUE+n_UNCLEAR))\nunclear_genomes_newest\n\n\n\n  \n\n\nCodewrite_tsv(unclear_genomes_newest, file.path(data_dir_new, \"gid-unclear-3-raw.tsv\"))\n\nmrg_unenriched_fasta_3 &lt;- mrg_newer %&gt;% \n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;% \n  group_by(genome_id) %&gt;% \n  mutate(nseq = n()) %&gt;% arrange(desc(nseq), desc(adj_score_max)) %&gt;%\n  filter(row_number() &lt;= 10) %&gt;% \n  mutate(seq_num_gid = row_number(), seq_head = paste0(\"&gt;\", genome_id, \"_\", seq_num_gid)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmu_fasta_out_3 &lt;- do.call(paste, c(mrg_unenriched_fasta_3, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mu_fasta_out_3, file.path(data_dir_new, \"cc-unclear-gid-3.fasta\"))\n\n\n\nCodeunclear_genomes_caused &lt;- read_tsv(file.path(data_dir_new, \"gid-unclear-3.tsv\"), show_col_types = FALSE)\nmrg_newest_unclear &lt;- mrg_ug_newest %&gt;% select(-cause) %&gt;%\n  filter(adj_score_max &gt;= 20, viral_status_out == \"UNCLEAR\") %&gt;%\n  left_join(unclear_genomes_caused %&gt;% select(genome_id, cause), by=\"genome_id\") %&gt;%\n  mutate(cause = replace_na(cause, \"NA\"),\n         viral_status = ifelse(cause == \"Appears real\", 2, \n                               ifelse(cause == \"No match\", pmax(1, viral_status), viral_status))) %&gt;%\n    mutate(viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\nmrg_newest_unclear %&gt;% group_by(cause) %&gt;% count\n\n\n\n  \n\n\n\n\nCodeg_unclear &lt;- mrg_newest_unclear %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nThe false matches don’t appear particularly differentiated from the true matches by alignment score, either using Bowtie2 (above) or BLAST (below):\n\nCodeblast_results_seqid &lt;- blast_results_highrank %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  mutate(seq_num = as.integer(seq_num), read_pair = as.integer(read_pair), sample=fct_inorder(sample)) %&gt;%\n  left_join(mrg %&gt;% select(sample, seq_num, seq_id, taxid_bowtie = taxid), by = c(\"sample\", \"seq_num\")) \nblast_results_unclear &lt;- blast_results_seqid %&gt;%\n  inner_join(mrg_newest_unclear %&gt;% select(seq_id, viral_status, viral_status_out, cause), by = \"seq_id\") %&gt;%\n  filter(viral)\n# First check if any BLAST taxid is a descendent of the corresponding Bowtie taxid, or vice versa\ntaxid_dec_bowtie &lt;- blast_results_unclear %&gt;% pull(taxid_bowtie) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$taxid_bowtie))\ntaxid_dec_blast &lt;- blast_results_unclear %&gt;% pull(staxid) %&gt;% unique %&gt;% as.numeric %&gt;% lapply(expand_taxids, nodes=tax_nodes) %&gt;% setNames(unique(blast_results_unclear$staxid))\nmatch_1 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$staxid[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$taxid_bowtie[n])]])\nmatch_2 &lt;- sapply(1:nrow(blast_results_unclear), function(n) as.numeric(blast_results_unclear$taxid_bowtie[n]) %in% taxid_dec_bowtie[[as.character(blast_results_unclear$staxid[n])]])\nmatch_any &lt;- match_1 | match_2\nblast_results_matched &lt;- blast_results_unclear %&gt;% mutate(taxid_match = match_any)\n\n\n\nCode# Otherwise, take the highest-scoring, longest viral match\nblast_results_single &lt;- blast_results_matched %&gt;% group_by(sample, seq_num, read_pair) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), length = as.numeric(length)) %&gt;%\n  filter(taxid_match == max(taxid_match)) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nbrs_out &lt;- blast_results_single %&gt;% select(sample, seq_num, bitscore, viral_status_out) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"seq_num\", \"viral_status_out\"), names_from = \"read_pair\", \n              values_from = \"bitscore\", names_prefix = \"read_\") %&gt;%\n  mutate(read_1 = replace_na(read_1, 0), read_2 = replace_na(read_2, 0))\ng_brs &lt;- ggplot(brs_out, aes(x=read_1, y=read_2, color = viral_status_out)) + \n    geom_point(alpha=0.5, shape=16, size=3) + \n  scale_x_continuous(name=\"Best viral bitscore (forward read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_y_continuous(name=\"Best viral bitscore (reverse read)\", limits=c(0,160), breaks=seq(0,160,40), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_brs\n\n\n\n\nOverall, I think pulling these out for manual inspection is currently the right call. But if we’re forced to classify them automatically, counting them as true viral matches is probably the better bet.\nHuman viral reads in Crits-Christoph (2021): Final assessment\nNow that I’ve settled on a pipeline and downstream analysis process I’m happy with, we can return to the question of overall human-viral abundance and composition in Crits-Christoph (2021). I’ll use a Bowtie2 alignment score cutoff of 20 here, as this is consistent with previous studies and gives good F1 scores in the tests above.\nUsing this cutoff, we find a total of 109868/8716917 human-viral reads in panel-enriched samples (\\(1.26 \\times 10^{-2}\\) , about 1 in 80), and 4064/297690777 in unenriched samples (\\(1.37 \\times 10^{-5}\\) , about 1 in 73,000). Unsurprisingly, enriched samples show much higher overall human-viral abundance than unenriched samples; however, even unenriched samples show much higher relative abundance than our previous BMC sludge sequences ( \\(\\sim 3 \\times 10^{-7}\\)):\n\nCode# Get raw read counts\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE)\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% select(sample, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nhv_reads_newest_cut &lt;- hv_reads_newest %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_newest_counts &lt;- hv_reads_newest_cut %&gt;% group_by(sample, enrichment) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_newest_counts, read_counts_raw, by=\"sample\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\ng_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, color=enrichment)) + geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  facet_wrap(~enrichment, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nIn comparison, the old pipeline returns an estimated overall relative abundance of human-infecting viruses in unenriched samples of roughly \\(3.1 \\times 10^{-6}\\), nearly 5 times lower.\nDigging into individual viruses, we see large but inconsistent differences in relative abundance between enriched and unenriched samples:\n\nCode# Import viral genera\nviral_taxids_path &lt;- file.path(data_dir_new, \"viral-taxids.tsv\")\nviral_taxids &lt;- read_tsv(viral_taxids_path, show_col_types = FALSE)\nviral_genera &lt;- viral_taxids %&gt;% filter(rank == \"genus\")\n\n# Get unique name for each viral genus\nviral_genera_unique &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() == 1)\nviral_genera_duplicate &lt;- viral_genera %&gt;% group_by(taxid) %&gt;% filter(n() &gt; 1)\nviral_genera_valid_1 &lt;- viral_genera_duplicate %&gt;% filter(grepl(\"virus$\", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_1 %&gt;% filter(n() == 1))\nviral_genera_valid_2 &lt;- viral_genera_valid_1 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(!grepl(\" \", name))\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_2 %&gt;% filter(n() == 1))\nviral_genera_valid_3 &lt;- viral_genera_valid_2 %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_3 %&gt;% filter(n() == 1))\nviral_genera_valid_4 &lt;- viral_genera_duplicate %&gt;% filter(! taxid %in% viral_genera_unique$taxid) %&gt;%\n  filter(row_number() == 1)\nviral_genera_unique &lt;- bind_rows(viral_genera_unique, viral_genera_valid_4 %&gt;% filter(n() == 1))\nwrite_tsv(viral_genera_unique, file.path(data_dir_new, \"viral-genera-unique.tsv\"))\n\n# Discover viral genera for HV reads\nhigh_ranks &lt;- c(\"class\", \"family\", \"kingdom\", \"order\", \"phylum\", \"subfamily\", \"suborder\", \"subphylum\", \"superkingdom\")\nhv_read_db &lt;- hv_reads_newest_cut\ntax_nodes_cut &lt;- rename(tax_nodes, taxid = child_taxid) %&gt;% filter(taxid %in% v_taxids)\nhv_read_genus &lt;- hv_read_db %&gt;% inner_join(viral_genera_unique, by=\"taxid\")\nhv_read_nogenus &lt;- hv_read_db %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;%\n  inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n  filter(!is.na(taxid), !rank %in% high_ranks)\n#cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\nwhile(nrow(hv_read_nogenus) &gt; 0){\n  hv_read_genus &lt;- bind_rows(hv_read_genus, hv_read_nogenus %&gt;% inner_join(viral_genera_unique, by=\"taxid\"))\n  hv_read_nogenus &lt;- hv_read_nogenus %&gt;% filter(!taxid %in% viral_genera_unique$taxid) %&gt;% select(-rank, -parent_taxid) %&gt;%\n    inner_join(tax_nodes_cut, by=\"taxid\") %&gt;% mutate(taxid = parent_taxid) %&gt;%\n    filter(!is.na(taxid), !rank %in% high_ranks)\n  #cat(nrow(hv_read_db), nrow(hv_read_genus), nrow(hv_read_nogenus), nrow(hv_read_genus)+nrow(hv_read_nogenus), \"\\n\")\n}\n\n# Get taxon names for higher-ranked assignments\nsmatch &lt;- hv_read_db$seq_id %in% hv_read_genus$seq_id\nhv_read_highrank &lt;- hv_read_db[!smatch,] %&gt;% inner_join(viral_taxids %&gt;% group_by(taxid) %&gt;% filter(row_number() == 1), by = \"taxid\")\n\n# Count viral genera (& unassigned viruses)\nhv_counts_wide &lt;- bind_rows(hv_read_genus, hv_read_highrank) %&gt;% group_by(name, enrichment) %&gt;% count %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"n\", values_fill = 0) \nhv_counts &lt;- hv_counts_wide %&gt;%\n  pivot_longer(-name, names_to = \"enrichment\", values_to = \"n_reads_virus\")\n\n\n\nCodehv_counts_fraction &lt;- hv_counts %&gt;%\n  inner_join(hv_reads_total %&gt;% select(enrichment, n_reads_hv, n_reads_raw), by=\"enrichment\") %&gt;%\n  mutate(p_reads_virus_all = n_reads_virus/n_reads_raw, p_reads_virus_hv = n_reads_virus/n_reads_hv)\ng_hv_counts &lt;- ggplot(hv_counts_fraction, aes(x=name, y=p_reads_virus_all, color=enrichment)) +\n  geom_point(shape=16) +\n  scale_y_log10(name = \"Relative abundance\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1), aspect.ratio = 1/5)\ng_hv_counts\n\n\n\n\nAs expected, viruses included in the respiratory virus panel see large increases in relative abundance in the enriched vs the unenriched samples, with the largest relative increases seen for Bocaparvovirus (Human bocavirus 1, 2c, 3), Betacoronavirus (SARS-CoV-2, OC43, HKU1), and Mastadenovirus (Human adenovirus B1, C2, E4). Confusingly, Orthopoxvirus, Cytomegalovirus and Gemygorvirus all also show substantially increased relative abundance, even though as far as I can tell there are no viruses from those genera in the Illumina panel. Numerous other viruses show weaker enrichment; even norovirus shows moderate (~4x) enrichment in the enriched vs the unenriched samples.\nConversely, a number of viruses are found in the unenriched samples that are absent in the enriched samples, including Rotavirus, Flavivirus, Parvovirus, and various papillomaviruses and polyomaviruses. One natural hypothesis for this is that these viruses were excluded by the enrichment panel and so had their relative abundance reduced; however, the enrichment observed for various other not-in-panel viruses calls this into question. An alternative hypothesis is that this difference is simply a consequence of the much deeper sequencing conducted on the unenriched samples (geometric mean of ~340k read pairs per enriched sample vs 42M read pairs per unenriched sample).\n\nCodehv_ra_wide &lt;- hv_counts_fraction %&gt;% select(name, enrichment, p_reads_virus_all) %&gt;%\n  pivot_wider(id_cols = \"name\", names_from = \"enrichment\", values_from = \"p_reads_virus_all\", values_fill = 0) %&gt;%\n  rename(ra_enriched = Enriched, ra_unenriched = Unenriched) %&gt;%\n  mutate(relative_enrichment = log10(ra_enriched/ra_unenriched)) %&gt;%\n  arrange(desc(relative_enrichment), desc(ra_enriched), ra_unenriched)\nhv_ra_wide\n\n\n\n  \n\n\n\nAt this point, I’m satisfied with my workflow’s ability to produce usable results on the Crits-Christoph data. Next, I’ll apply this updated workflow to another previously-published WMGS dataset, likely Rothman et al. (2021)."
  },
  {
    "objectID": "notebooks/2024-02-23_rothman-1.html",
    "href": "notebooks/2024-02-23_rothman-1.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 1",
    "section": "",
    "text": "In my last entry, I finished workflow analysis of Crits-Christoph et al. 2021 enriched and unenriched MGS data. In this entry, I apply the workflow to another pre-existing dataset from the P2RA project, namely Rothman et al. 2021.\nThis is a much bigger dataset than Crits-Christoph, with 6 billion reads spread out over 363 different samples. Interestingly, Nextflow seemed to struggle more with the number of samples than their size; when running the entire dataset through the pipeline I kept running into issues with caching that seemed to be due to the sheer number of jobs in the pipeline. I’ll keep working on these, but in the meantime I decided to separate the samples from the dataset into groups to make running them through the pipeline easier.\nIn this entry, I analyze the results of the pipeline on the 97 unenriched samples from the pipeline. These were collected from 9 California treatment plants between July 2020 and January 2021, pasteurized, filtered through a 0.22um filter and concentrated with 10-kDa Amicon filters. After this, they underwent RNA-seq library prep and were sequenced on an Illumina NovaSeq 6000.\nThe raw data\nThe Rothman unenriched samples totaled roughly 660M read pairs (151 gigabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 6.7 to 165.9M, with an average of 73.3M.\nWhile many samples had low levels of sequence duplication according to FASTQC, others had very high levels of duplication in excess of 75%. The median level of duplication according to FASTQ was roughly 56%. Adapter levels were high.\nRead qualities for most samples were consistently very high, but some samples showed a large drop in sequence quality around position 20, suggesting a serious need for preprocessing to remove low-quality bases.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-23_rothman-1/\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-unenriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 60% of total reads from unenriched samples, with the largest reductions seen during deduplication. Secondary ribodepletion removing a further 6%. However, these summary figures conceal significant inter-sample variation. Unsurprisingly, samples that showed a higher initial level of duplication also showed greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples, but failed to correct the large mid-read drop in quality seen in many samples. Since this group actually accounted for over half of samples, I decided to retain it for now, but will investigate ways to trim or discard such reads in future analyses, including of this data set.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 14%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nOverall, in these unenriched samples, between 66% and 86% of reads (mean 74%) are low-quality, duplicates, or unassigned. Of the remainder, 8-21% (mean 15%) were identified as ribosomal, and 2-12% (mean 5%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.4-6.5% (mean 1.8%) and that of viral reads from 0.6-8.6% (mean 4.5%).\nThe latter of these is strikingly high, about an order of magnitude higher than that observed in Crits-Christoph. In the public dashboard, high levels of total virus reads are also observed, and attributed almost entirely to elevated levels of tobamoviruses, especially tomato brown rugose fruit virus. Digging into the Kraken reports for these samples, we find that for most samples, Tobamovirus indeed makes up the vast majority of viral reads:\n\nCodetoba_path &lt;- file.path(data_dir, \"tobamovirus.tsv\")\ntoba &lt;- read_tsv(toba_path, show_col_types = FALSE)\ng_toba &lt;- ggplot(toba, aes(x=p_viral_reads_Tobamovirus)) +\n  geom_histogram(binwidth = 0.01, boundary=0) +\n  scale_x_continuous(name = \"% Viral reads assigned to Tobamovirus\",\n                     limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0),\n                     labels = function(x) x*100) +\n  scale_y_continuous(\"# samples\", breaks = seq(0,100,10),\n                   expand = c(0,0)) +\n  theme_base\ng_toba\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a modified form of the pipeline used in my last entry:\n\nAfter initial ribodepletion, surviving reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters.\nReads that aligned successfully with Bowtie2 were filtered to remove human, livestock, cloning vector, and other potential contaminant sequences, then run through Kraken2 using the standard 16GB database.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 16034 read pairs across all unenriched samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nTo investigate these putative viral reads, I ran them through BLASTN and analyzed the results in a similar manner to Crits-Christoph:\n\nCodemrg_num &lt;- mrg %&gt;% group_by(sample) %&gt;% \n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"rothman-putative-viral.fasta\"))\n\n\n\nCode# Import viral taxids and relationships\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"rothman-putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis initially looks pretty good, with high-scoring true-positives massively outnumbering false-positives and unclear cases. However, there are also a lot of low-scoring true-positives, which are excluded when the alignment score threshold is placed high enough to exclude the majority of false-positives. This pulls down sensitivity at higher score thresholds; as a result, F1 score is lower than I’d like, peaking at 0.947 for a disjunctive cutoff of 17 and dropping to 0.921 for a cutoff of 20.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nImproving precision for high-scoring sequences will only have a marginal effect on F1 score here, as the limiting factor is sensitivity. The best way to improve this would be to find ways to better exclude low-scoring false-positives from the dataset, then lower the score threshold closer to 15. I’m wary about doing this, however, as I suspect low-scoring false-positives will be substantially harder to remove than high-scoring ones, and I’m somewhat less worried about false-negatives than false-positives here in any case. As such, I’m going to stick with my current pipeline for now.\n\nCodemrg_unclear &lt;- mrg_blast %&gt;% ungroup %&gt;% filter(viral_status_out == \"UNCLEAR\") %&gt;%\n  arrange(sample, seq_num)\nmrg_unclear_fasta &lt;- mrg_unclear  %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_unclear_fasta_out &lt;- do.call(paste, c(mrg_unclear_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_unclear_fasta_out, file.path(data_dir, \"rothman-unclear-viral.fasta\"))\n\n\nMoving on to the small number of “unclear” viral matches (155 total, 107 high-scoring), as with Crits-Christoph, the vast majority of these appear to be true positives when run on NCBI BLAST online:\n\nCodeunclear_viral_status &lt;- c(\n  rep(TRUE, 53), FALSE, TRUE,\n  rep(TRUE, 13), FALSE, rep(TRUE, 15), FALSE, rep(TRUE, 6), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 5),\n  rep(TRUE, 2), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 29)\n)\nmrg_unclear_out &lt;- mrg_unclear %&gt;% mutate(viral_status_true = unclear_viral_status)\ng_unclear &lt;- mrg_unclear_out %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_true)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nSince this is two large datasets for which I’ve now gotten this result, I’ll henceforth treat reads for which only one of two reads shows a BLAST viral match as true positives for validation purposes.\nHuman-infecting virus reads: relative abundance\nIn total, my pipeline identified 12305 reads as human-viral out of 660M total reads, for a relative HV abundance of \\(1.87 \\times 10^{-5}\\). This compares to \\(4.3 \\times 10^{-6}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a 4.4-fold increase, similar to that observed for Crits-Christoph. Excluding false-positives identified by BLAST above, the new estimate is reduced slightly, to \\(1.81 \\times 10^{-5}\\).\nAggregating by treatment plant location, we see substantial variation in HV relative abundance, from \\(7.5 \\times 10^{-7}\\) for location JWPCP to \\(5.91 \\times 10^{-5}\\) for location SB:\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_blast %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20,\n         hv_true = viral_status &gt;= 1)\nread_counts_hv_all &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_all\")\nread_counts_hv_val &lt;- mrg_hv %&gt;% filter(hv_status, hv_true) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_validated\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv_all, by=\"sample\") %&gt;%\n  left_join(read_counts_hv_val, by=\"sample\") %&gt;%\n  mutate(n_reads_hv_all = replace_na(n_reads_hv_all, 0),\n         n_reads_hv_validated = replace_na(n_reads_hv_validated, 0),\n         p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_melted &lt;- read_counts %&gt;% \n  select(sample, location, date, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group))) %&gt;%\n  arrange(location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\nread_counts_agg_melted &lt;- read_counts_agg %&gt;% \n  select(location, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group)))\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg_melted, aes(x=location, y=p, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  facet_wrap(~group, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))#\ng_phv_agg\n\n\n\n\nPlotting HV relative abundance over time for each plant, we see that while some plants remain roughly stable over time, several others exhibit a gradual increase in HV relative abundance over the course of the study, possibly related to the winter disease season:\n\nCode# Visualize\ng_phv_time &lt;- ggplot(read_counts_melted, aes(x=date, y=p, color=location, linetype=group)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_linetype_discrete(name = \"Estimator\") +\n  facet_grid(location~.) +\n  theme_base\ng_phv_time\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially noroviruses (e.g. Norwalk virus) and hudisaviruses, though betacoronaviruses including SARS-CoV-2 make a respectable showing in several samples:\n\nCode# Get viral taxon names for putative HV reads\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, name) %&gt;%\n    summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, name) %&gt;%\n  summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results make sense aren’t especially surprising. In my next entry, I’ll turn to the enriched samples from Rothman, and compare them to these unenriched samples."
  },
  {
    "objectID": "notebooks/2024-02-27_rothman-1.html",
    "href": "notebooks/2024-02-27_rothman-1.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 1",
    "section": "",
    "text": "In my last entry, I finished workflow analysis of Crits-Christoph et al. 2021 enriched and unenriched MGS data. In this entry, I apply the workflow to another pre-existing dataset from the P2RA project, namely Rothman et al. 2021.\nThis is a much bigger dataset than Crits-Christoph, with 6 billion reads spread out over 363 different samples. Interestingly, Nextflow seemed to struggle more with the number of samples than their size; when running the entire dataset through the pipeline I kept running into issues with caching that seemed to be due to the sheer number of jobs in the pipeline. I’ll keep working on these, but in the meantime I decided to separate the samples from the dataset into groups to make running them through the pipeline easier.\nIn this entry, I analyze the results of the pipeline on the 97 unenriched samples from the pipeline. These were collected from 9 California treatment plants between July 2020 and January 2021, pasteurized, filtered through a 0.22um filter and concentrated with 10-kDa Amicon filters. After this, they underwent RNA-seq library prep and were sequenced on an Illumina NovaSeq 6000.\nThe raw data\nThe Rothman unenriched samples totaled roughly 660M read pairs (151 gigabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 6.7 to 165.9M, with an average of 73.3M.\nWhile many samples had low levels of sequence duplication according to FASTQC, others had very high levels of duplication in excess of 75%. The median level of duplication according to FASTQ was roughly 56%. Adapter levels were high.\nRead qualities for most samples were consistently very high, but some samples showed a large drop in sequence quality around position 20, suggesting a serious need for preprocessing to remove low-quality bases.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-23_rothman-1/\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-unenriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nOn average, cleaning, deduplication and conservative ribodepletion together removed about 60% of total reads from unenriched samples, with the largest reductions seen during deduplication. Secondary ribodepletion removing a further 6%. However, these summary figures conceal significant inter-sample variation. Unsurprisingly, samples that showed a higher initial level of duplication also showed greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples, but failed to correct the large mid-read drop in quality seen in many samples. Since this group actually accounted for over half of samples, I decided to retain it for now, but will investigate ways to trim or discard such reads in future analyses, including of this data set.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 14%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_summ, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_summ %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=location, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~enrichment, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nOverall, in these unenriched samples, between 66% and 86% of reads (mean 74%) are low-quality, duplicates, or unassigned. Of the remainder, 8-21% (mean 15%) were identified as ribosomal, and 2-12% (mean 5%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.4-6.5% (mean 1.8%) and that of viral reads from 0.6-8.6% (mean 4.5%).\nThe latter of these is strikingly high, about an order of magnitude higher than that observed in Crits-Christoph. In the public dashboard, high levels of total virus reads are also observed, and attributed almost entirely to elevated levels of tobamoviruses, especially tomato brown rugose fruit virus. Digging into the Kraken reports for these samples, we find that for most samples, Tobamovirus indeed makes up the vast majority of viral reads:\n\nCodetoba_path &lt;- file.path(data_dir, \"tobamovirus.tsv\")\ntoba &lt;- read_tsv(toba_path, show_col_types = FALSE)\ng_toba &lt;- ggplot(toba, aes(x=p_viral_reads_Tobamovirus)) +\n  geom_histogram(binwidth = 0.01, boundary=0) +\n  scale_x_continuous(name = \"% Viral reads assigned to Tobamovirus\",\n                     limits = c(0,1), breaks = seq(0,1,0.2), expand = c(0,0),\n                     labels = function(x) x*100) +\n  scale_y_continuous(\"# samples\", breaks = seq(0,100,10),\n                   expand = c(0,0)) +\n  theme_base\ng_toba\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a modified form of the pipeline used in my last entry:\n\nAfter initial ribodepletion, surviving reads were aligned to a DB of human-infecting virus genomes with Bowtie2, using permissive alignment parameters.\nReads that aligned successfully with Bowtie2 were filtered to remove human, livestock, cloning vector, and other potential contaminant sequences, then run through Kraken2 using the standard 16GB database.\nFor surviving reads, the length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\) was calculated, and reads were filtered out if they didn’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\nApplying all of these filtering steps leaves a total of 16034 read pairs across all unenriched samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location, enrichment) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nTo investigate these putative viral reads, I ran them through BLASTN and analyzed the results in a similar manner to Crits-Christoph:\n\nCodemrg_num &lt;- mrg %&gt;% group_by(sample) %&gt;% \n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"rothman-putative-viral.fasta\"))\n\n\n\nCode# Import viral taxids and relationships\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"rothman-putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, \"FALSE\",\n                                   ifelse(viral_status == 1, \"UNCLEAR\", \"TRUE\")),\n         viral_status_out = factor(viral_status_out, levels = c(\"FALSE\", \"UNCLEAR\", \"TRUE\")))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis initially looks pretty good, with high-scoring true-positives massively outnumbering false-positives and unclear cases. However, there are also a lot of low-scoring true-positives, which are excluded when the alignment score threshold is placed high enough to exclude the majority of false-positives. This pulls down sensitivity at higher score thresholds; as a result, F1 score is lower than I’d like, peaking at 0.947 for a disjunctive cutoff of 17 and dropping to 0.921 for a cutoff of 20.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nImproving precision for high-scoring sequences will only have a marginal effect on F1 score here, as the limiting factor is sensitivity. The best way to improve this would be to find ways to better exclude low-scoring false-positives from the dataset, then lower the score threshold closer to 15. I’m wary about doing this, however, as I suspect low-scoring false-positives will be substantially harder to remove than high-scoring ones, and I’m somewhat less worried about false-negatives than false-positives here in any case. As such, I’m going to stick with my current pipeline for now.\n\nCodemrg_unclear &lt;- mrg_blast %&gt;% ungroup %&gt;% filter(viral_status_out == \"UNCLEAR\") %&gt;%\n  arrange(sample, seq_num)\nmrg_unclear_fasta &lt;- mrg_unclear  %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_unclear_fasta_out &lt;- do.call(paste, c(mrg_unclear_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_unclear_fasta_out, file.path(data_dir, \"rothman-unclear-viral.fasta\"))\n\n\nMoving on to the small number of “unclear” viral matches (155 total, 107 high-scoring), as with Crits-Christoph, the vast majority of these appear to be true positives when run on NCBI BLAST online:\n\nCodeunclear_viral_status &lt;- c(\n  rep(TRUE, 53), FALSE, TRUE,\n  rep(TRUE, 13), FALSE, rep(TRUE, 15), FALSE, rep(TRUE, 6), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 5),\n  rep(TRUE, 2), FALSE, rep(TRUE, 12), FALSE, rep(TRUE, 29)\n)\nmrg_unclear_out &lt;- mrg_unclear %&gt;% mutate(viral_status_true = unclear_viral_status)\ng_unclear &lt;- mrg_unclear_out %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_true)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base + theme(aspect.ratio=1)\ng_unclear\n\n\n\n\nSince this is two large datasets for which I’ve now gotten this result, I’ll henceforth treat reads for which only one of two reads shows a BLAST viral match as true positives for validation purposes.\nHuman-infecting virus reads: relative abundance\nIn total, my pipeline identified 12305 reads as human-viral out of 660M total reads, for a relative HV abundance of \\(1.87 \\times 10^{-5}\\). This compares to \\(4.3 \\times 10^{-6}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a 4.4-fold increase, similar to that observed for Crits-Christoph. Excluding false-positives identified by BLAST above, the new estimate is reduced slightly, to \\(1.81 \\times 10^{-5}\\).\nAggregating by treatment plant location, we see substantial variation in HV relative abundance, from \\(7.5 \\times 10^{-7}\\) for location JWPCP to \\(5.91 \\times 10^{-5}\\) for location SB:\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_blast %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20,\n         hv_true = viral_status &gt;= 1)\nread_counts_hv_all &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_all\")\nread_counts_hv_val &lt;- mrg_hv %&gt;% filter(hv_status, hv_true) %&gt;% group_by(sample) %&gt;%\n  count(name = \"n_reads_hv_validated\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv_all, by=\"sample\") %&gt;%\n  left_join(read_counts_hv_val, by=\"sample\") %&gt;%\n  mutate(n_reads_hv_all = replace_na(n_reads_hv_all, 0),\n         n_reads_hv_validated = replace_na(n_reads_hv_validated, 0),\n         p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_melted &lt;- read_counts %&gt;% \n  select(sample, location, date, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group))) %&gt;%\n  arrange(location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated)) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\nread_counts_agg_melted &lt;- read_counts_agg %&gt;% \n  select(location, starts_with(\"p\")) %&gt;%\n  pivot_longer(starts_with(\"p\"), names_to = \"group\", values_to = \"p\") %&gt;%\n  mutate(group = str_to_title(sub(\"p_reads_hv_\", \"\", group)))\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg_melted, aes(x=location, y=p, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  facet_wrap(~group, scales = \"free_x\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))#\ng_phv_agg\n\n\n\n\nPlotting HV relative abundance over time for each plant, we see that while some plants remain roughly stable over time, several others exhibit a gradual increase in HV relative abundance over the course of the study, possibly related to the winter disease season:\n\nCode# Visualize\ng_phv_time &lt;- ggplot(read_counts_melted, aes(x=date, y=p, color=location, linetype=group)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_linetype_discrete(name = \"Estimator\") +\n  facet_grid(location~.) +\n  theme_base\ng_phv_time\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially noroviruses (e.g. Norwalk virus) and hudisaviruses, though betacoronaviruses including SARS-CoV-2 make a respectable showing in several samples:\n\nCode# Get viral taxon names for putative HV reads\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, name) %&gt;%\n    summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, name) %&gt;%\n  summarize(n_reads_hv_all = sum(hv_status),\n            n_reads_hv_validated = sum(hv_status & hv_true), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, n_reads_raw), by=\"location\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv_all = sum(n_reads_hv_all),\n            n_reads_hv_validated = sum(n_reads_hv_validated),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv_all = n_reads_hv_all/n_reads_raw,\n         p_reads_hv_validated = n_reads_hv_validated/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(location) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv_all), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=location, y=n_reads_hv_all, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results make sense aren’t especially surprising. In my next entry, I’ll turn to the enriched samples from Rothman, and compare them to these unenriched samples."
  },
  {
    "objectID": "notebooks/2024-02-29_rothman-2.html",
    "href": "notebooks/2024-02-29_rothman-2.html",
    "title": "Workflow analysis of Rothman et al. (2021), part 2",
    "section": "",
    "text": "In my last entry, I analyzed the unenriched samples from Rothman et al. 2021. In this entry, I extend that analysis to the 266 samples that underwent panel enrichment using the Illumina respiratory virus panel prior to sequencing. (These were otherwise processed identially to the unenriched samples described in my last entry.)\nThe raw data\nThe Rothman panel-enriched samples totaled roughly 5.4B read pairs (1.1 terabases of sequence). The number of reads per sample varied from 1.3M to 23.5M, with an average of 6.8M reads per sample. The number of reads per treatment plant varied from 0.8M to 89M, with an average of 20M. The great majority of reads came from three treatment plant locations: ESC, HTP, and PL. Duplication and adapter levels were similar to the unenriched samples, which a median FASTQC-measured duplication level of 57%. Read qualities were high, albeit with a dropoff towards the end of longer reads; the large mid-read drop seen in some unenriched samples was not observed here.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-02-29_rothman-2\"\nlibraries_path &lt;- file.path(data_dir, \"rothman-libraries-enriched.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Enriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_read_pairs, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=n_bases_approx, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=enrichment, y=percent_duplicates, fill=location, group=sample)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\nDeduplication and conservative ribodepletion together removed about 57% of total reads on average, similar to unenriched samples. Secondary ribodepletion removing a further 4% on average. However, as before, these summary figures conceal significant inter-sample variation, with samples that showed a higher initial level of duplication also showing greater read losses during preprocessing.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\nn_reads_rel &lt;- basic_stats %&gt;% select(sample, location, stage, enrichment, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location, enrichment) %&gt;% arrange(sample, location, enrichment, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs)\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\nCode# Plot read losses vs initial duplication levels\ng_loss_dup &lt;- n_reads_rel %&gt;% \n  mutate(percent_duplicates_raw = percent_duplicates[1]) %&gt;% \n  filter(stage == \"ribo_initial\") %&gt;% \n  ggplot(aes(x=percent_duplicates_raw, y=p_reads_lost_abs, color=location)) + \n  geom_point(shape = 16) +\n  scale_color_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Reads lost by initial ribodepletion)\", expand=c(0,0), labels = function(x) x*100, breaks = seq(0,1,0.2), limits = c(0,1)) +\n  scale_x_continuous(\"% FASTQC-measured duplicates in raw data\", expand=c(0,0),\n                     breaks = seq(0,100,20), limits=c(0,100)) +\n  theme_base + theme(aspect.ratio = 1)\ng_loss_dup\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with no adapter sequences found by FASTQC at any stage after the raw data. Preprocessing successfully removed the terminal decline in quality seen in many samples.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nDeduplication and ribodepletion were collectively quite effective at reducing measured duplicate levels, with the average detected duplication level after both processes reduced to roughly 11%. Note that the pipeline still doesn’t have a reverse-complement-sensitive deduplication pipeline, so only same-orientation duplicates have been removed.\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import data for unenriched samples\ndata_dir_old &lt;- \"../data/2024-02-23_rothman-1/\"\nbracken_path_old &lt;- file.path(data_dir_old, \"bracken_counts.tsv\")\nbasic_stats_path_old &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nlibraries_path_old &lt;- file.path(data_dir_old, \"rothman-libraries-unenriched.csv\")\n\nbracken_old &lt;- read_tsv(bracken_path_old, show_col_types = FALSE)\nlibraries_old &lt;- read_csv(libraries_path_old, show_col_types = FALSE) %&gt;%\n  separate(sample, c(\"location\", \"month\", \"day\", \"year\", \"x1\", \"x2\"), remove = FALSE) %&gt;%\n  mutate(month = as.numeric(month), year = as.numeric(year), day = as.numeric(day),\n         enrichment = \"Unenriched\", \n         date = ymd(paste(year, month, day, sep=\"-\"))) %&gt;%\n  select(-x1, -x2) %&gt;%\n  arrange(enrichment, location, date) %&gt;%\n  mutate(location = fct_inorder(location))\nbasic_stats_old &lt;- read_tsv(basic_stats_path_old, show_col_types = FALSE) %&gt;%\n  mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries_old, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\n\n# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\ntotal_assigned_old &lt;- bracken_old %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread_old &lt;- bracken_old %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\nread_counts_preproc_old &lt;- basic_stats_old %&gt;% \n  select(sample, location, enrichment, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\", \"enrichment\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts_old &lt;- read_counts_preproc_old %&gt;%\n  inner_join(total_assigned_old %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread_old, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:enrichment), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\nread_comp_old &lt;- transmute(read_counts_old, sample=sample, location=location,\n                       enrichment=enrichment,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long_old &lt;- pivot_longer(read_comp_old, -(sample:enrichment), \n                                   names_to = \"classification\",\n                                   names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ_old &lt;- read_comp_long_old %&gt;% \n  group_by(location, enrichment, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n# Merge data\nread_comp_mrg &lt;- bind_rows(read_comp_summ %&gt;% mutate(enrichment = \"Enriched\"),\n                           read_comp_summ_old %&gt;% mutate(enrichment = \"Unenriched\"))\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_mrg, aes(x=enrichment, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_mrg %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=enrichment, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp_minor\n\n\n\n\nThe average fraction of low-quality, duplicate, and unassigned reads is slightly higher in enriched vs unenriched samples (mean 80% vs 75%), while the average fraction of ribosomal reads is lower (10% vs 15%). The fraction of bacterial reads is similar (7% vs 5%), while the overall fraction of viral reads is actually slightly lower (2% vs 5%). This latter finding is perhaps surprising, given that the enriched samples are enriched for a group of viruses; I suspect the observed results are due to non-human viruses (especially tobamoviruses) dominating human viruses in the overall counts.\nHuman-infecting virus reads\nNow we come to the main result of interest: the fraction of human-infecting virus reads in enriched vs unenriched samples.\n\nCode# New HV reads\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Old HV reads\nhv_reads_filtered_path_old &lt;- file.path(data_dir_old, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered_old &lt;- read_tsv(hv_reads_filtered_path_old, show_col_types = FALSE) %&gt;%\n  inner_join(libraries_old, by=\"sample\") %&gt;% \n  arrange(enrichment, location, date) %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n# Combined\nmrg &lt;- bind_rows(hv_reads_filtered, hv_reads_filtered_old) %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev)) %&gt;%\n  filter(assigned_hv | hit_hv | adj_score_max &gt;= 20)\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(kraken_label~enrichment, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nFollowing the same selection criteria used for the unenriched samples, we identify 177,246 HV reads across the panel-enriched samples, compared to 12,305 in the unenriched samples. This corresponds to an overall relative HV abundance estimate of \\(3.3 \\times 10^{-5}\\) - about double the estimate for unenriched samples of \\(1.87 \\times 10^{-5}\\). This is much lower than the HV relative abundance observed in Crits-Christoph’s enriched samples, which exceeded \\(10^{-2}\\).\nPanel-enriched relative abundance for individual treatment plants varied from \\(9.3 \\times 10^{-6}\\) to \\(1.6 \\times 10^{-4}\\), with enrichment factors compared to unenriched samples from the same plant ranging from \\(1\\times\\) (HTP) to \\(12\\times\\) (ESC). Overall, this seems like a relatively disappointing showing for panel-enrichment compared to some other datasets we’ve seen.\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, enrichment, date, n_reads_raw = n_read_pairs)\nread_counts_raw_old &lt;- basic_stats_old %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, location, enrichment, date, n_reads_raw = n_read_pairs)\nread_counts_raw_mrg &lt;- bind_rows(read_counts_raw, read_counts_raw_old)\n\n# Get HV read counts & RA\nread_counts_hv_mrg &lt;- mrg %&gt;% group_by(sample, location, enrichment) %&gt;%\n  count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw_mrg %&gt;%\n  left_join(read_counts_hv_mrg, by=c(\"sample\", \"location\", \"enrichment\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate by location\nread_counts_loc &lt;- read_counts %&gt;% group_by(location, enrichment) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nread_counts_total &lt;- read_counts_loc %&gt;% group_by(enrichment) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         location = \"All locations\")\nread_counts_agg &lt;- read_counts_loc %&gt;% mutate(location = as.character(location)) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(location = fct_inorder(location))\n\n# Calculate enrichment factors\nread_counts_enrichment &lt;- read_counts_agg %&gt;% select(location, enrichment, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = \"location\", names_from = enrichment, values_from = p_reads_hv) %&gt;%\n  mutate(enrichment = Enriched/Unenriched)\n\n# Visualize\npalette_loc &lt;- c(brewer.pal(9, \"Set1\"), \"black\")\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=location, y=p_reads_hv, color=location, \n                                         shape=enrichment)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_manual(values=palette_loc, name=\"Location\") +\n  scale_shape_discrete(name = \"Panel enrichment\") +\n  guides(shape = guide_legend(nrow=2, vjust=0.5), color=\"none\") +\n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv_agg\n\n\n\n\nDigging into specific viruses, the results are…odd. While the list of most highly enriched viruses encludes some included in the Illumina panel (e.g. SARS-CoVs, betapolyomaviruses), many are fecal-oral viruses with no obvious relationship to the panel (e.g. various astroviruses). The most highly enriched viral genus is Lentivirus, which includes HIV and is not included in the Illumina panel. I don’t know enough about the panel or these respective viruses to give a strong take on what’s going on here, but it certainly seems that the Illumina RVP is less effective at enriching for specific respiratory viruses in Rothman than in Crits-Christoph.\n\nCodeviral_taxa_path &lt;- file.path(data_dir_old, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get viral taxon names for putative HV reads\nmrg_named &lt;- mrg %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\n\nhv_reads_species &lt;- raise_rank(mrg_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_named, viral_taxa, \"genus\")\n\n# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(location, enrichment, name) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, enrichment, n_reads_raw), \n             by=c(\"location\", \"enrichment\"))\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name, enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(location, enrichment, name) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(location, enrichment, n_reads_raw), \n             by=c(\"location\", \"enrichment\"))\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name, enrichment) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCodemax_rank_species &lt;- 10\n\n# Compute species enrichment\nhv_species_enr &lt;- hv_species_counts_agg %&gt;% filter(location == \"All locations\") %&gt;%\n  select(location, enrichment, name, n_reads_raw, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = c(\"location\", \"name\"), names_from = \"enrichment\", \n              values_from = c(\"p_reads_hv\", \"n_reads_raw\")) %&gt;%\n  mutate(p_reads_hv_Enriched = replace_na(p_reads_hv_Enriched, 0),\n         p_reads_hv_Unenriched = replace_na(p_reads_hv_Unenriched, 0),\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  group_by(location) %&gt;%\n  mutate(n_reads_raw_Enriched = max(n_reads_raw_Enriched, na.rm = TRUE),\n         n_reads_raw_Unenriched = max(n_reads_raw_Unenriched, na.rm = TRUE))\n\n# Identify most enriched/de-enriched species\nhv_species_enr_ranked &lt;- hv_species_enr %&gt;% group_by(location) %&gt;%\n  filter(log_enrichment &lt; Inf, log_enrichment &gt; -Inf) %&gt;%\n  mutate(rank_enrichment_neg = row_number(log_enrichment),\n         rank_enrichment_pos = row_number(desc(log_enrichment)),\n         major = rank_enrichment_neg &lt;= max_rank_species | rank_enrichment_pos &lt;= max_rank_species)\n\n# Aggregate and visualize\nhv_species_enr_plot &lt;- hv_species_enr_ranked %&gt;%\n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(location, name_display, n_reads_raw_Unenriched, n_reads_raw_Enriched) %&gt;%\n  summarize(n_reads_hv_Enriched = sum(p_reads_hv_Enriched * n_reads_raw_Enriched),\n            n_reads_hv_Unenriched = sum(p_reads_hv_Unenriched * n_reads_raw_Unenriched),\n            .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv_Enriched = n_reads_hv_Enriched / n_reads_raw_Enriched,\n         p_reads_hv_Unenriched = n_reads_hv_Unenriched / n_reads_raw_Unenriched,\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  arrange(log_enrichment) %&gt;% mutate(name_display = fct_inorder(name_display))\ng_species_enr &lt;- ggplot(hv_species_enr_plot,\n                       aes(x=name_display, y=log_enrichment)) +\n  geom_hline(yintercept = 0, color=\"red\", linetype = \"dashed\") +\n  geom_point(shape=16) +\n  scale_y_continuous(name = \"Log10 enrichment in panel-enriched samples\",\n                     limits = c(-1.5,1.5), expand=c(0,0)) +\n  facet_wrap(location~., scales=\"free\", ncol=1) +\n  theme_kit + theme(plot.margin = margin(l=1, unit=\"cm\"))\ng_species_enr\n\n\n\nCode# Single-sample viruses\nhv_species_enr_solo &lt;- hv_species_enr %&gt;%\n  filter(log_enrichment == Inf) %&gt;%\n  arrange(desc(p_reads_hv_Enriched))\n\n\n\nCodemax_rank_genera &lt;- 10\n\n# Compute genus enrichment\nhv_genera_enr &lt;- hv_genera_counts_agg %&gt;% filter(location == \"All locations\") %&gt;%\n  select(location, enrichment, name, n_reads_raw, p_reads_hv) %&gt;%\n  pivot_wider(id_cols = c(\"location\", \"name\"), names_from = \"enrichment\", \n              values_from = c(\"p_reads_hv\", \"n_reads_raw\")) %&gt;%\n  mutate(p_reads_hv_Enriched = replace_na(p_reads_hv_Enriched, 0),\n         p_reads_hv_Unenriched = replace_na(p_reads_hv_Unenriched, 0),\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  group_by(location) %&gt;%\n  mutate(n_reads_raw_Enriched = max(n_reads_raw_Enriched, na.rm = TRUE),\n         n_reads_raw_Unenriched = max(n_reads_raw_Unenriched, na.rm = TRUE))\n\n# Identify most enriched/de-enriched genera\nhv_genera_enr_ranked &lt;- hv_genera_enr %&gt;% group_by(location) %&gt;%\n  filter(log_enrichment &lt; Inf, log_enrichment &gt; -Inf) %&gt;%\n  mutate(rank_enrichment_neg = row_number(log_enrichment),\n         rank_enrichment_pos = row_number(desc(log_enrichment)),\n         major = rank_enrichment_neg &lt;= max_rank_genera | rank_enrichment_pos &lt;= max_rank_genera)\n\n# Aggregate and visualize\nhv_genera_enr_plot &lt;- hv_genera_enr_ranked %&gt;%\n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(location, name_display, n_reads_raw_Unenriched, n_reads_raw_Enriched) %&gt;%\n  summarize(n_reads_hv_Enriched = sum(p_reads_hv_Enriched * n_reads_raw_Enriched),\n            n_reads_hv_Unenriched = sum(p_reads_hv_Unenriched * n_reads_raw_Unenriched),\n            .groups = \"drop\") %&gt;%\n  mutate(p_reads_hv_Enriched = n_reads_hv_Enriched / n_reads_raw_Enriched,\n         p_reads_hv_Unenriched = n_reads_hv_Unenriched / n_reads_raw_Unenriched,\n         log_enrichment = log10(p_reads_hv_Enriched/p_reads_hv_Unenriched)) %&gt;%\n  arrange(log_enrichment) %&gt;% mutate(name_display = fct_inorder(name_display))\ng_genera_enr &lt;- ggplot(hv_genera_enr_plot,\n                       aes(x=name_display, y=log_enrichment)) +\n  geom_hline(yintercept = 0, color=\"red\", linetype = \"dashed\") +\n  geom_point(shape=16) +\n  scale_y_continuous(name = \"Log10 enrichment in panel-enriched samples\",\n                     limits = c(-3,3), expand=c(0,0)) +\n  facet_wrap(location~., scales=\"free\", ncol=1) +\n  theme_kit + theme(plot.margin = margin(l=1, unit=\"cm\"))\ng_genera_enr\n\n\n\nCode# Single-sample viruses\nhv_genera_enr_solo &lt;- hv_genera_enr %&gt;%\n  filter(log_enrichment == Inf) %&gt;%\n  arrange(desc(p_reads_hv_Enriched))"
  },
  {
    "objectID": "notebooks/2024-03-01_dedup.html",
    "href": "notebooks/2024-03-01_dedup.html",
    "title": "Improving read deduplication in the MGS workflow",
    "section": "",
    "text": "After investigating several options in previous entries, I settled on Clumpify for deduplication of reads for my MGS pipeline. Unfortunately, Clumpify as I’m currently running it has a key flaw, which it shares with most other deduplication tools that run on paired reads: it’s unable to detect “reverse-complement” duplicates in which the forward read of one pair matches the reverse read of another & vice-versa. Since the orientation of reads is random in many cases, this will leave a random subset of duplicates undetected.\nThis is tolerable in many instances, but is a problem for cases where we care a lot about getting accurate read counts, such as when estimating the relative abundance of human-infecting viruses. As such, it would be great to find a solution that lets us remove these extra duplicates prior to estimating viral relative abundance.\nClumpify does have a configuration where it unpairs reads prior to deduplication, allowing them to be deduplicated as individual reads. This solves the problem above, but (a) can lead to over-removal in cases where only one read in a pair is a duplicate, and (b) typically breaks on large datasets, I think due to memory issues. Despite looking at quite a number of possible options, I was unable to find a deduplication tool that met my desiderata of (i) running on paired reads, (ii) identifying duplicates in a sensible, error-tolerant way, and (iii) handling reverse-complement duplicates.\nAs a result, I turned to an alternative approach, which was to restrict deduplication of the whole read set to RC-insensitive Clumpify, then apply additional, more stringent deduplication to putative human-viral reads. During the process of HV read identification, downstream of Bowtie2 but upstream of Kraken2, putative viral read pairs are merged & concatenated to produce a single sequence per read pair. This makes deduplication much easier as I can run deduplication tools on the result without needing them to specifically handle paired reads. In particular, Clumpify and its sister program Dedupe should both work well on these sequences, removing duplicates in either orientation while correctly handling errors and “containments” (partial duplicates in which one sequence is completely contained within another).\nTo investigate this, I downloaded the putative human-viral reads for each sample in Crits-Christoph 2021, after identification with Bowtie2 and screening against potential contaminants but before taxonomic assignment with Kraken. I ran three deduplication tools on these sequences:\n\nClumpify in single-end mode\n\nclumpify.sh in=reads/${s}_bowtie2_mjc.fastq.gz out=clumpify/${s}.fastq.gz dedupe containment\n\nDedupe in single-end mode\n\ndedupe.sh in=reads/${s}_bowtie2_mjc.fastq.gz out=dedupe/${s}.fastq.gz\n\nrmdup from the seqkit package\n\nseqkit rmdup -so seqkit/${s}.fastq.gz reads/${s}_bowtie2_mjc.fastq.gz\nI then quantified the number of surviving sequences in each case with FASTQC and MultiQC.\n\nCode# Paths to data\ndata_dir &lt;- \"../data/2024-02-29_dedup/\"\nraw_path &lt;- file.path(data_dir, \"multiqc/multiqc_raw_fastqc.txt\")\nclumpify_path &lt;- file.path(data_dir, \"multiqc/multiqc_clumpify_fastqc.txt\")\ndedupe_path &lt;- file.path(data_dir, \"multiqc/multiqc_dedupe_fastqc.txt\")\nseqkit_path &lt;- file.path(data_dir, \"multiqc/multiqc_seqkit_fastqc.txt\")\n\n# Import data\nraw &lt;- read_tsv(raw_path, show_col_types = FALSE) %&gt;%\n  mutate(Sample = sub(\"_bowtie2_mjc\", \"\", Sample),\n         Method = \"none\")\nclumpify &lt;- read_tsv(clumpify_path, show_col_types = FALSE) %&gt;% mutate(Method = \"clumpify\")\ndedupe &lt;- read_tsv(dedupe_path, show_col_types = FALSE) %&gt;% mutate(Method = \"dedupe\")\nseqkit &lt;- read_tsv(seqkit_path, show_col_types = FALSE) %&gt;% mutate(Method = \"seqkit\")\nprocessed &lt;- bind_rows(raw, clumpify, dedupe, seqkit) %&gt;%\n  mutate(Method = fct_inorder(Method),\n         seqs_abs = `Total Sequences`) %&gt;%\n  group_by(Sample) %&gt;%\n  mutate(seqs_rel = seqs_abs/max(seqs_abs))\n\n# Visualize\ng_dedup_abs &lt;- ggplot(processed, aes(x=Sample, y=seqs_abs, fill=Method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_y_continuous(name=\"# Surviving Reads\", expand = c(0,0), limits = c(0,140000), breaks = seq(0,200000,40000)) +\n  theme_kit\ng_dedup_abs\n\n\n\nCodeg_dedup_rel &lt;- ggplot(processed, aes(x=Sample, y=seqs_rel, fill=Method)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_y_continuous(name=\"% Surviving Reads\", breaks = seq(0,1,0.2), labels = function(x) x*100, expand = c(0,0)) +\n  theme_kit\ng_dedup_rel\n\n\n\n\nClumpify consistently removed the most reads, with Dedupe close behind and seqkit’s rmdup performing much less well. As such, I decided to implement Clumpify to remove duplicates at this point in the pipeline, then look at the effect on predicted relative abundance of viruses in one previously-analyzed dataset.\nBrief re-analysis of Crits-Christoph 2021\nOn average, adding in RC-sensitive deduplication reduced overall HV relative abundance measurements by about 4% in unenriched samples and about 10% in panel-enriched samples in Crits-Christoph 2021:\n\nCodecc_dir_new &lt;- file.path(data_dir, \"cc-rerun\")\ncc_dir_old &lt;- file.path(data_dir, \"cc-prev\")\nbasic_stats_new_path &lt;- file.path(cc_dir_new, \"qc_basic_stats.tsv\")\nbasic_stats_old_path &lt;- file.path(cc_dir_old, \"qc_basic_stats.tsv\")\nhv_reads_new_path &lt;- file.path(cc_dir_new, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_old_path &lt;- file.path(cc_dir_old, \"hv_hits_putative_filtered.tsv\")\nlibraries_path &lt;- file.path(cc_dir_new, \"cc-libraries.txt\")\n\n# Get raw read counts\nbasic_stats_new &lt;- read_tsv(basic_stats_new_path, show_col_types = FALSE)\nbasic_stats_old &lt;- read_tsv(basic_stats_old_path, show_col_types = FALSE)\nread_counts_raw_new &lt;- basic_stats_new %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, n_reads_raw = n_read_pairs) %&gt;%\n  mutate(dedup_rc = TRUE)\nread_counts_raw_old &lt;- basic_stats_old %&gt;% filter(stage == \"raw_concat\") %&gt;% \n  select(sample, n_reads_raw = n_read_pairs) %&gt;%\n  mutate(dedup_rc = FALSE)\nread_counts_raw &lt;- bind_rows(read_counts_raw_new, read_counts_raw_old)\n\n# Get HV read counts\nlibraries &lt;- read_tsv(libraries_path, show_col_types = FALSE) %&gt;%\n  mutate(enrichment = str_to_title(enrichment))\nhv_reads_new &lt;- read_tsv(hv_reads_new_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         dedup_rc = TRUE)\nhv_reads_old &lt;- read_tsv(hv_reads_old_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(enrichment, location, collection_date) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         dedup_rc = FALSE)\nhv_reads &lt;- bind_rows(hv_reads_new, hv_reads_old)\nhv_reads_cut &lt;- hv_reads %&gt;%\n    mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nhv_reads_counts &lt;- hv_reads_cut %&gt;% group_by(sample, enrichment, dedup_rc) %&gt;% count(name=\"n_reads_hv\")\n\n# Merge\nhv_reads_ra &lt;- inner_join(hv_reads_counts, read_counts_raw, by=c(\"sample\", \"dedup_rc\")) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_total &lt;- hv_reads_ra %&gt;% group_by(enrichment, dedup_rc) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), n_reads_raw = sum(n_reads_raw), .groups=\"drop\") %&gt;%\n  mutate(sample = paste(\"All\", str_to_lower(enrichment)), p_reads_hv = n_reads_hv/n_reads_raw)\nhv_reads_bound &lt;- bind_rows(hv_reads_ra, hv_reads_total) %&gt;% arrange(enrichment, dedup_rc)\nhv_reads_bound$sample &lt;- fct_inorder(hv_reads_bound$sample)\n\n# Calculate effect of dedup on RA\nra_rel &lt;- hv_reads_bound %&gt;% ungroup %&gt;% select(sample, enrichment, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"dedup_rc\", values_from = \"p_reads_hv\", names_prefix = \"dedup_\") %&gt;%\n  mutate(rel_ra = dedup_TRUE/dedup_FALSE)\n\n\n\nCodeg_phv &lt;- ggplot(hv_reads_bound, aes(x=sample, y=p_reads_hv, \n                                    color=enrichment, shape=dedup_rc)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  guides(color=\"none\") +\n  facet_wrap(~enrichment, scales = \"free\") + \n  theme_base + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_phv\n\n\n\n\nTurning to specific viruses, just plotting out the relative abundances with and without deduplication isn’t terribly informative due to the wide log scales involved:\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxa.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get viral taxon names for putative HV reads\nhv_named &lt;- hv_reads_cut %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\n\nhv_reads_genera &lt;- raise_rank(hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% \n  group_by(sample, enrichment, name, dedup_rc) %&gt;%\n  count(name=\"n_reads_hv\") %&gt;%\n  inner_join(read_counts_raw, by=c(\"sample\", \"dedup_rc\")) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name, enrichment, dedup_rc) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw), .groups = \"drop\") %&gt;%\n  mutate(sample = \"All samples\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCodehv_genera_counts_plot &lt;- hv_genera_counts_agg %&gt;%\n  filter(sample == \"All samples\", !is.na(name))\ng_genera &lt;- ggplot(hv_genera_counts_plot,\n                   aes(x=name, y=p_reads_hv, shape=dedup_rc, color=enrichment)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_genera\n\n\n\n\n\nCodehv_genera_counts_wide &lt;- hv_genera_counts_agg %&gt;%\n  select(sample, enrichment, name, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"enrichment\", values_from=\"p_reads_hv\") %&gt;%\n  mutate(Enriched = replace_na(Enriched, 0),\n         Unenriched = replace_na(Unenriched, 0),\n         rel_enrichment = Enriched/Unenriched,\n         log_enrichment = log10(rel_enrichment))\nhv_genera_counts_rel_plot &lt;- hv_genera_counts_wide %&gt;%\n  filter(sample == \"All samples\", log_enrichment &lt; \"Inf\", log_enrichment &gt; \"-Inf\")\ng_rel_ra &lt;- ggplot(hv_genera_counts_rel_plot,\n                   aes(x=name, y=log_enrichment, shape=dedup_rc)) +\n  geom_point() +\n  scale_y_continuous(name=\"Log10 enrichment in panel-enriched samples\",\n                     limits = c(0,5), breaks = seq(0,10,1), expand=c(0,0)) +\n  scale_shape_discrete(name=\"RC-sensitive deduplication\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_rel_ra\n\n\n\n\nHowever, if we specifically look at the change in relative abundance with vs without deduplication, we do see some variation, with many viruses showing no change and others showing reductions in measured RA of up to 20%:\n\nCodehv_genera_counts_rel_dedup &lt;- hv_genera_counts_agg %&gt;%\n  filter(sample == \"All samples\", !is.na(name)) %&gt;%\n  select(sample, enrichment, name, dedup_rc, p_reads_hv) %&gt;%\n  pivot_wider(names_from=\"dedup_rc\", names_prefix=\"dedup_\", \n              values_from=\"p_reads_hv\", values_fill=0) %&gt;%\n  mutate(dedup_rel = dedup_TRUE/dedup_FALSE,\n         dedup_rel_log = log10(dedup_rel))\ng_genera_rel_dedup &lt;- ggplot(hv_genera_counts_rel_dedup,\n                             aes(x=name, y=dedup_rel, color=enrichment)) +\n  geom_point(alpha=0.5) +\n  scale_y_continuous(\"Viral relative abundance in deduplicated vs\\nnon-deduplicated samples\") +\n  scale_color_brewer(palette=\"Dark2\", name=\"Panel enrichment\") +\n  theme_kit + theme(plot.margin = margin(l=1, t=0.5, unit=\"cm\"))\ng_genera_rel_dedup\n\n\n\n\nGoing forward, I’ll include this additional deduplication step in analysis of future data."
  },
  {
    "objectID": "notebooks/2024-03-16_yang.html",
    "href": "notebooks/2024-03-16_yang.html",
    "title": "Workflow analysis of Yang et al. (2020)",
    "section": "",
    "text": "As we work on expanding and updating the P2RA preprint for publication, I’m working on running the relevant datasets through my pipeline to compare the results to those from the original Kraken-based approach. I began by processing Yang et al. (2020), a study that carried out RNA viral metagenomics on raw sewage samples from Xinjiang, China.\nThe study collected 7 1L grab samples of raw sewage from the inlets of three WWTPs in Xinjiang, and processed them using an “anion filter membrane absorption” method quite distinct from the other studies I’ve looked at so far:\n\nInitially, samples were centrifuged to pellet cells and debris.\nThe supernatant was treated with magnesium chloride and HCl, then filtered through a mixed cellulose ester membrane filter, discarding the filtrate.\nMaterial was then eluted from the filters by ultrasonication with 3% beef extract, before going a second filtering step using a 0.22um filter, this time retaining the filtrate.\nNext, the samples underwent RNA extraction using the QIAamp viral RNA Mini Kit.\nFinally, sequencing libraries were generated using KAPA Hyper Prep Kit. No mention is made of rRNA depletion or panel enrichment.\n\nThis study is of particular interest because previous analysis found that the MGS data it generated contains a particularly high fraction of both total viruses and human-infecting viruses. It would be good to know whether this is attributable to the fact that the authors used an unusual method, or if it’s the result of genuine differences on the ground (e.g. an elevated prevalence of enteric disease compared to the USA).\nThe raw data\nThe Yang data is unusual in that it contains only a small number of samples (7 samples from 3 different locations) but each of these samples is sequenced quite deeply: the number of reads per sample varied from 162M to 283M, with an average of 203M. Taken together, the samples totaled roughly 1.4B read pairs (425 gigabases of sequence).\nRead qualities were consistently very high, but adapter levels were also elevated. The level of sequence duplication levels was very high according to FASTQC, with a minimum of 89% across all samples, suggesting a high degree of oversequencing; perhaps unsurprising given the sheer depth of these samples.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-03-15_yang/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Visualize basic stats\ng_nreads_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=n_read_pairs, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"# Read pairs\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit\nlegend_location &lt;- get_legend(g_nreads_raw)\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\ng_nbases_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=n_bases_approx, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_ndup_raw &lt;- ggplot(basic_stats_raw, aes(x=sample, y=percent_duplicates, fill=location)) + geom_col(position=\"dodge\") + scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) + scale_fill_brewer(palette=\"Set1\", name=\"Location\") + theme_kit + theme(legend.position = \"none\")\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_location, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\nCode# Visualize adapters\ng_adapters_raw &lt;- ggplot(adapter_stats_raw, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_wrap(~adapter) + theme_base\ng_adapters_raw\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- ggplot(quality_base_stats_raw, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_seq_raw &lt;- ggplot(quality_seq_stats_raw, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\ng_quality_base_raw\n\n\n\nCodeg_quality_seq_raw\n\n\n\n\nPreprocessing\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, location, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, location) %&gt;% arrange(sample, location, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\n\n\nOn average, cleaning & deduplication together removed about 86% of total reads from each sample, with the largest losses seen during deduplication. Only a few reads (low single-digit percentages at most) were lost during subsequent ribodepletion – a surprising finding given the apparent lack of rRNA depletion in the study methods. This makes me suspect that the methods did include rRNA depletion without mentioning it; if this isn’t the case, it suggests that the methods used by the authors might be particularly effective at removing bacteria from the sample.\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost_abs_marginal,fill=location,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette=\"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with very few adapter sequences found by FASTQC at any stage after the raw data. Improvements in quality were unsurprisingly minimal, given the high initial quality observed.\n\nCode# Visualize adapters\ng_adapters &lt;- ggplot(adapter_stats, aes(x=position, y=pc_adapters, color=location, linetype = read_pair, group=interaction(sample, read_pair))) + geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~adapter) + theme_base\ng_adapters\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- ggplot(quality_base_stats, aes(x=position, y=mean_phred_score, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,140),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~.) + theme_base\ng_quality_seq &lt;- ggplot(quality_seq_stats, aes(x=mean_phred_score, y=n_sequences, color=location, linetype = read_pair, group=interaction(sample,read_pair))) +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  scale_linetype_discrete(name = \"Read Pair\") +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  facet_grid(stage~., scales=\"free_y\") + theme_base\ng_quality_base\n\n\n\nCodeg_quality_seq\n\n\n\n\nAccording to FASTQC, deduplication was only moderately effective at reducing measured duplicate levels, despite the high read losses observed. After deduplication, FASTQC-measured duplicate levels fell from an average of 92% to one of 58%:\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=location, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set1\", name=\"Location\") +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\nCodeg_readlen_stages\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:location), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", breaks = seq(0,0.1,0.01),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~location, scales = \"free_x\") +\n  theme_kit\ng_comp_minor\n\n\n\n\n\nCodep_reads_summ &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification))) %&gt;%\n  group_by(classification, sample, location) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, pc_mean = mean(p_reads)*100)\n\n\nOverall, in these unenriched samples, between 89% and 95% of reads (mean 93%) are low-quality, duplicates, or unassigned. Of the remainder, 0.4-3.9% (mean 1.6%) were identified as ribosomal, and 0.06-0.22% (mean 0.13%) were otherwise classified as bacterial. The fraction of human reads ranged from 0.006-0.198% (mean 0.049%). Strikingly, those of viral reads ranged from 0.6% all the way up to 10.2% (mean 5.5%).\nThe total fraction of viral reads is strikingly high, even higher on average than for Rothman unenriched samples. As in Rothman, this is primarily accounted for by very high levels of tobamoviruses (viral family Virgaviridae), which make up 74-98% (mean 84%) of viral reads. Most of the remainder is accounted for by Tombusviridae and Solemoviridae, two other families of plant viruses. Only one family of vertebrate viruses, Astroviridae, accounted for more than 1% of viral reads in any sample.\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\nsamples &lt;- as.character(basic_stats_raw$sample)\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\", \"rank\", \"taxid\", \"name\")\nreport_paths &lt;- paste0(data_dir, \"kraken/\", samples, \".report.gz\")\nkraken_reports &lt;- lapply(1:length(samples), \n                         function(n) read_tsv(report_paths[n], col_names = col_names,\n                                              show_col_types = FALSE) %&gt;%\n                           mutate(sample = samples[n])) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nviral_families &lt;- kraken_reports_viral %&gt;% filter(rank == \"F\")\nviral_families_major_list &lt;- viral_families %&gt;% group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= 0.01) %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral)) %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- bind_rows(viral_families_major, viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\")))\ng_families &lt;- ggplot(viral_families_display, aes(x=sample, y=p_reads_viral, fill=name)) +\n  geom_col(position=\"stack\") +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Viral family\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  theme_kit\ng_families\n\n\n\n\nAlmost as striking as the high fraction of viral reads in these data is the extremely low prevalence of bacterial reads. For comparison, non-ribosomal bacterial reads in unenriched Crits-Christoph samples ranged from 16-20%, and in unenriched Rothman samples it ranged from 2-12%. Yang et al. are thus somehow achieving a greater than 10-fold reduction in the fraction of bacterial reads compared to other studies. Unlike the increased prevalence of viruses, this seems harder to explain away via differences in conditions on the ground. Together with the elevated viral fraction, these results suggest to me that Yang’s ideosyncratic methods are worth investigating further.\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a pipeline identical to that described in my entry on unenriched Rothman samples. This process identified a total of 532,583 read pairs across all samples (0.3% of surviving reads):\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(location) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, location) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\nThis is a far larger number of putative viral reads than I previously needed to analyze, and is far too many to realistically process with BLASTN. To address this problem, I selected 1% of putative viral sequences (5,326) and ran them through BLASTN in a similar manner to past datasets:\n\nCodeset.seed(83745)\nmrg_sample &lt;- sample_frac(mrg, 0.01)\nmrg_num &lt;- mrg_sample %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n    mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_fasta &lt;-  mrg_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"yang-putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"yang-putative-viral-all.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(sample, seq_num, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg_num, blast_results_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_mrg_blast_0 &lt;- mrg_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_0\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\nHigh-scoring true-positives massively outnumber false-positives, while low-scoring true-positives are fairly rare. My usual score cutoff, a disjunctive threshold at S=20, achieves a sensitivity of ~97% and an F1 score of ~98%. As such, I think this looks pretty good, and feel comfortable moving on to analyzing the entire HV dataset.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_0 &lt;- range_f1(mrg_blast, inc_special) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt) %&gt;% filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\nHuman-infecting virus reads: analysis\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, location, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate\nread_counts_total &lt;- read_counts %&gt;% ungroup %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         sample= \"All samples\", location = \"All locations\")\nread_counts_agg &lt;- read_counts %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(sample = fct_inorder(sample),\n         location = fct_inorder(location))\n\n\nApplying a disjunctive cutoff at S=20 identifies 513,259 reads as human-viral out of 1.42B total reads, for a relative HV abundance of \\(3.62 \\times 10^{-4}\\). This compares to \\(2.0 \\times 10^{-4}\\) on the public dashboard, corresponding to the results for Kraken-only identification: an 80% increase, smaller than the 4-5x increases seen for Crits-Christoph and Rothman. Relative HV abundances for individual samples ranged from \\(6.36 \\times 10^{-5}\\) to \\(1.19 \\times 10^{-3}\\):\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=sample, y=p_reads_hv, color=location)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette = \"Set1\", name = \"Location\") +\n  theme_kit\ng_phv_agg\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially Mamastrovirus, Rotavirus, Salivirus, and fecal-oral Enterovirus species (especially Enterovirus C, which includes poliovirus):\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n\n\nCode# Count relative abundance for species\nhv_species_counts_raw &lt;- hv_reads_species %&gt;% group_by(sample, location, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(sample, n_reads_raw), by=\"sample\")\nhv_species_counts_all &lt;- hv_species_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = \"All samples\", location = \"All locations\")\nhv_species_counts_agg &lt;- bind_rows(hv_species_counts_raw, hv_species_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(sample, location, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(sample, n_reads_raw), by=\"sample\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(sample = \"All samples\", location = \"All locations\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n\n\nCode# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_species &lt;- 5\nmax_rank_genera &lt;- 5\nhv_species_counts_ranked &lt;- hv_species_counts_agg %&gt;% group_by(sample) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_species) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(sample) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"))\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=sample, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=5)) +\n  theme_kit\ng_vcomp_species &lt;- ggplot(hv_species_counts_ranked, aes(x=sample, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral species\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(nrow=7)) +\n  theme_kit\n\ng_vcomp_genera\n\n\n\nCodeg_vcomp_species\n\n\n\n\nThese results are consistent with what’s been observed before, e.g. in the P2RA preprint and public dashboard.\nConclusion\nI analyzed Yang et al. for two reasons: first, because it’s included in the P2RA dataset that we’re currently reanalyzing, and second, because it has the highest relative abundance of both total and human-infecting viruses of any wastewater study we’ve looked at so far. Having done this analysis, the key question to ask is: is this elevated viral abundance a consequence of the unusual sample-processing methods employed by the authors, or the region in which samples were collected? Both are plausible: the processing methods used in this paper are distinct from any other paper we’ve looked at, which makes it possible that they are significantly superior; on the other hand, it’s also plausible that the methods are comparable in efficacy to more standard approaches, and the difference arises from a genuinely elevated level of viruses in Xinjiang compared to e.g. California (for Rothman and Crits-Christoph).\nThe analyses conducted here aren’t able to give a dispositive answer to that question, but they are suggestive. The fact that human-infecting viruses are strongly dominated by fecal-oral species is consistent with the elevated-infection theory, as these are the kinds of viruses I’d most expect to be elevated in a poorer region of the world compared to a rich one. On the other hand, the fact that total viruses (including plant viruses) are also elevated points in the other direction; I don’t see any obvious reason why we’d expect more tobamoviruses in Xinjiang than California.\nMost compellingly, from my perspective, is the very low fraction of bacterial and ribosomal reads found in the Yang data compared to other datasets I’ve analyzed. The methods make no mention of rRNA depletion prior to sequencing; even if this was conducted without being mentioned, that still doesn’t explain the very low level of non-ribosomal bacterial reads. In general, I’d expect a region with elevated enteric viral pathogens to also show elevated enteric bacterial pathogens, so this absence is difficult to explain with a catchment-based theory. Instead, it points towards the viral enrichment protocols used by this study as potentially achieving genuinely better results than other methods we’ve tried.\nThis certainly isn’t conclusive evidence, or close to it. However, based on these results, I’d be very interested in learning more about the details of Yang et al’s approach to viral enrichment, and to see it tried by another group in a different context to see if these impressive results can be replicated."
  },
  {
    "objectID": "notebooks/2024-03-19_yang-2.html",
    "href": "notebooks/2024-03-19_yang-2.html",
    "title": "Followup analysis of Yang et al. (2020)",
    "section": "",
    "text": "Some people had some helpful questions about my previous analysis of Yang et al. (2020), and the finding that the Yang data contains an unusually high relative abundance of viral sequences, together with an unusually low relative abundance of bacterial sequences. Here are the Bracken composition results I presented last time:\n\nCodedata_dir &lt;- \"../data/2024-03-18_yang-2\"\ndata_dir_old &lt;- file.path(data_dir, \"yang-1\")\nlibraries_path &lt;- file.path(data_dir_old, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir_old, \"qc_basic_stats.tsv\")\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\n\n# Import Bracken data\nbracken_path &lt;- file.path(data_dir_old, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, location, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"location\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:location), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~location, scales=\"free\") +\n  theme_kit\ng_comp\n\n\n\n\nMultiple people noted that almost all of the reads in all samples were either removed during filtering or flagged as duplicates, and wondered whether the results would stand if we looked at the pre-deduplicated composition instead. Put another way, they wondered whether deduplication might be disproportionately removing ribosomal or other bacterial sequences, and thus giving a misleadingly optimistic picture of the results.\nTo address this, I re-ran the taxonomic composition analysis on a 1% subset of the pre-deduplication Yang data, then compared the effect of deduplication on the inferred taxonomic makeup (excluding filtered and deduplicated reads). In running this analysis, I made the assumption that all of the post-deduplication reads identified as ribosomal were from bacterial ribosomes; this seems to be generally true in our experience of wastewater data and allows us to make a more like-for-like comparison (as we don’t have ribosomal status for the pre-deduplication reads).\nThe results were as follows:\n\nCodeclass_levels &lt;- c(\"Unassigned\", \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# 1. Remove filtered & duplicate reads from original Bracken output & renormalize\nread_comp_renorm &lt;- read_comp_long %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads),\n         classification = classification %&gt;% as.character %&gt;%\n           ifelse(. == \"Ribosomal\", \"Bacterial\", .)) %&gt;%\n  group_by(sample, location, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  mutate(classification = factor(classification, levels = class_levels))\n  \n# 2. Import pre-deduplicated \nbracken_path_predup &lt;- file.path(data_dir, \"bracken_counts_subset.tsv\")\nbracken_predup &lt;- read_tsv(bracken_path_predup, show_col_types = FALSE)\ntotal_assigned_predup &lt;- bracken_predup %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread_predup &lt;- bracken_predup %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_predup &lt;- read_counts_preproc %&gt;%\n  select(sample, location, raw_concat, cleaned) %&gt;%\n  mutate(raw_concat = raw_concat * 0.01, cleaned = cleaned * 0.01) %&gt;%\n  inner_join(total_assigned_predup %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread_predup, by=\"sample\")\n# Assess composition\nread_comp_predup &lt;- transmute(read_counts_predup, sample=sample, location=location,\n                       n_filtered = raw_concat-cleaned,\n                       n_unassigned = cleaned-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_predup_long &lt;- pivot_longer(read_comp_predup, -(sample:location), \n                                      names_to = \"classification\",\n                                      names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads)) %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads))\n\n# 3. Combine\nread_comp_comb &lt;- bind_rows(read_comp_predup_long %&gt;% mutate(deduplicated = FALSE),\n                            read_comp_renorm %&gt;% mutate(deduplicated = TRUE)) %&gt;%\n  mutate(label = ifelse(deduplicated, \"Post-dedup\", \"Pre-dedup\") %&gt;% fct_inorder)\n\n# Plot overall composition\ng_comp_predup &lt;- ggplot(read_comp_comb, aes(x=label, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~sample, scales=\"free\", ncol=4) +\n  theme_kit\ng_comp_predup\n\n\n\n\nIn most cases, deduplication made little-to-no difference to the inferred taxonomic composition, with bacterial and viral read fractions remaining roughly the same in both cases. In a few cases, and especially in sample SRR12204735, deduplication substantially reduced the inferred relative abundance of bacterial reads, probably due to collapsing genuine biological duplicates of common (e.g. ribosomal) sequences. In 5/7 samples, the viral fraction substantially exceeded the bacterial fraction both pre- and post-deduplication.\nOverall, then, it appears that the finding of unusually high viral and low bacterial abundance in the Yang data is not an artefact of deduplication. This is a relief, and strengthens my confidence that their methods are worth investigating further."
  },
  {
    "objectID": "notebooks/2024-04-01_spurbeck.html",
    "href": "notebooks/2024-04-01_spurbeck.html",
    "title": "Workflow analysis of Spurbeck et al. (2023)",
    "section": "",
    "text": "Continuing my analysis of datasets from the P2RA preprint, I analyzed the data from Spurbeck et al. (2023), a wastewater RNA-sequencing study from Ohio. Samples for this study underwent a variety of processing protocols at different research centers across the state. Along with Rothman and Crits-Christoph, Spurbeck is one of three RNA-sequencing studies that underwent full in-depth analysis in the P2RA study, so is worth looking at closely here.\nThis one turned out to be a bit of a saga. As we’ll see in the section on human-virus identification, it took multiple tries and several substantial changes to the pipeline to get things to a state I was happy with. Still, I am happy with the outcome and think the changes will improve analysis of future datasets.\nThe raw data\nThe Spurbeck data comprises 55 samples from 8 processing groups, with 4 to 6 samples per group. The number of sequencing read pairs per sample varied widely from 4.5M-106.7M (mean 33.4M). Taken together, the samples totaled roughly 1.8B read pairs (425 gigabases of sequence). Read qualities were generally high but in need of some light preprocessing. Adapter levels were moderate. Inferred duplication levels were fairly high: 14-91% with a mean of 55%.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-04-01_spurbeck/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats_1.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE)\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n    # mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n\n\nCode# Visualize basic stats\nscale_fill_grp &lt;- purrr::partial(scale_fill_brewer, palette=\"Set3\", name=\"Processing group\")\ng_basic &lt;- ggplot(basic_stats_raw, aes(x=group, fill=group, group=sample)) +\n  scale_fill_grp() + theme_kit\ng_nreads_raw &lt;- g_basic + geom_col(aes(y=n_read_pairs), position = \"dodge\") +\n  scale_y_continuous(name=\"# Read pairs\", expand=c(0,0))\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\nlegend_group &lt;- get_legend(g_nreads_raw)\n\n\ng_nbases_raw &lt;- g_basic + geom_col(aes(y=n_bases_approx), position = \"dodge\") +\n  scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + \n  theme(legend.position = \"none\")\ng_ndup_raw &lt;- g_basic + geom_col(aes(y=percent_duplicates), position = \"dodge\") +\n  scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) +\n  theme(legend.position = \"none\")\n\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_group, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\n\n\n\n\n\nCodescale_color_grp &lt;- purrr::partial(scale_color_brewer,palette=\"Set3\",name=\"Processing group\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_grp() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,50),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_wrap(~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. Significant numbers of reads were lost at each stage in the preprocessing pipeline. In total, cleaning, deduplication and initial ribodepletion removed 17-96% (mean 72%) of input reads, while secondary ribodepletion removed an additional 0-11% (mean 6%).\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, group, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, group) %&gt;% arrange(sample, group, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% rename(Stage=stage) %&gt;% group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\")) %&gt;% tail(-1) %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=group,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost_abs_marginal,fill=group,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning with FASTP was mostly successful at removing adapters; however, detectable levels of Illumina Universal Adapter sequences persisted through the preprocessing pipeline. FASTP was successful at improving read quality.\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_grp() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, deduplication was quite effective at reducing measured duplicate levels, with FASTQC-measured levels falling from an average of 55% to one of 22%:\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=group, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=group, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_grp() +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, group, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"group\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, group=group,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:group), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(group, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\\n\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~group, scales=\"free\", ncol=5) +\n  theme_kit\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\\n\",\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~group, scales = \"free_x\", ncol=5) +\n  theme_kit\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, group) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, group) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_total &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n                  classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, group) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")  %&gt;%\n  mutate(group = \"All groups\")\np_reads_summ_prep &lt;- bind_rows(p_reads_summ_total, p_reads_summ_group) %&gt;%\n  mutate(group = fct_inorder(group),\n         classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(classification, group, display) %&gt;%\n  pivot_wider(names_from=group, values_from = display)\np_reads_summ\n\n\n  \n\n\n\nAverage composition varied substantially between groups. Four groups (A, B, I, J) showed high within-group consistency, with high levels of ribosomal reads and very low levels of assigned reads. Other groups showed much more variability between samples, with higher average levels of assigned reads.\nOverall, the average relative abundance of viral reads was 0.04%; however, groups F, G & H showed substantially higher average abundance of around 0.1%. Even these elevated groups, however, still show lower total viral abundance than Rothman or Crits-Christoph, so this doesn’t seem particularly noteworthy.\nHuman-infecting virus reads: validation, round 1\nNext, I investigated the human-infecting virus read content of these unenriched samples, using a pipeline identical to that described in my entry on unenriched Rothman samples. This process identified a total of 31,610 read pairs across all samples (0.007% of surviving reads):\n\nCodehv_reads_filtered_1_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered_1.tsv.gz\")\nhv_reads_filtered_1 &lt;- read_tsv(hv_reads_filtered_1_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_1 &lt;- hv_reads_filtered_1 %&gt;% group_by(sample, group) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_1_summ &lt;- n_hv_filtered_1 %&gt;% ungroup %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCodemrg_1 &lt;- hv_reads_filtered_1 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg_1 &lt;- ggplot(mrg_1, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_1\n\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_1, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\n\n\n\nTo analyze all these reads in a reasonable amount of time, I set up a dedicated EC2 instance, downloaded nt, and ran BLASTN locally there, otherwise using the same parameters I’ve used for past datasets:\n\nCodemrg_1_num &lt;- mrg_1 %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n    mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_1_fasta &lt;-  mrg_1_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_1_fasta_out &lt;- do.call(paste, c(mrg_1_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_1_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-1.fasta\"))\n\n\n\nCodeviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# NB: Importing partially-processed BLAST results to save storage space\n# Import BLAST results\nblast_results_1_path &lt;- file.path(data_dir, \"putative-viral-blast-best-1.tsv.gz\")\nblast_results_1 &lt;- read_tsv(blast_results_1_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_1_best &lt;- blast_results_1 %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_1_ranked &lt;- blast_results_1_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_1_highrank &lt;- blast_results_1_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_1_paired &lt;- blast_results_1_highrank %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_1_viral &lt;- blast_results_1_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_1_assign &lt;- mrg_1_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_1_assign &lt;- full_join(blast_results_1_viral, mrg_1_assign, by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_1_out &lt;- blast_results_1_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_1_blast &lt;- full_join(mrg_1_num, blast_results_1_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_mrg_blast_1 &lt;- mrg_1_blast %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,40), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg_blast_1\n\n\n\n\n\n\n\n\nCodeg_hist_blast_1 &lt;- ggplot(mrg_1_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_wrap(~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_blast_1\n\n\n\n\n\n\n\nThere results were…okay. There were a lot of false positives, but nearly all of them were low-scoring and excluded by my usual score filters. Most true positives, meanwhile, had high enough scores to be retained by those filters. However, there were enough high-scoring false positives and low-scoring true positives to drag down my precision and sensitivity, resulting in an F1 score (at a disjunctive score threshold of 20) a little under 90% – quite a few percentage points lower than I usually aim for.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special=FALSE, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\nstats_1 &lt;- range_f1(mrg_1_blast)\nthreshold_opt_1 &lt;- stats_1 %&gt;% group_by(conj_label) %&gt;% \n  filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_1 &lt;- ggplot(stats_1, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_1, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats_1\n\n\n\n\n\n\n\nDigging into taxonomic assignments more deeply, we find that low-scoring false positives are primarily mapped by Bowtie2 to SARS-CoV-2, while higher-scoring false positives are mainly mapped to a variety of parvoviruses and parvo-like viruses, as well as Orf virus (cows again?). High-scoring true positives map to a wide range of viruses, but low-scoring true-positives primarily map to human gammaherpesvirus 4. Given that the latter is a DNA virus, I find this quite suspicious.\n\nCodefp_1 &lt;- mrg_1_blast %&gt;% \n  group_by(viral_status_out, highscore = adj_score_max &gt;= 20, taxid) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name))\nfp_1_major_tab &lt;- fp_1 %&gt;% filter(p &gt; 0.05) %&gt;% arrange(desc(p))\nfp_1_major_list &lt;- fp_1_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_1_major &lt;- fp_1 %&gt;% mutate(major = p &gt; 0.1) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_1_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\ng_fp_1 &lt;- ggplot(fp_1_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_brewer(palette = \"Set3\", name = \"Viral\\ntaxon\") +\n  facet_wrap(~status_display) +\n  guides(fill=guide_legend(nrow=6)) +\n  theme_kit\ng_fp_1\n\n\n\n\n\n\n\nSince sensitivity is more of a problem here than precision, I decided to dig into into the “true positive” herpesvirus reads. There are 1,197 of these in total, 93% of which are low scoring. When I look into the top taxids that BLAST maps these reads to, we see the following:\n\nCode# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n# Get BLAST staxids\nherp_seqs_1 &lt;- mrg_1_blast %&gt;% filter(taxid == 10376, viral_status_out) %&gt;%\n  select(sample, seq_num, taxid, adj_score_max) %&gt;%\n  mutate(highscore = adj_score_max &gt;= 20)\nherp_seqs_1_total &lt;- herp_seqs_1 %&gt;% group_by(highscore) %&gt;% \n  count(name=\"n_seqs_total\")\nherp_blast_1_hits &lt;- herp_seqs_1 %&gt;% \n  left_join(blast_results_1_paired, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(staxid = as.integer(staxid))\nherp_blast_1_hits_top &lt;- herp_blast_1_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% \n  arrange(desc(n)) %&gt;% left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name=ifelse(staxid == 3004166, \n                     \"Caudopunctatus cichlid hepacivirus\", name)) %&gt;%\n  mutate(name = fct_inorder(name))\n\n# Plot\ng_herp_1 &lt;- ggplot(herp_blast_1_hits_top %&gt;% head(10), \n                   aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name=\"# Mapped Read Pairs\") + theme_base + \n  theme(axis.title.y = element_blank())\ng_herp_1\n\n\n\n\n\n\n\nTwo things strike me as notable about these results. First, human gammaherpesvirus 4 is only the second-most-common subject taxid, after SARS-CoV-2, an unrelated virus with a different nucleic-acid type. Hmm. Second, close behind these two viruses is a distinctly non-viral taxon, the common carp. This jumped out at me, because the common carp genome is somewhat notorious for being contaminated with Illumina adapter sequences. This makes me suspect that Illumina adapter contamination is playing a role in these results.\n\nCodecarp_hits_1 &lt;- herp_blast_1_hits %&gt;% filter(staxid == 7962) %&gt;% \n  select(sample, seq_num, taxid) %&gt;% \n  left_join(mrg_1_blast, by=c(\"sample\", \"seq_num\", \"taxid\")) %&gt;% \n  select(sample, seq_num, taxid, adj_score_max, query_seq_fwd, query_seq_rev)\ncarp_hits_1_out &lt;- carp_hits_1 %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\ncarp_hits_1_fasta &lt;- carp_hits_1_out %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\ncarp_hits_1_fasta_out &lt;- do.call(paste, c(carp_hits_1_fasta, sep=\"\\n\")) %&gt;% \n  paste(collapse=\"\\n\")\nwrite(carp_hits_1_fasta_out, file.path(data_dir, \"spurbeck-carp-hits-1.fasta\"))\n\n\nInspecting these reads manually, I find that a large fraction do indeed show substantial adapter content. In particular, of the 1658 individual reads queried, at least 1338 (81%) showed a strong match to the Illumina multiplexing primer. As such, better removal of these adapter sequences would likely remove most or all of these “false true positives”, potentially significantly improving my results for this dataset.\nHuman-infecting virus reads: validation, round 2\nTo address this problem, I added Cutadapt to the pipeline, using settings that allowed it to trim known adaptors internal to the read:\ncutadapt -b file:&lt;adapter_file&gt; -B file:&lt;adapter_file&gt; -m 15 -e 0.25 --action=trim -o &lt;fwd_reads_out&gt; -p &lt;rev_reads_out&gt; &lt;fwd_reads_in&gt; &lt;rev_reads_in&gt;\nRunning this additional preprocessing step reduced the total number of reads surviving cleaning by 10.4M, or an average of 189k reads per sample. The number of putative human-viral reads, meanwhile, was reduced from 31,610 to 19,799, a 37% decrease. This reduction is primarily due to a large decrease in the number of low-scoring Bowtie2-only putative HV hits.\n\nCodebasic_stats_2_path &lt;- file.path(data_dir, \"qc_basic_stats_2.tsv\")\nbasic_stats_2 &lt;- read_tsv(basic_stats_2_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nbasic_stats_2_sum &lt;- basic_stats_2 %&gt;% group_by(stage) %&gt;% summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx)) %&gt;% mutate(label = \"With Cutadapt\")\nbasic_stats_sum &lt;- basic_stats %&gt;% group_by(stage) %&gt;% summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx)) %&gt;% mutate(label = \"Without Cutadapt\")\nsum_cat &lt;- bind_rows(basic_stats_sum, basic_stats_2_sum) %&gt;% mutate(label=fct_inorder(label))\ng_sum_cat_reads &lt;- ggplot(sum_cat, aes(x=stage, y=n_read_pairs, fill=label)) +\n  geom_col(position=\"dodge\") + \n  scale_fill_brewer(palette = \"Set1\", name=\"Preprocessing\") + \n  scale_y_continuous(name=\"# Read Pairs\", expand=c(0,0)) +\n  theme_kit\ng_sum_cat_reads\n\n\n\n\n\n\n\n\nCodehv_reads_filtered_2_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered_2.tsv.gz\")\nhv_reads_filtered_2 &lt;- read_tsv(hv_reads_filtered_2_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_2 &lt;- hv_reads_filtered_2 %&gt;% group_by(sample, group) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_2_summ &lt;- n_hv_filtered_2 %&gt;% ungroup %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Make new labelled HV dataset\nmrg_2 &lt;- hv_reads_filtered_2 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE))\n\n# Merge and plot histogram\nmrg_2_join &lt;- bind_rows(mrg_1 %&gt;% mutate(label=\"Without Cutadapt\"),\n                      mrg_2 %&gt;% mutate(label=\"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label))\n\n\ng_hist_2 &lt;- ggplot(mrg_2_join, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + \n  facet_grid(label~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_2\n\n\n\n\n\n\n\nComparing the lengths of the same sequences in the old versus new results, we see that many reads are reduced in length as a result of adding Cutadapt. As such, it made sense to repeat the BLAST analysis on the reprocessed reads rather than using the BLAST assignments from the previous analysis.\n\nCodemrg_num_comp &lt;- inner_join(mrg_2 %&gt;% ungroup %&gt;% select(-seq_num), \n                           mrg_1_num %&gt;% ungroup %&gt;% select(seq_id, seq_num), by=c(\"seq_id\"))\nmrg_num_diff &lt;- mrg_1_num %&gt;% select(sample, seq_num, query_len_fwd_old = query_len_fwd,\n                                   query_len_rev_old = query_len_rev) %&gt;% \n  inner_join(mrg_num_comp %&gt;% select(sample, seq_num, query_len_fwd_new = query_len_fwd, \n                                  query_len_rev_new = query_len_rev),\n             by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(query_len_fwd_diff = query_len_fwd_new - query_len_fwd_old,\n         query_len_rev_diff = query_len_rev_new - query_len_rev_old)\nmrg_num_diff_long &lt;- mrg_num_diff %&gt;% \n  select(sample, seq_num, Forward=query_len_fwd_diff, Reverse=query_len_rev_diff) %&gt;%\n  pivot_longer(-(sample:seq_num), names_to=\"read\", values_to=\"length_difference\")\ng_len_diff &lt;- ggplot(mrg_num_diff_long, aes(x=length_difference)) +\n  geom_histogram(binwidth=4, boundary=2) +\n  scale_x_continuous(name=\"Effect of Cutadapt on read length\") +\n  scale_y_continuous(name=\"# of Reads\", expand=c(0,0)) +\n  facet_wrap(~read) +\n  theme_base\ng_len_diff\n\n\n\n\n\n\n\n\nCodemrg_2_num &lt;- mrg_2 %&gt;% group_by(sample) %&gt;%\n  arrange(sample, desc(adj_score_max)) %&gt;%\n  mutate(seq_num = row_number(), seq_head = paste0(\"&gt;\", sample, \"_\", seq_num))\nmrg_2_fasta &lt;-  mrg_2_num %&gt;% \n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_2_fasta_out &lt;- do.call(paste, c(mrg_1_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_2_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-2.fasta\"))\n\n\n\nCode# Import BLAST results (again, pre-filtered to save space)\nblast_results_2_path &lt;- file.path(data_dir, \"putative-viral-blast-best-2.tsv.gz\")\nblast_results_2 &lt;- read_tsv(blast_results_2_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_2_best &lt;- blast_results_2 %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_2_ranked &lt;- blast_results_2_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_2_highrank &lt;- blast_results_2_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_num = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-2), \n         sample = str_split(qseqid, \"_\") %&gt;% lapply(head, n=-2) %&gt;% \n           sapply(paste, collapse=\"_\")) %&gt;%\n    mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num))\n\n# Summarize by read pair and taxid\nblast_results_2_paired &lt;- blast_results_2_highrank %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_2_viral &lt;- blast_results_2_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_2_assign &lt;- mrg_2_num %&gt;% select(sample, seq_num, taxid, assigned_taxid)\nblast_results_2_assign &lt;- full_join(blast_results_2_viral, mrg_2_assign, \n                                    by=c(\"sample\", \"seq_num\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_2_out &lt;- blast_results_2_assign %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0))\n\n\nThe addition of Cutadapt results in a substantial decrease in low-scoring putative HV sequences, resulting in a large improvement in measured sensitivity and F1 score:\n\nCode# Merge BLAST results with unenriched read data\nmrg_2_blast &lt;- full_join(mrg_2_num, blast_results_2_out, by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Combine and label\nhist_blast_2_prep &lt;- bind_rows(mrg_1_blast %&gt;% mutate(label=\"Without Cutadapt\"),\n                               mrg_2_blast %&gt;% mutate(label=\"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label))\n\n# Plot\ng_hist_blast_2 &lt;- ggplot(hist_blast_2_prep, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(label~viral_status_out) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_blast_2\n\n\n\n\n\n\n\n\nCodestats_2 &lt;- range_f1(mrg_2_blast) %&gt;% mutate(label = \"With Cutadapt\")\nstats_comb &lt;- bind_rows(stats_1 %&gt;% mutate(label = \"Without Cutadapt\"), stats_2)\n\ng_stats_2 &lt;- ggplot(stats_comb, aes(x=threshold, y=value, color=label, linetype=conj_label)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(NA,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\", name = \"Configuration\") +\n  scale_linetype_discrete(name=\"Threshold type\") +\n  facet_wrap(~metric, nrow=2, scales = \"free_y\") +\n  theme_base\ng_stats_2\n\n\n\n\n\n\n\nAt a disjunctive threshold of 20, excluding “fake true positives” arising from adapter contamination improved measured sensitivity from 86% to 97%, bringing the measured F1 score up from 89% to 95%. I would feel much better about using these results for further downstream analyses.\nThat said, I think further improvement is possible. While addition of Cutadapt processing substantially improved measured sensitivity, measured precision only improved slightly. Looking at the apparent taxonomic makeup of putative HV reads, we see that, while low-scoring “true positives” mapping to herpesviruses have been mostly eliminated, high-scoring false positives still map primarily to parvoviruses and parvo-like viruses:\n\nCode# Calculate composition for 2nd attempt\nmajor_threshold &lt;- 0.1\nfp_2 &lt;- mrg_2_blast %&gt;% group_by(viral_status_out, highscore = adj_score_max &gt;= 20, taxid) %&gt;% \n  count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name),\n         major = p &gt; major_threshold)\nfp_2_major_tab &lt;- fp_2 %&gt;% filter(major) %&gt;% arrange(desc(p))\nfp_2_major_list &lt;- fp_2_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_2_major &lt;- fp_2 %&gt;% mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_2_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Combine composition\nfp_major_comb &lt;- bind_rows(fp_1_major %&gt;% mutate(label = \"Without Cutadapt\"),\n                           fp_2_major %&gt;% mutate(label = \"With Cutadapt\")) %&gt;%\n  mutate(label = fct_inorder(label)) %&gt;%\n  arrange(as.character(name_display)) %&gt;% arrange(str_detect(name_display, \"Other\")) %&gt;%\n  mutate(name_display = fct_inorder(name_display))\n\n# Palette\npalette_fp_2 &lt;- c(brewer.pal(12, \"Set3\"), \"#999999\")\ng_fp_2 &lt;- ggplot(fp_major_comb, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), breaks = seq(0,1,0.2), expand = c(0,0), label = function(y) y*100) +\n  scale_fill_manual(name = \"Viral\\ntaxon\", values=palette_fp_2) +\n  facet_grid(label~status_display) +\n  guides(fill=guide_legend(nrow=6)) +\n  theme_kit\ng_fp_2\n\n\n\n\n\n\n\nNot only are these parvoviruses and parvo-like viruses DNA viruses (and thus suspicious as abundant components of the RNA virome), they are also the subject of a famous controversy in early viral metagenomics, where viruses apparently widespread in Chinese hepatitis patients were found to arise from spin column contamination. In fact, these viruses seem not to be human-infecting at all; the authors of the study reporting the contamination suggested that they might be viruses of the diatom algae used as the source of silica for these columns. As such, it seems appropriate to remove these from the human-virus database used to identify putative HV reads.\nWe also see a significant number of false-positive reads mapped to Orf virus. Historically this has been a sign of contamination with bovine sequences, and indeed the top taxa mapped to these reads by BLAST include Bos taurus (cattle), Bos mutus (wild yak), and Cervus elaphus (red deer).\n\nCodeorf_taxid &lt;- 10258\norf_seqs &lt;- mrg_1_blast %&gt;% filter(taxid == 10258)\norf_matches &lt;- orf_seqs %&gt;% inner_join(blast_results_1_paired, by=c(\"sample\", \"seq_num\"))\norf_staxids &lt;- orf_matches %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;% mutate(staxid = as.integer(staxid)) %&gt;% left_join(tax_names, by=\"staxid\")\norf_fp_out &lt;- orf_seqs %&gt;% filter(!viral_status_out) %&gt;% arrange(desc(adj_score_max)) %&gt;%\n  ungroup %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", taxid, \"_\", row_number()))\norf_fp_fasta &lt;- orf_fp_out %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\norf_fp_fasta_out &lt;- do.call(paste, c(orf_fp_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(orf_fp_fasta_out, file.path(data_dir, \"spurbeck-orf-fp.fasta\"))\n\n\nThe ideal approach to removing these false positives would be to add the cow genome to the Kraken database we use for validation of Bowtie2 assignments, along with the pig genome and probably some other known contaminants causing issues. This is probably worth doing at some point, but represents a substantial amount of additional work; I’d rather use a simpler alternative right now if one is available. One option that comes to mind is to try replacing the BBMap aligner I’m using to detect and remove contaminants with Bowtie2; in quick experiments, running Bowtie2 (--sensitive) on the putative Orf virus sequences above, using the same dataset of contaminants for the index, successfully removed about two-thirds of them. While this doesn’t guarantee that Bowtie2 would perform as well on the whole population of contaminating cow sequences, it suggests that using Bowtie2 in place of BBMap is worth trying.\nAs such, I re-ran the HV detection pipeline again, having made two changes: first, removing Parvovirus NIH-CQV and the parvo-like viruses from the HV database used to identify putative HV reads, and secondly, replacing BBMap with Bowtie2 for contaminant removal.\nHuman-infecting virus reads: validation, round 3\nI tried re-running the HV detection pipeline with three different sets of Bowtie2 settings: (a) --sensitive, (b) --local --sensitive-local, and (c) --local --very-sensitive-local. In total, these find 26,370, 15,338 and 13,160 putative human-viral reads, respectively, compared to 31,610 for my initial attempt and 19,799 after the addition of Cutadapt:\n\nCodehv_reads_filtered_3_paths &lt;- paste0(data_dir, \"hv_hits_putative_filtered_3\", \n                                    c(\"a\", \"b\", \"c\"), \".tsv.gz\")\nhv_reads_filtered_3_raw &lt;- lapply(hv_reads_filtered_3_paths, read_tsv, show_col_types = FALSE)\nhv_reads_filtered_3 &lt;- lapply(1:3, function(n) hv_reads_filtered_3_raw[[n]] %&gt;% \n                                mutate(attempt=paste0(\"Bowtie2\", c(\"(a)\",\"(b)\",\"(c)\")[n]))) %&gt;% \n  bind_rows() %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(group) %&gt;%\n  mutate(sample = fct_inorder(sample))\nn_hv_filtered_3 &lt;- hv_reads_filtered_3 %&gt;% group_by(sample, group, attempt) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_3_summ &lt;- n_hv_filtered_3 %&gt;% group_by(attempt) %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\nbind_rows(n_hv_filtered_2_summ %&gt;% mutate(attempt=\"BBMap\"), n_hv_filtered_3_summ) %&gt;% \n  select(attempt, everything())\n\n\n  \n\n\n\n\nCode# Make new labelled HV dataset\nmrg_3 &lt;- hv_reads_filtered_3 %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE))\n\n# Merge and plot histogram\nmrg_join_3 &lt;- bind_rows(mrg_2 %&gt;% mutate(attempt=\"BBMap\"), mrg_3)\n\n\ng_hist_3 &lt;- ggplot(mrg_join_3, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + \n  facet_grid(attempt~kraken_label) + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_3\n\n\n\n\n\n\n\nIn all cases, the great majority of false-positive sequences from previous attempts were successfully removed (85%/89%/90%, respectively), along with a small number of true-positives (0.19%/0.23%/0.23%, respectively). At the same time, a number of new putative HV sequences arose as a result of the change in filtering algorithm: 8,512/2,224/1,166 respectively. The great majority of these (97.6%/99.1%/98.8%) are low-scoring, suggesting that they are mostly or entirely false matches that are being let through by Bowtie2 that were previously being caught by BBMap.\n\nCodemrg_2_blast_prep &lt;- mrg_2_blast %&gt;% ungroup %&gt;% \n  select(seq_id, seq_num, viral_status, viral_status_out)\nmrg_3_blast_old &lt;- mrg_join_3 %&gt;% select(-seq_num) %&gt;% ungroup %&gt;% \n  left_join(mrg_2_blast_prep, by = \"seq_id\") %&gt;%\n  rename(viral_status_old = viral_status) %&gt;%\n  mutate(viral_status_old_out = replace_na(as.character(viral_status_out), \"UNKNOWN\")) %&gt;%\n  select(-viral_status_out)\nmrg_3_blast_old_cum &lt;- mrg_3_blast_old %&gt;%\n  mutate(vs_label = paste0(\"HV status:\\n\", viral_status_old_out),\n         attempt_label = paste(\"Attempt\", attempt))\nmrg_3_blast_old_summ &lt;- mrg_3_blast_old_cum %&gt;% \n  group_by(attempt, viral_status_old_out, highscore = adj_score_max &gt;= 20) %&gt;% count %&gt;%\n  mutate(score_label = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         vs_label = paste(\"HV status:\", viral_status_old_out))\ng_status_summ &lt;- ggplot(mrg_3_blast_old_summ, aes(x=attempt, y=n, fill=attempt)) +\n  geom_col(position=\"dodge\") +\n  facet_wrap(score_label~vs_label, scales = \"free_y\") +\n  scale_y_continuous(name=\"# Read pairs\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  theme_kit\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\nCodeg_status_summ\n\n\n\n\n\n\n\nChecking putative Orf virus reads specifically, all three Bowtie2-based attempts successfully remove most putative Orf reads from the previous attempt, while introducing some number of new putative hits (of which I assume the vast majority are fake hits arising from bovine contamination). In attempt (a), these new putative hits outnumber the old hits that were removed, resulting in a net increase in putative (but, I’m fairly confident, false) Orf-virus hits, whether total or high-scoring. Conversely, attempts (b) and (c) manage to remove even more old putative Orf hits while creating many fewer new ones, resulting in a large net decrease in total and high-scoring putative Orf hits:\n\nCodeorf_seqs_3 &lt;- mrg_3_blast_old_cum %&gt;% filter(taxid == 10258)\norf_seqs_3_summ &lt;- orf_seqs_3 %&gt;% \n  group_by(attempt, viral_status_old_out, highscore = adj_score_max &gt;= 20) %&gt;%\n  count\ng_orf_total &lt;- orf_seqs_3_summ %&gt;% group_by(attempt) %&gt;% summarize(n=sum(n)) %&gt;%\n  ggplot(aes(x=attempt, y=n, fill=attempt)) + geom_col() +\n  scale_y_continuous(name=\"Total reads mapped to Orf virus\", limits=c(0,250), expand=c(0,0)) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  theme_kit\ng_orf_high &lt;- orf_seqs_3_summ %&gt;% filter(highscore) %&gt;%\n  ggplot(aes(x=attempt, y=n, fill=attempt)) + geom_col() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  guides(fill=FALSE) +\n  scale_y_continuous(name=\"High-scoring reads mapped to Orf virus\",\n                     limits=c(0,250), expand=c(0,0)) +\n  theme_kit\ng_orf_total + g_orf_high\n\n\n\n\n\n\n\nNext, I validated all new putative HV sequences with BLASTN as before, then evaluated each attempt’s performance against all the putative HV reads identified by any attempt:\n\nCodemrg_3_fasta_prep &lt;- mrg_join_3 %&gt;%\n  filter(! seq_id %in% mrg_2_blast$seq_id) %&gt;%\n  group_by(seq_id) %&gt;% filter(row_number() == 1) %&gt;% ungroup\nmrg_3_fasta &lt;- mrg_3_fasta_prep %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;% ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_3_fasta_out &lt;- do.call(paste, c(mrg_2_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_3_fasta_out, file.path(data_dir, \"spurbeck-putative-viral-3.fasta\"))\n\n\n\nCode# Import BLAST results (again, pre-filtered to save space)\n# blast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\n# blast_results_3_path_0 &lt;- file.path(data_dir, \"spurbeck-putative-viral-3.blast.gz\")\n# blast_results_3 &lt;- read_tsv(blast_results_3_path_0, show_col_types = FALSE,\n#                           col_names = blast_cols, col_types = cols(.default=\"c\"))\n\nblast_results_3_path &lt;- file.path(data_dir, \"putative-viral-blast-best-3.tsv.gz\")\nblast_results_3 &lt;- read_tsv(blast_results_3_path, show_col_types = FALSE, col_types = cols(.default=\"c\"))\n\n# Filter for best hit for each query/subject\nblast_results_3_best &lt;- blast_results_3 %&gt;% group_by(qseqid, staxid) %&gt;%\n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query\nblast_results_3_ranked &lt;- blast_results_3_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_3_highrank &lt;- blast_results_3_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=2), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_3_paired &lt;- blast_results_3_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            best_rank = min(rank),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_3_viral &lt;- blast_results_3_paired %&gt;%\n  mutate(viral = staxid %in% viral_taxa$taxid,\n         viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_3_assign &lt;- mrg_3_fasta_prep %&gt;% select(seq_id, taxid, assigned_taxid)\nblast_results_3_assign &lt;- full_join(blast_results_3_viral, mrg_3_assign, by=c(\"seq_id\")) %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_3_out &lt;- blast_results_3_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status_new = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\") %&gt;%\n  mutate(viral_status_new = replace_na(viral_status_new, 0))\n\n\n\nCodemrg_3_blast_new &lt;- mrg_3_blast_old %&gt;%\n  left_join(blast_results_3_out, by=\"seq_id\")\nmrg_3_blast &lt;- mrg_3_blast_new %&gt;%\n  mutate(viral_status = pmax(viral_status_old, viral_status_new, na.rm = TRUE),\n         viral_status = replace_na(viral_status, 0),\n         viral_status_out = viral_status &gt; 0)\nmrg_3_blast_fill_fwd &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, adj_score_fwd) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"adj_score_fwd\", values_fill=0) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"adj_score_fwd\")\nmrg_3_blast_fill_rev &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, adj_score_rev) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"adj_score_rev\", values_fill=0) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"adj_score_rev\")\nmrg_3_blast_fill_ass &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, assigned_hv) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"assigned_hv\", values_fill=FALSE) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"assigned_hv\")\nmrg_3_blast_fill_hit &lt;- mrg_3_blast %&gt;% select(attempt, seq_id, hit_hv) %&gt;%\n  pivot_wider(names_from=\"attempt\", values_from=\"hit_hv\", values_fill=FALSE) %&gt;%\n  pivot_longer(-seq_id, names_to = \"attempt\", values_to = \"hit_hv\")\nmrg_3_blast_fill_ref &lt;- mrg_3_blast %&gt;% group_by(seq_id, sample, viral_status_out, taxid) %&gt;% summarize(.groups = \"drop\") %&gt;% group_by(seq_id, sample, viral_status_out) %&gt;% summarize(taxid = taxid[1], .groups = \"drop\")\nmrg_3_blast_fill &lt;- full_join(mrg_3_blast_fill_fwd, mrg_3_blast_fill_rev, \n                              by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_ass, by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_hit, by=c(\"attempt\", \"seq_id\")) %&gt;%\n  left_join(mrg_3_blast_fill_ref, by=\"seq_id\") %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n\n\n\nCodeattempts &lt;- mrg_3_blast_fill %&gt;% group_by(attempt) %&gt;% summarize %&gt;% pull(attempt)\nstats_3_raw &lt;- mrg_3_blast_fill %&gt;% group_by(attempt) %&gt;% group_split %&gt;%\n  lapply(range_f1)\nstats_3 &lt;- lapply(1:4, function(n) stats_3_raw[[n]] %&gt;% mutate(attempt=attempts[n])) %&gt;%\n  bind_rows\ng_stats_3 &lt;- ggplot(stats_3, aes(x=threshold, y=value, color=attempt, linetype=conj_label)) +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(NA,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\", name=\"Configuration\") +\n  scale_linetype_discrete(name=\"Threshold type\") +\n  facet_wrap(~metric, nrow=2, scales = \"free_y\") +\n  guides(color=guide_legend(nrow=2), linetype=guide_legend(nrow=2)) +\n  theme_base\ng_stats_3\n\n\n\n\n\n\n\nAt a disjunctive threshold of 20, Bowtie2(c) (--very-sensitive-local) performs the best, with an F1 score of 0.979; Bowtie2(b) (--sensitive-local) is very close behind. In both cases, precision (&gt;=0.988) is higher than sensitivity (~0.969), suggesting that any further improvements would come from investigating low-scoring true-positives:\n\nCodemajor_threshold &lt;- 0.1\nfp_3 &lt;- mrg_3_blast %&gt;% group_by(attempt, viral_status_out, \n                           highscore = adj_score_max &gt;= 20, taxid) %&gt;% \n  count %&gt;% \n  group_by(attempt, viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name),\n         major = p &gt; major_threshold)\nfp_3_major_tab &lt;- fp_3 %&gt;% filter(major) %&gt;% arrange(desc(p))\nfp_3_major_list &lt;- fp_3_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_3_major &lt;- fp_3 %&gt;% mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(attempt, viral_status_out, highscore, name_display) %&gt;% summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_3_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = paste(\"VS:\", viral_status_out)) #ifelse(viral_status_out, \"True positive\", \"False positive\"))\npalette_fp_3 &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(3, \"Dark2\"), \"#999999\")\ng_fp_3 &lt;- ggplot(fp_3_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values=palette_fp_3, name = \"Viral\\ntaxon\") +\n  facet_grid(attempt~status_display) +\n  guides(fill=guide_legend(nrow=8)) +\n  theme_kit\ng_fp_3\n\n\n\n\n\n\n\nI suspect that small additional gains are possible here, since I’m suspicious about the low-scoring “true-positives” mapping to human betaherpesvirus 5 and human mastadenovirus F. However, I’ve already spent a long time optimizing the results for this dataset, and the results are now good enough that I feel okay with leaving this here. Going forward, I’ll use Bowtie2(c) as my alignment strategy for the HV identification pipeline.\nHuman-infecting virus reads: analysis\nAfter several rounds of validation and refinement, we finally come to actually analyzing the human-infecting virus content of Spurbeck et al. This section might seem disappointingly short compared to all the effort expended to get here.\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, group, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts & RA\nmrg_hv &lt;- mrg_3 %&gt;% filter(attempt == \"Bowtie2(c)\") %&gt;%\n  mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% count(name = \"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Aggregate\nread_counts_group &lt;- read_counts %&gt;% group_by(group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\nread_counts_total &lt;- read_counts_group %&gt;% ungroup %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         group = \"All groups\")\nread_counts_agg &lt;- read_counts_group %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  arrange(group) %&gt;% arrange(str_detect(group, \"All groups\")) %&gt;%\n  mutate(group = fct_inorder(group))\n\n\nApplying a disjunctive cutoff at S=20 identifies 9,990 reads as human-viral out of 1.84B total reads, for a relative HV abundance of \\(5.44 \\times 10^{-6}\\). This compares to \\(2.8 \\times 10^{-4}\\) on the public dashboard, corresponding to the results for Kraken-only identification: a roughly 2x increase, smaller than the 4-5x increases seen for Crits-Christoph and Rothman. Relative HV abundances for individual sample groups ranged from \\(9.19 \\times 10^{-8}\\) to \\(1.58 \\times 10^{-5}\\); as with total virus reads, groups F, G & H showed the highest relative abundance:\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=group, y=p_reads_hv, color=group)) +\n  geom_point() +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(palette = \"Set3\", name = \"Group\") +\n  theme_kit\ng_phv_agg\n\n\n\n\n\n\n\nDigging into particular viruses, we see that Mamastrovirus, Mastadenovirus, Rotavirus and Betacoronavirus are the most abundant genera across samples:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 694009] &lt;- \"SARS-CoV\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 568715] &lt;- \"Astrovirus MLB1\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_genera &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\n\n# Count relative abundance for genera\nhv_genera_counts_raw &lt;- hv_reads_genera %&gt;% group_by(group, name) %&gt;%\n  summarize(n_reads_hv = sum(hv_status), .groups = \"drop\") %&gt;%\n  inner_join(read_counts_agg %&gt;% select(group, n_reads_raw), by=\"group\")\nhv_genera_counts_all &lt;- hv_genera_counts_raw %&gt;% group_by(name) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv),\n            n_reads_raw = sum(n_reads_raw)) %&gt;%\n  mutate(group = \"All groups\")\nhv_genera_counts_agg &lt;- bind_rows(hv_genera_counts_raw, hv_genera_counts_all) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Compute ranks for species and genera and restrict to high-ranking taxa\nmax_rank_genera &lt;- 5\nhv_genera_counts_ranked &lt;- hv_genera_counts_agg %&gt;% group_by(group) %&gt;%\n  mutate(rank = rank(desc(n_reads_hv), ties.method=\"max\"),\n         highrank = rank &lt;= max_rank_genera) %&gt;%\n  group_by(name) %&gt;%\n  mutate(highrank_any = any(highrank),\n         name_display = ifelse(highrank_any, name, \"Other\")) %&gt;%\n  group_by(name_display) %&gt;%\n  mutate(mean_rank = mean(rank)) %&gt;%\n  arrange(mean_rank) %&gt;% ungroup %&gt;%\n  mutate(name_display = fct_inorder(name_display)) %&gt;%\n  arrange(str_detect(group, \"Other\")) %&gt;%\n  mutate(group = fct_inorder(group))\n\n\n# Plot composition\npalette_rank &lt;- c(brewer.pal(8, \"Dark2\"), brewer.pal(8, \"Set2\"), \"#888888\")\ng_vcomp_genera &lt;- ggplot(hv_genera_counts_ranked, aes(x=group, y=n_reads_hv, fill=name_display)) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(values = palette_rank, name = \"Viral genus\") +\n  scale_y_continuous(name = \"Fraction of HV reads\", breaks = seq(0,1,0.2), expand = c(0,0)) +\n  guides(fill = guide_legend(ncol=3)) +\n  theme_kit\ng_vcomp_genera\n\n\n\n\n\n\n\nMamastrovirus and Rotavirus are predominantly enteric RNA viruses whose prominence here makes sense, while the prevalence of Betacoronavirus is probably a result of the ongoing SARS-CoV-2 pandemic. As a DNA virus, the abundance of Mastadenovirus is a bit more surprising; however, this finding is consistent with the public dashboard, and is also borne out by the other BLASTN matches for these sequences, all of which are other adenovirus taxa:\n\nCodemasta_ids &lt;- hv_reads_genera %&gt;% filter(name==\"Mastadenovirus\") %&gt;% pull(seq_id)\nmasta_hits &lt;- bind_rows(blast_results_2_paired %&gt;% full_join(mrg_2_blast %&gt;% select(seq_id, sample, seq_num), by=c(\"sample\", \"seq_num\")),\n                        blast_results_3_paired) %&gt;%\n  select(-sample, -seq_num) %&gt;%\n  filter(seq_id %in% masta_ids) %&gt;%\n  mutate(staxid = as.integer(staxid))\nmasta_hits_sorted &lt;- masta_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name = fct_inorder(name))\nmasta_hits_sorted_head &lt;- masta_hits_sorted %&gt;% head(10) %&gt;%\n  mutate(name=fct_inorder(as.character(name)))\ng_masta &lt;- ggplot(masta_hits_sorted_head,\n                  aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name = \"# Mapped read pairs\") + theme_base +\n  theme(axis.title.y = element_blank())\ng_masta\n\n\n\n\n\n\n\nConclusion\nCompared to the last few datasets I analyzed, the analysis of Spurbeck took a long time, numerous attempts, and a lot of computational resources. However, as I said at the start of this post, I’m happy with the outcome and am confident it will improve analysis of future datasets. While the overall prevalence of human-infecting viruses is fairly low in Spurbeck compared to other wastewater datasets I’ve looked at, its inclusion as a core dataset for the P2RA analysis make it especially important to process in a reliable and high-quality manner.\nNext, I’ll turn my attention to more datasets included in the P2RA analysis, as well as some air-sampling datasets we’re interested in for another project."
  },
  {
    "objectID": "notebooks/2024-04-08_brumfield.html",
    "href": "notebooks/2024-04-08_brumfield.html",
    "title": "Workflow analysis of Brumfield et al. (2022)",
    "section": "",
    "text": "Continuing my analysis of datasets from the P2RA preprint, I analyzed the data from Brumfield et al. (2022). This study conducted longitudinal monitoring of SARS-CoV-2 via qPCR from a single manhole in Maryland, combined with comprehensive microbiome analysis using metagenomics during a major COVID spike in early 2021. In total, they sequenced six samples from February to April 2021.\nOne unusual feature that makes this study interesting is that they conducted both RNA and DNA sequencing on each study. Prior to sequencing, samples underwent concentration with the InnovaPrep Concentrating Pipette, followed by separate DNA & RNA extraction using different kits. RNA samples underwent rRNA depletion prior to library prep. All samples were sequenced on an Illumina HiSeq4000 with 2x150bp reads.\nThe raw data\nThe 6 samples from the Brumfield dataset yielded 24M-45M (mean 33M) DNA-sequencing reads and 29M-64M (mean 46M) RNA-sequencing reads per sample, for a total of 196M DNA read pairs and 276M RNA read pairs (59 and 83 gigabases of sequence, respectively). Read qualities were mostly high but tailed off at the 3’ end in some samples, suggesting the need for trimming. Adapter levels were very high. Inferred duplication levels were 44-58% in DNA reads and 90-96% in RNA reads, i.e. very high.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-03-19_brumfield/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries &lt;- read_csv(libraries_path, show_col_types = FALSE) %&gt;%\n  arrange(date) %&gt;% mutate(date = fct_inorder(as.character(date))) %&gt;%\n  arrange(desc(na_type)) %&gt;% mutate(na_type = fct_inorder(na_type))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n\n\nCode# Visualize basic stats\nscale_fill_na &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", name=\"Nucleic acid type\")\ng_basic &lt;- ggplot(basic_stats_raw, aes(x=date, fill=na_type, group=sample)) +\n  scale_fill_na() + theme_kit\ng_nreads_raw &lt;- g_basic + geom_col(aes(y=n_read_pairs), position = \"dodge\") +\n  scale_y_continuous(name=\"# Read pairs\", expand=c(0,0))\ng_nreads_raw_2 &lt;- g_nreads_raw + theme(legend.position = \"none\")\nlegend_group &lt;- get_legend(g_nreads_raw)\n\ng_nbases_raw &lt;- g_basic + geom_col(aes(y=n_bases_approx), position = \"dodge\") +\n  scale_y_continuous(name=\"Total base pairs (approx)\", expand=c(0,0)) + \n  theme(legend.position = \"none\")\ng_ndup_raw &lt;- g_basic + geom_col(aes(y=percent_duplicates), position = \"dodge\") +\n  scale_y_continuous(name=\"% Duplicates (FASTQC)\", expand=c(0,0), limits = c(0,100), breaks = seq(0,100,20)) +\n  theme(legend.position = \"none\")\n\ng_basic_raw &lt;- plot_grid(g_nreads_raw_2 + g_nbases_raw + g_ndup_raw, legend_group, ncol = 1, rel_heights = c(1,0.1))\ng_basic_raw\n\n\n\n\n\n\n\n\nCodescale_color_na &lt;- purrr::partial(scale_color_brewer,palette=\"Set1\",name=\"Nucleic acid type\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_wrap(~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. On average, cleaning & deduplication together removed about 50% of total reads from DNA libraries and about 92% from RNA libraries, primarily during deduplication. Only a few reads (low single-digit percentages at most) were lost during subsequent ribodepletion, even in RNA reads, consistent with the samples having undergone rRNA depletion prior to sequencing.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, na_type, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(sample, na_type, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% rename(Stage=stage, `NA Type`=na_type) %&gt;% \n  group_by(`NA Type`, Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCode# Plot reads over preprocessing\ng_reads_stages &lt;- ggplot(basic_stats, aes(x=stage, y=n_read_pairs,fill=na_type,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_na() +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, y=p_reads_lost_abs_marginal,fill=na_type,group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_na() +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning with FASTP was very successful at removing adapters, with very few adapter sequences found by FASTQC at any stage after the raw data. FASTP was also successful at improving read quality.\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, deduplication was moderately effective at reducing measured duplicate levels, with FASTQC-measured levels falling from an average of 50% to one of 16% for DNA reads, and from 93% to 42% for RNA reads:\n\nCodeg_dup_stages &lt;- ggplot(basic_stats, aes(x=stage, y=percent_duplicates, fill=na_type, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_na() +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,100), breaks=seq(0,100,20), expand=c(0,0)) +\n  theme_kit\ng_readlen_stages &lt;- ggplot(basic_stats, aes(x=stage, y=mean_seq_len, fill=na_type, group=sample)) +\n  geom_col(position=\"dodge\") + scale_fill_na() +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0)) +\n  theme_kit\nlegend_loc &lt;- get_legend(g_dup_stages)\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, na_type, date, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"na_type\", \"date\"), names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, na_type=na_type, date=date,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:date), names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(na_type, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n  \n# Plot overall composition\ng_comp &lt;- ggplot(read_comp_long, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_wrap(.~na_type, scales=\"free\", ncol=5) +\n  theme_kit\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=sample, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\",\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  facet_wrap(.~na_type, scales = \"free_x\", ncol=5) +\n  theme_kit\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, na_type) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, na_type) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(classification, na_type, display) %&gt;%\n  pivot_wider(names_from=na_type, values_from = display)\np_reads_summ\n\n\n  \n\n\n\nAs previously noted, RNA samples were overwhelmingly composed of duplicates. Despite this, the RNA samples achieved a decently high level of total viral reads, with an average of 0.55% of reads mapping to viruses (higher than Crits-Christoph). Levels of total viral reads were much lower in DNA libraries, which were dominated by unassigned and (non-ribosomal) bacterial sequences.\nWithin viral reads, RNA libraries were (as usual) dominated by plant viruses, particularly Virgaviridae – though one sample, unusually, has a substantial minority of reads from Fiersviridae, a family of RNA phages. Detected DNA viruses come from a wider range of families, but the most abundant families (Suoliviridae, Intestiviridae, Autographiviridae, Myoviridae) are all DNA phages.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import Kraken reports & extract & count viral families\nsamples &lt;- as.character(basic_stats_raw$sample)\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\", \"rank\", \"taxid\", \"name\")\nreport_paths &lt;- paste0(data_dir, \"kraken/\", samples, \".report.gz\")\nkraken_reports &lt;- lapply(1:length(samples), \n                         function(n) read_tsv(report_paths[n], col_names = col_names,\n                                              show_col_types = FALSE) %&gt;%\n                           mutate(sample = samples[n])) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nviral_families &lt;- kraken_reports_viral %&gt;% filter(rank == \"F\") %&gt;%\n  inner_join(libraries, by=\"sample\")\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% group_by(name, taxid, na_type) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= 0.04)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, na_type, date, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% group_by(sample, date, na_type) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, na_type, date, p_reads_viral)\nviral_families_display &lt;- bind_rows(viral_families_major, viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\")))\ng_families &lt;- ggplot(viral_families_display, aes(x=date, y=p_reads_viral, fill=name)) +\n  geom_col(position=\"stack\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Viral family\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  facet_wrap(~na_type) +\n  theme_kit\ng_families\n\n\n\n\n\n\n\nGiven the very high level of duplicates in the RNA data, I also repeated the analysis from my second Yang et al. entry, re-running taxonomic composition analysis on pre-deduplication data and comparing the effects of deduplication on composition:\n\nCodeclass_levels &lt;- c(\"Unassigned\", \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\nsubset_factor &lt;- 0.05\n\n# 1. Remove filtered & duplicate reads from original Bracken output & renormalize\nread_comp_renorm &lt;- read_comp_long %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads),\n         classification = classification %&gt;% as.character %&gt;%\n           ifelse(. == \"Ribosomal\", \"Bacterial\", .)) %&gt;%\n  group_by(sample, na_type, date, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  mutate(classification = factor(classification, levels = class_levels))\n  \n# 2. Import pre-deduplicated \nbracken_path_predup &lt;- file.path(data_dir, \"bracken_counts_subset.tsv\")\nbracken_predup &lt;- read_tsv(bracken_path_predup, show_col_types = FALSE)\ntotal_assigned_predup &lt;- bracken_predup %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread_predup &lt;- bracken_predup %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", values_from = \"new_est_reads\")\n# Count reads\nread_counts_predup &lt;- read_counts_preproc %&gt;%\n  select(sample, na_type, date, raw_concat, cleaned) %&gt;%\n  mutate(raw_concat = raw_concat * subset_factor, cleaned = cleaned * subset_factor) %&gt;%\n  inner_join(total_assigned_predup %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread_predup, by=\"sample\")\n# Assess composition\nread_comp_predup &lt;- transmute(read_counts_predup, sample=sample, na_type = na_type,\n                              date=date,\n                       n_filtered = raw_concat-cleaned,\n                       n_unassigned = cleaned-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_predup_long &lt;- pivot_longer(read_comp_predup, -(sample:date), \n                                      names_to = \"classification\",\n                                      names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads)) %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads))\n\n# 3. Combine\nread_comp_comb &lt;- bind_rows(read_comp_predup_long %&gt;% mutate(deduplicated = FALSE),\n                            read_comp_renorm %&gt;% mutate(deduplicated = TRUE)) %&gt;%\n  mutate(label = ifelse(deduplicated, \"Post-dedup\", \"Pre-dedup\") %&gt;% fct_inorder)\n\n# Plot overall composition\ng_comp_predup &lt;- ggplot(read_comp_comb, aes(x=label, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1.01), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  facet_grid(na_type~date) +\n  theme_kit\ng_comp_predup\n\n\n\n\n\n\n\nIn general, deduplication has little effect on the composition of DNA samples, which remain primarily bacterial and unassigned. A few RNA samples show a reduction in the bacterial (more precisely, bacterial + rRNA) fraction after deduplication, consistent with the presence of some true biological duplicate sequences (most likely rRNA) that are being collapsed. Surprisingly, however, the largest effect is in the opposite direction, with several samples showing large increases in bacterial sequences following deduplication. This suggests that some highly abundant non-bacterial sequence is being collapsed in these samples, increasing the apparent abundance of bacteria.\nThe increase in bacterial reads in these samples comes at the expense of (1) human reads and (2) the unassigned fraction. The former suggests the presence of human rRNA sequences that are being erroneously incorporated into the post-deduplication “bacterial” fraction; however, this effect is smaller than the decrease in unassigned reads. One possibility might be other non-human eukaryotic rRNA sequences, which Kraken is unable to assign using our current database.\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. To get good results here, I needed to make some changes to the pipeline, including adding Trimmomatic as an additional primer-trimming step to prevent further primer-contamination-based false positives. However, for the sake of time I’m not describing them in detail here; if you want to see more I encourage you to check the commit log at the workflow repo.\nHaving made these changes, the workflow identified a total of 14,073 RNA read pairs and 70 DNA read pairs across all samples (0.09% and 0.00009% of surviving reads, respectively).\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(date, na_type)\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, date, na_type) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% group_by(na_type) %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total)) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(na_type~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~kraken_label, scales=\"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\n\n\n\nAs previously described, I ran BLASTN on these reads via a dedicated EC2 instance, using the same parameters I’ve used for previous datasets.\n\nCodemrg_fasta &lt;-  mrg %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;%\n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"brumfield-putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"blast/brumfield-putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, \n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results, viral = staxid %in% viral_taxa$taxid)\n\n# Filter for best hit for each query/subject, then rank hits for each query\nblast_results_best &lt;- blast_results_viral %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(seq_id, staxid, viral) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\") %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max, na_type)\nblast_results_assign &lt;- left_join(blast_results_paired, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_results_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\nmrg_blast_rna &lt;- mrg_blast %&gt;% filter(na_type == \"RNA\")\nmrg_blast_dna &lt;- mrg_blast %&gt;% filter(na_type == \"DNA\")\n\n# Plot RNA\ng_mrg_blast_rna &lt;- mrg_blast_rna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"RNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_rna\n\n\n\n\n\n\nCode# Plot DNA\ng_mrg_blast_dna &lt;- mrg_blast_dna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"DNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_dna\n\n\n\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~viral_status_out, scales = \"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\n\n\n\nThese results look very good on visual inspection, and indeed precision and sensitivity are both very high. For a disjunctive score threshold of 20, my updated workflow achieves an F1 score of 99.0% for RNA sequences and 99.2% for DNA sequences; more than high enough to continue on to human viral analysis.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_rna &lt;- range_f1(mrg_blast_rna, inc_special) %&gt;% mutate(na_type = \"RNA\")\nstats_dna &lt;- range_f1(mrg_blast_dna, inc_special) %&gt;% mutate(na_type = \"DNA\")\nstats_0 &lt;- bind_rows(stats_rna, stats_dna) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt,na_type) %&gt;% \n  filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_grid(na_type~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\n\n\n\nHuman-infecting virus reads: analysis\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, date, na_type, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_total &lt;- read_counts %&gt;% group_by(na_type) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv)) %&gt;%\n  mutate(sample= \"All samples\", date = \"All dates\")\nread_counts_agg &lt;- read_counts %&gt;% arrange(date) %&gt;% \n  mutate(date = as.character(date)) %&gt;% arrange(sample) %&gt;%\n  bind_rows(read_counts_total) %&gt;% \n  mutate(sample = fct_inorder(sample),\n         date = fct_inorder(date),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n# Get old counts\nn_hv_reads_old &lt;- c(691, 121, 18, 224, 2, 26, 6, 21, 4, 29, 12, 22)\nn_hv_reads_old[13] &lt;- sum(n_hv_reads_old[which(read_counts$na_type==\"RNA\")])\nn_hv_reads_old[14] &lt;- sum(n_hv_reads_old[which(read_counts$na_type==\"DNA\")])\n\nread_counts_comp &lt;- read_counts_agg %&gt;%\n  mutate(n_reads_hv_old = n_hv_reads_old,\n         p_reads_hv_old = n_reads_hv_old/n_reads_raw)\n\n\nApplying a disjunctive cutoff at S=20 identifies 13,809 RNA reads and 66 DNA reads as human-viral. This gives an overall relative HV abundance of \\(5.00 \\times 10^{-5}\\) for RNA reads and \\(3.66 \\times 10^{-7}\\) for DNA reads. In contrast, the overall HV in the public dashboard is \\(3.93 \\times 10^{-6}\\) for RNA reads and \\(4.54 \\times 10^{-7}\\); the measured HV has thus increased 12.7x for RNA reads, but decreased slightly for DNA reads.\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=date, color=na_type)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_na() + theme_kit\ng_phv_agg\n\n\n\n\n\n\n\nDigging into individual viruses, we see that fecal-oral viruses predominate, especially Mamastrovirus, Rotavirus, Salivirus, and fecal-oral Enterovirus species (especially Enterovirus C, which includes poliovirus):\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\n\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCode# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, date, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; 0.01)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\n\n# Plot family composition\npalette &lt;- c(brewer.pal(12, \"Set3\"), \"#AAAAAA\")\ng_hv_families &lt;- ggplot(hv_family_counts_major, \n                        aes(x=date, y=p_reads_hv, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_fill_manual(name=\"Viral\\nfamily\", values=palette) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1), breaks=seq(0,1,0.2)) +\n  facet_grid(.~na_type) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_hv_families\n\n\n\n\n\n\n\n\nCode# Count reads for each human-viral genus\nhv_genus_counts &lt;- hv_reads_genus %&gt;% \n  group_by(sample, date, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking genera and group others\nhv_genus_major_tab &lt;- hv_genus_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; 0.02)\nhv_genus_counts_major &lt;- hv_genus_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_genus_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_genus_major_tab$name, \"Other\")))\n\n# Plot genus composition\npalette &lt;- c(brewer.pal(12, \"Set3\"), \"#AAAAAA\")\ng_hv_genera &lt;- ggplot(hv_genus_counts_major, \n                        aes(x=date, y=p_reads_hv, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_fill_manual(name=\"Viral\\ngenus\", values=palette) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1), breaks=seq(0,1,0.2)) +\n  facet_grid(.~na_type) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_hv_genera\n\n\n\n\n\n\n\n\nCode# Count reads for each human-viral species\nhv_species_counts &lt;- hv_reads_species %&gt;% \n  group_by(sample, date, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking species and group others\nhv_species_major_tab &lt;- hv_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; 0.05)\nhv_species_counts_major &lt;- hv_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_species_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_species_major_tab$name, \"Other\")))\n\n# Plot species composition\npalette &lt;- c(brewer.pal(12, \"Set3\"), \"#AAAAAA\")\ng_hv_species &lt;- ggplot(hv_species_counts_major, \n                      aes(x=date, y=p_reads_hv, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_fill_manual(name=\"Viral\\nspecies\", values=palette) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1), breaks=seq(0,1,0.2)) +\n  facet_grid(.~na_type) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_hv_species\n\n\n\n\n\n\n\nBy far the most common virus according to my pipeline (with over 91% of human-viral reads) is Rotavirus A; second (with 8%) is Orthopicobirnavirus hominis. Together, these two enteric RNA viruses dominate HV RNA reads in all samples. Both appear to be real; at least, BLASTN primarily maps these reads to the same or closely-related viruses to that identified by Bowtie2:\n\nCode# Import names db\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tibble(\n  staxid = c(557241, 557245, 557232, 557247, 557242, 557245),\n  name = c(\"Canine rotavirus A79-10/G3P[3]\", \"Human rotavirus Ro1845\", \"Canine rotavirus K9\",\n           \"Human rotavirus HCR3A\", \"Feline rotavirus Cat97/G3P[3]\", \"Feline rotavirus Cat2/G3P[9]\")\n)\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\nrota_ids &lt;- hv_reads_species %&gt;% filter(taxid==28875) %&gt;% pull(seq_id)\nrota_hits &lt;- blast_results_paired %&gt;%\n  filter(seq_id %in% rota_ids) %&gt;%\n  mutate(staxid = as.integer(staxid))\nrota_hits_sorted &lt;- rota_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name = fct_inorder(name))\nrota_hits_sorted_head &lt;- rota_hits_sorted %&gt;% head(10) %&gt;%\n  mutate(name=fct_inorder(as.character(name)))\ng_rota &lt;- ggplot(rota_hits_sorted_head,\n                  aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name = \"# Mapped read pairs\") + theme_base +\n  theme(axis.title.y = element_blank())\ng_rota\n\n\n\n\n\n\n\n\nCode# Add missing names\ntax_names_new &lt;- tibble(\n  staxid = c(442302, 3003954, 585895),\n  name = c(\"Porcine picobirnavirus\", \"ticpantry virus 7\", \"uncultured picobirnavirus\")\n)\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\npico_ids &lt;- hv_reads_species %&gt;% filter(taxid==2956252) %&gt;% pull(seq_id)\npico_hits &lt;- blast_results_paired %&gt;%\n  filter(seq_id %in% pico_ids) %&gt;%\n  mutate(staxid = as.integer(staxid))\npico_hits_sorted &lt;- pico_hits %&gt;% group_by(staxid) %&gt;% count %&gt;% arrange(desc(n)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;%\n  mutate(name = fct_inorder(name))\npico_hits_sorted_head &lt;- pico_hits_sorted %&gt;% head(10) %&gt;%\n  mutate(name=fct_inorder(as.character(name)))\ng_pico &lt;- ggplot(pico_hits_sorted_head,\n                 aes(x=n, y=fct_inorder(name))) + geom_col() +\n  scale_x_continuous(name = \"# Mapped read pairs\") + theme_base +\n  theme(axis.title.y = element_blank())\ng_pico\n\n\n\n\n\n\n\nConclusion\nI’m quite happy with the quality of the human-viral selection process for this dataset, which ended up achieving very high precision and sensitivity. The most striking result coming out of this analysis is probably the drastic difference in total HV abundance between RNA and DNA reads, with the former &gt;100x higher despite very similar processing methods. In the past we’ve attributed low HV abundance in the DNA datasets we’ve analyzed to the lack of viral enrichment in available DNA datasets; in this case, however, there is very little difference between the DNA and RNA processing methods, so this explanation doesn’t really hold. This makes me more pessimistic about achieving good HV relative abundance with DNA sequencing, even with improved viral enrichment methods."
  },
  {
    "objectID": "notebooks/2024-04-12_prussin.html",
    "href": "notebooks/2024-04-12_prussin.html",
    "title": "Workflow analysis of Prussin et al. (2019)",
    "section": "",
    "text": "Taking a break from working on P2RA datasets, we’re also working on a review of air sampling for viral pathogen detection. For that study, we’re collecting and analyzing air MGS data that could give us a high-level idea of the likely viral composition of such samples.\nThe first dataset I’m looking at for this work is Prussin et al. (2019), a study of HVAC filter samples in a Virginia daycare center between 2014 and 2015. Samples were eluted from MERV-14 air filters collected every two weeks, with pairs of successive samples combined into four-week sampling periods. Like Brumfield et al, this study conducted both RNA and DNA sequencing; all samples were sequenced on an Illumina NextSeq500 with 2x150bp reads.\nThe raw data\nThe Prussin dataset comprised sequencing data from 14 timepoints spread across the year, from 20th January 2014 to 2nd February 2015. Each sample represents a four-week sampling period. In addition to the 14 on-site samples, there were also two control samples, a negative control (NC) and an “unexposed filter” control (UFC), which were collected on December 23rd 2014.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-04-11_prussin/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- read_csv(libraries_path, show_col_types = FALSE)\nlibraries &lt;- libraries_raw %&gt;%\n  arrange(desc(na_type)) %&gt;% mutate(na_type = fct_inorder(na_type)) %&gt;%\n  arrange(date) %&gt;% rename(start_date = date) %&gt;%\n  mutate(end_date = start_date + 28) %&gt;%\n  mutate(date = fct_inorder(as.character(end_date)),\n         ctrl = ifelse(grepl(\"Negative_Control\", sample_alias), \"NC\",\n                       ifelse(grepl(\"Unexposed_Filter\", sample_alias),\n                              \"UFC\", \"Non-Control\")),\n         ctrl = factor(ctrl, levels = c(\"Non-Control\",\n                                        \"NC\",\n                                        \"UFC\")),\n         open = (season != \"Closed\") & (ctrl == \"On-Site\"))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% group_by(na_type, ctrl) %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), .groups = \"drop\")\nraw_read_totals &lt;- basic_stats_raw %&gt;% group_by(na_type, ctrl) %&gt;% \n  summarize(n_read_pairs = sum(n_read_pairs), \n            n_bases_approx = sum(n_bases_approx), .groups = \"drop\")\nraw_dup &lt;- basic_stats_raw %&gt;% group_by(na_type, ctrl) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThe 14 positive samples from the dataset yielded 5M-20M (mean 11.3M) RNA-sequencing reads and 10M-42M (mean 21.0M) DNA-sequencing reads per sample, for a total of 159M RNA read pairs and 294M DNA read pairs (46.3 and 87.0 gigabases of sequence, respectively). Controls contributed an additional 1M RNA read pairs and 17.5M DNA read pairs.\nIn positive samples, read qualities were mostly high but tailed off slightly at the 3’ end in some samples, suggesting the need for trimming. Adapter levels were high. Inferred duplication levels were low (2-4%) in DNA reads and moderate (21-44%) in RNA reads. Control sample reads were more problematic, with higher duplication and adapter levels and lower quality.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(date, na_type, ctrl,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(date:ctrl), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_na &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", \n                                name=\"Nucleic acid type\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=date, y=value, fill=na_type)) +\n  geom_col(position = \"dodge\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_na() + \n  facet_grid(metric~ctrl, scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_rotate + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(ctrl~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(ctrl~.)\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(ctrl~., scales = \"free_y\")\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. For positive samples, on average, cleaning and deduplication removed about 19% of total read pairs from RNA libraries and about 7% from DNA libraries. Subsequent ribodepletion removed a further ~32% of total read pairs on average from RNA libraries but &lt;0.5% of total read pairs from DNA libraries.\nControl samples, meanwhile, lost an average of 63% of RNA read pairs and 21% of DNA read pairs during cleaning and deduplication, consistent with the lower qualities observed above. Subsequent ribodepletion removed an additional 9% of total RNA read pairs and 0.3% of total DNA read pairs.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, date, na_type, ctrl, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(sample, na_type, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage, `NA Type`=na_type, `Control?`=ctrl) %&gt;% \n  group_by(`Control?`, `NA Type`, Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=na_type, group=sample)) +\n  scale_color_na() +\n  facet_wrap(ctrl~na_type, scales=\"free\", ncol=2,\n             labeller = label_wrap_gen(multi_line=FALSE)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=na_type, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_na() +\n  facet_wrap(ctrl~na_type, scales=\"free\", ncol=2,\n             labeller = label_wrap_gen(multi_line=FALSE)) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nIn both positive and control samples, data cleaning with FASTP was very successful at removing adapters, with very few adapter sequences found by FASTQC at any stage after the raw data. FASTP was also successful at improving read quality.\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, deduplication was moderately effective at reducing measured duplicate levels in on-site samples, with FASTQC-measured levels falling from an average of 34% to 23% for RNA reads and from 2.7% to 2.2% for DNA reads.\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(na_type, ctrl, stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", \n              values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, na_type, ctrl, date, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"na_type\", \"ctrl\", \"date\"),\n              names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, na_type=na_type,\n                       ctrl = ctrl, date = date,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:date), \n                               names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\n\n# Summarize composition\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(na_type, ctrl, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=date, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_grid(na_type~ctrl, scales = \"free\", space = \"free_x\") +\n  theme_rotate\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = read_comp_long, position = \"stack\") +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[c(6,7,9)]\ng_comp_minor &lt;- g_comp_base + geom_col(data=read_comp_minor, position = \"stack\") +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, na_type, ctrl) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, na_type, ctrl) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(ctrl, classification, na_type, display) %&gt;%\n  pivot_wider(names_from=na_type, values_from = display) %&gt;% \n  arrange(ctrl, classification)\np_reads_summ\n\n\n  \n\n\n\nWhat we see is a very different picture from the wastewater samples I’ve been analyzing so far. Most notably, the fraction of (non-ribosomal) human reads is much higher. Even the 2015-01-20 samples, which were taken when the daycare center was closed for the winter holidays, showed human read fractions (for both nucleic-acid types) of &gt;12%; non-control samples as a whole averaged 27% for RNA reads and 34% for DNA reads. Compare Brumfield (average 0.08% for RNA and 0.02% for DNA), Yang (mean 0.05% for RNA) or even Rothman (mean 1.8% for non-panel-enriched RNA samples).\nConversely, total viral reads are very low: mean 0.033% for RNA reads and 0.019% for DNA reads. Wastewater RNA datasets have typically had much higher total viruses: mean 0.5% for Brumfield, about the same for Crits-Christoph, 5.5% for Yang, 4.5% for Rothman. Brumfield’s DNA data contained substantially fewer viruses than their RNA data, but still more than Prussin: about 0.08% on average.\nLooking at viral families…was less informative than usual, especially for DNA reads. It turns out that these samples contain a lot of viral reads that Kraken2 was only able to classify to the class level. In DNA reads, samples were dominated by Caudoviricetes phages, though Herviviricetes (which includes herpesviruses) and Naldaviricetes (a class of arthropod-infecting viruses) also put in a respectable showing. In RNA reads, Caudoviricetes was again a major presence, but Alsuviricetes (a family of primarily plant pathogens) was often as or more prevalent, and Revtraviricetes (a class that includes Hepatitis B virus and retroviruses) was also significant.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import Kraken reports & extract viral taxa\nsamples &lt;- as.character(basic_stats_raw$sample)\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\",\n               \"rank\", \"taxid\", \"name\")\nreport_paths &lt;- paste0(data_dir, \"kraken/\", samples, \".report.gz\")\nkraken_reports &lt;- lapply(1:length(samples), function(n) \n  read_tsv(report_paths[n], col_names = col_names, show_col_types = FALSE) %&gt;%\n    mutate(sample = samples[n])) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCode# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= 0.04)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, na_type, ctrl, date) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Set2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_classes\n\n\n\n\n\n\n\nOf these, the most interesting are the strong presence of Herviviricetes in the DNA reads and Revtraviricetes in the RNA reads, as both of these are families that contain important human pathogens.\nDigging into the former, it turns out these reads are composed almost exclusively of Herpesviridae at the family level. Within non-control samples, these arise overwhelmingly from Cytomegalovirus. Digging in at the species level, these in turn are primarily attributed to a single CMV, Human betaherpesvirus 5, a.k.a. human cytomegalovirus (HCMV). I was excited to see this: this is the first time a single human pathogen, or even all human pathogens combined, have constituted a significant fraction of all viral reads in a sample I’ve analyzed with this pipeline.\n\nCode# Get all read counts in class\nhervi_taxid &lt;- 2731363\nhervi_desc_taxids_old &lt;- hervi_taxid\nhervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(hervi_desc_taxids_new) &gt; length(hervi_desc_taxids_old)){\n  hervi_desc_taxids_old &lt;- hervi_desc_taxids_new\n  hervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\n}\nhervi_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% hervi_desc_taxids_new) %&gt;%\n  mutate(p_reads_hervi = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nhervi_genera &lt;- hervi_counts %&gt;% filter(rank == \"S\", na_type == \"DNA\")\nhervi_genera_major_tab &lt;- hervi_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_hervi_max = max(p_reads_hervi), .groups=\"drop\") %&gt;%\n  filter(p_reads_hervi_max &gt;= 0.04)\nhervi_genera_major_list &lt;- hervi_genera_major_tab %&gt;% pull(name)\nhervi_genera_major &lt;- hervi_genera %&gt;% \n  filter(name %in% hervi_genera_major_list) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_hervi)\nhervi_genera_minor &lt;- hervi_genera_major %&gt;% \n  group_by(sample, na_type, ctrl, date) %&gt;%\n  summarize(p_reads_hervi_major = sum(p_reads_hervi), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_hervi = 1-p_reads_hervi_major) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_hervi)\nhervi_genera_display &lt;- bind_rows(hervi_genera_major, hervi_genera_minor) %&gt;%\n  arrange(desc(p_reads_hervi)) %&gt;% \n  mutate(name = factor(name, levels=c(hervi_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_hervi, classification=name)\n\n# Plot\ng_hervi_genera &lt;- g_comp_base + \n  geom_col(data=hervi_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Herviviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\")\ng_hervi_genera\n\n\n\n\n\n\n\nRevtraviricetes reads are similarly dominated by a single viral genus, Alpharetrovirus. Digging in at the species level, we see contributions from a variety of avian oncoviruses. To my (admittedly non-expert) knowledge, none of these infect humans, and I think they are probably primarily arising from local birds (or possibly rats).\n\nCode# Get all read counts in class\nrevtra_taxid &lt;- 2732514\nrevtra_desc_taxids_old &lt;- revtra_taxid\nrevtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(revtra_desc_taxids_new) &gt; length(revtra_desc_taxids_old)){\n  revtra_desc_taxids_old &lt;- revtra_desc_taxids_new\n  revtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\n}\nrevtra_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% revtra_desc_taxids_new) %&gt;%\n  mutate(p_reads_revtra = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nrevtra_species &lt;- revtra_counts %&gt;% filter(rank == \"S\", na_type == \"RNA\")\nrevtra_species_major_tab &lt;- revtra_species %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_revtra_max = max(p_reads_revtra), .groups=\"drop\") %&gt;%\n  filter(p_reads_revtra_max &gt;= 0.04)\nrevtra_species_major_list &lt;- revtra_species_major_tab %&gt;% pull(name)\nrevtra_species_major &lt;- revtra_species %&gt;% \n  filter(name %in% revtra_species_major_list) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_revtra)\nrevtra_species_minor &lt;- revtra_species_major %&gt;% \n  group_by(sample, na_type, ctrl, date) %&gt;%\n  summarize(p_reads_revtra_major = sum(p_reads_revtra), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_revtra = 1-p_reads_revtra_major) %&gt;%\n  select(name, taxid, sample, na_type, ctrl, date, p_reads_revtra)\nrevtra_species_display &lt;- bind_rows(revtra_species_major, revtra_species_minor) %&gt;%\n  arrange(desc(p_reads_revtra)) %&gt;% \n  mutate(name = factor(name, levels=c(revtra_species_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_revtra, classification=name)\n\n# Plot\ng_revtra_species &lt;- g_comp_base + \n  geom_col(data=revtra_species_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Revtraviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\")\ng_revtra_species\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. Using the same workflow I used for Brumfield et al, I identified 2811 RNA read pairs and 12792 DNA read pairs as putatively human viral: 0.003% and 0.005% of surviving reads, respectively.\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(date, na_type)\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% group_by(sample, date, na_type, ctrl) %&gt;% count %&gt;% inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% select(sample, n_read_pairs), by=\"sample\") %&gt;% rename(n_putative = n, n_total = n_read_pairs) %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% group_by(na_type, ctrl) %&gt;% summarize(n_putative = sum(n_putative), n_total = sum(n_total), .groups=\"drop\") %&gt;% mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(na_type~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~kraken_label, scales=\"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\n\n\n\nAs previously described, I ran BLASTN on these reads via a dedicated EC2 instance, using the same parameters I’ve used for previous datasets.\n\nCodemrg_fasta &lt;-  mrg %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;%\n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"blast/putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\n# Pre-filtering BLAST results to save space\n# blast_results_path &lt;- file.path(data_dir, \"blast/putative-viral.blast.gz\")\n# blast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\n# blast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE,\n#                           col_names = blast_cols, col_types = cols(.default=\"c\"))\nblast_results_path &lt;- file.path(data_dir, \"blast/putative-viral-best.blast.gz\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE)\n\n# Filter for best hit for each query/subject combination\nblast_results_best &lt;- blast_results %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query and filter for high-ranking hits\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max, na_type)\nblast_results_assign &lt;- left_join(blast_results_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_results_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\nmrg_blast_rna &lt;- mrg_blast %&gt;% filter(na_type == \"RNA\")\nmrg_blast_dna &lt;- mrg_blast %&gt;% filter(na_type == \"DNA\")\n\n# Plot RNA\ng_mrg_blast_rna &lt;- mrg_blast_rna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"RNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_rna\n\n\n\n\n\n\nCode# Plot DNA\ng_mrg_blast_dna &lt;- mrg_blast_dna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"DNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_dna\n\n\n\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~viral_status_out, scales = \"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\n\n\n\nThese results look good on visual inspection, and indeed precision and sensitivity are both very high. For a disjunctive score threshold of 20, my updated workflow achieves an F1 score of 96.7% for RNA sequences and 98.2% for DNA sequences.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_rna &lt;- range_f1(mrg_blast_rna, inc_special) %&gt;% mutate(na_type = \"RNA\")\nstats_dna &lt;- range_f1(mrg_blast_dna, inc_special) %&gt;% mutate(na_type = \"DNA\")\nstats_0 &lt;- bind_rows(stats_rna, stats_dna) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt,na_type) %&gt;% \n  filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_grid(na_type~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\n\n\n\nLooking into the composition of different read groups, nothing stands out except the predominance of HCMV (human betaherpesvirus 5), which is consistent with the Kraken results above and borne out by the BLASTN alignments:\n\nCodeviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\n\nmajor_threshold &lt;- 0.05\nfp &lt;- mrg_blast %&gt;% \n  group_by(na_type, viral_status_out, \n           highscore = adj_score_max &gt;= 20, taxid) %&gt;% count %&gt;% \n  group_by(na_type, viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(na_type, viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(na_type~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\n\nCode# Configure\nref_taxid &lt;- 10359\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\nref_name &lt;- tax_names %&gt;% filter(staxid == ref_taxid) %&gt;% pull(name)\n\n# Get major matches\nfp_staxid &lt;- mrg_blast %&gt;% filter(taxid == ref_taxid) %&gt;%\n  group_by(na_type, highscore = adj_score_max &gt;= 20) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, na_type, \n           taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, na_type, taxid, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;0.1, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(na_type~status_display+score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name, \" (taxid \", ref_taxid, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.5), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, date, na_type, ctrl, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_total &lt;- read_counts %&gt;% group_by(na_type, ctrl) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\", date = \"All dates\")\nread_counts_agg &lt;- read_counts %&gt;% arrange(date) %&gt;%\n  arrange(sample) %&gt;%\n  bind_rows(read_counts_total) %&gt;% \n  mutate(sample = fct_inorder(sample),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n\nIn non-control samples, applying a disjunctive cutoff at S=20 identifies 2591 RNA reads and 12236 DNA reads as human-viral. This gives an overall relative HV abundance of \\(1.63 \\times 10^{-5}\\) for RNA reads and \\(4.16 \\times 10^{-5}\\) for DNA reads. Reassuringly, relative abundances in control samples are at least 10x lower. We also see a sharp drop in relative abundance for both nucleic-acid types for the period when the daycare was closed (2015-01-20):\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=date, color=na_type)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_grid(.~ctrl, scales = \"free\", space = \"free_x\") +\n  scale_color_na() + theme_rotate\ng_phv_agg\n\n\n\n\n\n\n\nThese overall RA values are similar to those we’ve seen previously for non-panel-enriched wastewater RNA data. That said, it’s notable that the DNA read RA seen here is much higher than that seen in the only DNA wastewater dataset I’ve analyzed so far (Brumfield):\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE)\n\n# Plot\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nAt the family level, we see that Papillomaviridae, Herpesviridae, and Polyomaviridae are the most abundant families in both DNA and RNA reads, with Adenoviridae and (in RNA reads) Picornaviridae also making a respectable showing. The Herpesviridae reads are, predictably, overwhelmingly from HCMV, while the Papillomaviridae and Polyomaviridae reads are split up among a larger number of related viruses:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\n\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\n\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.06\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, date, ctrl, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, ctrl, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, ctrl, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\")\ng_hv_family\n\n\n\n\n\n\n\n\nCodethreshold_major_genus &lt;- 0.06\n\n# Count reads for each human-viral genus\nhv_genus_counts &lt;- hv_reads_genus %&gt;% \n  group_by(sample, date, ctrl, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, ctrl, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_genus_major_tab &lt;- hv_genus_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_genus)\nhv_genus_counts_major &lt;- hv_genus_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_genus_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, ctrl, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_genus_major_tab$name, \"Other\")))\nhv_genus_counts_display &lt;- hv_genus_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_genus &lt;- g_comp_base + \n  geom_col(data=hv_genus_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\")\ng_hv_genus\n\n\n\n\n\n\n\n\nCodethreshold_major_species &lt;- 0.10\n\n# Count reads for each human-viral species\nhv_species_counts &lt;- hv_reads_species %&gt;% \n  group_by(sample, date, ctrl, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, ctrl, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_species_major_tab &lt;- hv_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_species)\nhv_species_counts_major &lt;- hv_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_species_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, ctrl, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_species_major_tab$name, \"Other\")))\nhv_species_counts_display &lt;- hv_species_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_species &lt;- g_comp_base + \n  geom_col(data=hv_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\")\ng_hv_species\n\n\n\n\n\n\n\nCompared to the previous datasets I’ve analyzed, the most notable difference is the absence of enteric viruses: Norovirus, Rotavirus, and Enterovirus are all absent from the list of abundant human-viral taxa, as are ~all other gastrointestinal pathogens.\nFinally, here are the overall relative abundances of a set of specific viral genera picked out manually on the basis of scientific or medical interest. In the future, I’ll quantify the RA of these genera across all datasets analyzed with this pipeline to date, which should give us a better sense of how these data compare to others’ under this pipeline.\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\",\n                 \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nn_path_genera &lt;- hv_reads_genus %&gt;% \n  group_by(sample, date, na_type, ctrl, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"date\", \"na_type\", \"ctrl\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(na_type, ctrl,\n           name, taxid, genome_type) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", date=\"All dates\",\n         p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=na_type)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_na() +\n  facet_grid(genome_type~ctrl, scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\nConclusion\nThis is the first air-sampling dataset I’ve analyzed using this pipeline, and it was interesting to see the differences from the wastewater datasets I’ve been analyzing so far. Among the more striking differences were:\n\nA much higher total fraction of human reads;\nA lower total fraction of viral reads;\nNear-total absence of enteric viruses and Tobamovirus;\nMuch higher relative abundance of herpesviruses, particularly HCMV, and papillomaviruses among human-infecting virus reads.\n\nConversely, one thing that was not notably different, at least in RNA viruses, was the total relative abundance of human-infecting viruses as a whole. Given the lower fraction of reads made up of all viruses, this means that the fraction of total viruses arising from human-infecting viruses is much higher here than we’ve historically seen with wastewater data. In particular, HCMV represents a nontrivial fraction of total viruses for many DNA libraries, the first time I’ve seen a human pathogen show up significantly in the total virus composition data.\nGoing forward, I have two more air sampling datasets to analyze, Rosario et al. 2018 and Leung et al. 2021. It will be interesting to see whether the findings from this dataset generalize to other air sampling contexts.\nAddendum: Checking HIV\nJeff from my team asks:\n\nDo you think the HIV reads are real? The v1 pipeline shows 39 HIV reads in that dataset\n\nMy pipeline finds 62 Lentivirus reads for this dataset, 34 from HIV-1 and 28 from HIV-2. Oddly, and perhaps suspiciously, these nearly all come from DNA libraries:\n\nCodelenti_read_ids &lt;- hv_reads_genus %&gt;% filter(name == \"Lentivirus\") %&gt;% pull(seq_id)\nlenti_reads &lt;- hv_reads_species %&gt;% filter(seq_id %in% lenti_read_ids)\nlenti_read_count &lt;- lenti_reads %&gt;% group_by(name, na_type) %&gt;% count(name=\"n_reads\")\nlenti_read_count\n\n\n  \n\n\n\nHere’s what I get when I look at the subject taxa that BLASTN maps these reads to:\n\nCode# Configure\nref_taxid &lt;- 11646\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         257877, \"Macaca thibetana thibetana\",\n                         256321, \"Lentiviral transfer vector pHsCXW\",\n                         419242, \"Shuttle vector pLvCmvMYOCDHA\",\n                         419243, \"Shuttle vector pLvCmvLacZ\",\n                         421868, \"Cloning vector pLvCmvLacZ.Gfp\",\n                         421869, \"Cloning vector pLvCmvMyocardin.Gfp\",\n                         426303, \"Lentiviral vector pNL-GFP-RRE(SA)\",\n                         436015, \"Lentiviral transfer vector pFTMGW\",\n                         454257, \"Shuttle vector pLvCmvMYOCD2aHA\",\n                         476184, \"Shuttle vector pLV.mMyoD::ERT2.eGFP\",\n                         476185, \"Shuttle vector pLV.hMyoD.eGFP\",\n                         591936, \"Piliocolobus tephrosceles\",\n                         627481, \"Lentiviral transfer vector pFTM3GW\",\n                         680261, \"Self-inactivating lentivirus vector pLV.C-EF1a.cyt-bGal.dCpG\",\n                         2952778, \"Expression vector pLV[Exp]-EGFP:T2A:Puro-EF1A\",\n                         3022699, \"Vector PAS_122122\",\n                         3025913, \"Vector pSIN-WP-mPGK-GDNF\",\n                         3105863, \"Vector pLKO.1-ZsGreen1\",\n                         3105864, \"Vector pLKO.1-ZsGreen1 mouse Wfs1 shRNA\",\n                         3108001, \"Cloning vector pLVSIN-CMV_Neo_v4.0\",\n                         3109234, \"Vector pTwist+Kan+High\",\n                         3117662, \"Cloning vector pLV[Exp]-CBA&gt;P301L\",\n                         3117663, \"Cloning vector pLV[Exp]-CBA&gt;P301L:T2A:mRuby3\",\n                         3117664, \"Cloning vector pLV[Exp]-CBA&gt;hMAPT[NM_005910.6](ns):T2A:mRuby3\",\n                         3117665, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3\",\n                         3117666, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3/NFAT3 fusion protein\",\n                         3117667, \"Cloning vector pLV[Exp]-Neo-mPGK&gt;{EGFP-hSEPT6}\"\n\n)\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\nref_name &lt;- tax_names %&gt;% filter(staxid == ref_taxid) %&gt;% pull(name)\n\n# Get major matches\nfp_staxid &lt;- hv_reads_genus %&gt;% filter(taxid == ref_taxid) %&gt;%\n  group_by(na_type, highscore = adj_score_max &gt;= 20) %&gt;% mutate(n_seq = n()) %&gt;%\n  select(-name) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(na_type, taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(na_type, taxid, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;0.1, !is.na(staxid)) %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(sname = fct_inorder(sname)) %&gt;%\n  group_by(na_type) %&gt;% filter(row_number() &lt;= 20)\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(na_type~., scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name, \" (taxid \", ref_taxid, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.5), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nThe four RNA reads seem very likely not to be true HIV reads, since they map primarily to humans and other primates. The 58 DNA reads are more ambiguous; they do map to HIV, but also to a range of synthetic cloning vectors. On the other hand, I just now took a random slice from the HIV-1 genome and BLASTed it online, and it also mapped to cloning vectors; it’s just the case that a lot of people use lentiviruses as the backbones for their cloning vectors.\nThis makes disambiguating whether or not these reads are “real” a bit tricky. The fact that they’re DNA rather than RNA makes me suspicious, as does the fact that so few of the reads apparently come from regions of the HIV genome not included in cloning vectors. On balance, I think these are probably not real HIV reads! But it’s hard to be highly confident one way or the other.\nIf we decide to treat these reads as artifacts arising from contamination with cloning vectors, what do we do about it? Just removing any read that matches a lentiviral cloning vector seems like a risky strategy when these vectors share so much sequence with real viruses. For now, I’ll leave my contamination database as it is and continue checking HIV reads manually. But I’ll need to develop a better solution to this sooner or later."
  },
  {
    "objectID": "notebooks/2024-04-12_rosario.html",
    "href": "notebooks/2024-04-12_rosario.html",
    "title": "Workflow analysis of Rosario et al. (2018)",
    "section": "",
    "text": "Continuing our look at air sampling datasets, we turn to Rosario et al. (2018), another study of air filters, this time from HVAC filters from an undergraduate dorm building at the University of Colorado campus in Boulder. As in Prussin, samples were eluted from filters (in this case MERV-8, so less stringent than Prussin’s MERV-14 filters) and underwent both RNA and DNA sequencing – this time on an Illumina MiSeq with 2x250bp reads.\nThe raw data\nThe Rosario dataset comprised sequencing data from 12 individual dormitory rooms at the sampling site, as well as a pooled sample of eight additional rooms and a negative control.\n\nCode# Data input paths\ndata_dir &lt;- \"../data/2024-04-12_rosario/\"\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- read_csv(libraries_path, show_col_types = FALSE)\nlibraries &lt;- libraries_raw %&gt;%\n  arrange(desc(na_type)) %&gt;% mutate(na_type = fct_inorder(na_type)) %&gt;%\n  mutate(sample_type = ifelse(grepl(\"Control\", room), \"NC\",\n                              ifelse(grepl(\"Pool\", room), \"Pool\", \"Single-Room\")),\n         sample_type = factor(sample_type, levels=c(\"Single-Room\", \"Pool\", \"NC\")),\n         room = ifelse(grepl(\"Control\", room), \"Negative Control\", room))\n\n# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  mutate(stage = factor(stage, levels = stages),\n         sample = fct_inorder(sample))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;%\n    mutate(sample = sub(\"_2$\", \"\", sample)) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(stage = factor(stage, levels = stages),\n         read_pair = fct_inorder(as.character(read_pair)))\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% group_by(na_type, sample_type) %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), .groups = \"drop\")\nraw_read_totals &lt;- basic_stats_raw %&gt;% group_by(na_type, sample_type) %&gt;% \n  summarize(n_read_pairs = sum(n_read_pairs), \n            n_bases_approx = sum(n_bases_approx), .groups = \"drop\")\nraw_dup &lt;- basic_stats_raw %&gt;% group_by(na_type, sample_type) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThe 12 single-room samples yielded 0.57-0.74M (mean 0.67M) RNA-sequencing reads and 0.48M-0.84M (mean 0.67M) DNA-sequencing reads; the pooled and control samples had similar depth. In total, non-control samples yielded 8.0M RNA read pairs and 8.0M DNA read pairs (4 gigabases of sequence each).\nRead qualities were so-so, in need of cleaning. Adapter levels were high. Inferred duplication levels were low (4.6-8.2%) in DNA reads but highly variable (3-83%) in RNA reads.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(room, na_type, sample_type,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(room:sample_type), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_na &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", \n                                name=\"Nucleic acid type\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=room, y=value, fill=na_type)) +\n  geom_col(position = \"dodge\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_na() + \n  facet_grid(metric~sample_type, scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_rotate + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(sample_type~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(sample_type~.)\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(sample_type~., scales = \"free_y\")\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. Single-room samples lost an average of 52%/14% (RNA/DNA) of total read pairs during cleaning and deduplication, with a further loss of 4.3%/0.1% during ribodepletion. However, the fraction of reads lost, especially during cleaning and deduplication, was highly variable between samples.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, na_type, sample_type, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(sample, na_type, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage, `NA Type`=na_type, `Sample Type`=sample_type) %&gt;% \n  group_by(`Sample Type`, `NA Type`, Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=na_type, group=sample)) +\n  scale_color_na() +\n  facet_wrap(sample_type~na_type, scales=\"free\", ncol=2,\n             labeller = label_wrap_gen(multi_line=FALSE)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=na_type, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_na() +\n  facet_wrap(sample_type~na_type, scales=\"free\", ncol=2,\n             labeller = label_wrap_gen(multi_line=FALSE)) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at improving read qualities, as well as removing most adapter sequences. The exception to the latter was terminal poly-A sequences, which remained at a substantial (though reduced) prevalence throughout the pipeline. (This doesn’t seem to have caused downstream problems that I can see.)\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=na_type, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_na() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, deduplication was very effective at reducing measured duplicate levels, which for single-room samples fell from an average of 40% to 7.8% for RNA reads and from 5.5% to 0.3% for DNA reads.\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(na_type, sample_type, stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_path &lt;- file.path(data_dir, \"bracken_counts.tsv\")\nbracken &lt;- read_tsv(bracken_path, show_col_types = FALSE)\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", \n              values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, room, na_type, sample_type, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"room\", \"na_type\", \"sample_type\"),\n              names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample=sample, na_type=na_type,\n                       room=room, sample_type=sample_type,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:sample_type), \n                               names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\n\n# Summarize composition\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(na_type, sample_type, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=room, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_grid(na_type~sample_type, scales = \"free\", space = \"free_x\") +\n  theme_rotate\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = read_comp_long, position = \"stack\") +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[c(6,7,9)]\ng_comp_minor &lt;- g_comp_base + geom_col(data=read_comp_minor, position = \"stack\") +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, na_type, sample_type) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, na_type, sample_type) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(sample_type, classification, na_type, display) %&gt;%\n  pivot_wider(names_from=na_type, values_from = display) %&gt;% \n  arrange(sample_type, classification)\np_reads_summ\n\n\n  \n\n\n\nAs with the Prussin data, human reads are very elevated compared to wastewater, making up an average of 21% of read pairs in RNA libraries and 4% for DNA libraries for single-room samples (in both cases, the maximum abundance of human reads across samples is much higher). Viral read abundances are higher than Prussin, averaging 0.08% for single-room RNA libraries and 0.04% for DNA libraries; for some reason, the pooled RNA library shows much higher viral prevalence, at 0.33%. The most obvious difference from the Prussin data is that DNA libraries in Rosario show a much higher fraction of non-ribosomal unassigned reads, with an average of 78% of all reads for single-room libraries.\nAs in Prussin, viral DNA reads were dominated by Caudoviricetes phages, with many unclassifiable below the class level. Other prominent viral classes in DNA reads included Papovaviricetes (which includes Polyomaviridae and Papillomaviridae), Megaviridae (giant viruses) and Repensiviricetes (plant and fungal viruses). Pooled samples also showed a significant level of Quintoviricetes (parvoviruses).\nRNA libraries, meanwhile, were dominated by Alsuviricetes (mainly plant viruses) and, notably, Pisoniviricetes (which includes coronaviruses, picornaviruses including Enterovirus, and caliciviruses including Norovirus). Caudoviricetes, Herviviricetes (herpesviruses) and Revtraviricetes (retroviruses + Hep B) were also prominent in some samples.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Import Kraken reports & extract viral taxa\nsamples &lt;- as.character(basic_stats_raw$sample)\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\",\n               \"rank\", \"taxid\", \"name\")\nreport_paths &lt;- paste0(data_dir, \"kraken/\", samples, \".report.gz\")\nkraken_reports &lt;- lapply(1:length(samples), function(n) \n  read_tsv(report_paths[n], col_names = col_names, show_col_types = FALSE) %&gt;%\n    mutate(sample = samples[n])) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCode# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= 0.04)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, room, na_type, sample_type) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_classes\n\n\n\n\n\n\n\nOf these, the strong presence of Papovaviricetes and Pisoniviricetes across multiple samples is the most interesting, as both contain important human pathogens. The presence of Quintoviricetes, Herviviricetes and Revtraviricetes in a smaller number of samples is also notable.\nIn DNA reads, Papovaviricetes are quite heterogeneous across samples, while mostly being dominated by one or two genera within samples. The three rooms with the largest Papovaviricetes fraction (Female room 1 & 3 and Male room 4) are each dominated by a single genus, specifically Dyothetapapillomavirus (of which the sole member is Dyothetapapillomavirus 1, a cat pathogen), Betapapillomavirus (a genus of human wart-causing viruses), and Alphapolyomavirus (specifically Merkel cell polyomavirus).\n\nCode# Get all read counts in class\npapova_taxid &lt;- 2732421\npapova_desc_taxids_old &lt;- papova_taxid\npapova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(papova_desc_taxids_new) &gt; length(papova_desc_taxids_old)){\n  papova_desc_taxids_old &lt;- papova_desc_taxids_new\n  papova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\n}\npapova_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% papova_desc_taxids_new) %&gt;%\n  mutate(p_reads_papova = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\npapova_genera &lt;- papova_counts %&gt;% filter(rank == \"G\", na_type == \"DNA\")\npapova_genera_major_tab &lt;- papova_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_papova_max = max(p_reads_papova), .groups=\"drop\") %&gt;%\n  filter(p_reads_papova_max &gt;= 0.04)\npapova_genera_major_list &lt;- papova_genera_major_tab %&gt;% pull(name)\npapova_genera_major &lt;- papova_genera %&gt;% \n  filter(name %in% papova_genera_major_list) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_papova)\npapova_genera_minor &lt;- papova_genera_major %&gt;% \n  group_by(sample, room, na_type, sample_type) %&gt;%\n  summarize(p_reads_papova_major = sum(p_reads_papova), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_papova = 1-p_reads_papova_major) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_papova)\npapova_genera_display &lt;- bind_rows(papova_genera_major, papova_genera_minor) %&gt;%\n  arrange(desc(p_reads_papova)) %&gt;% \n  mutate(name = factor(name, levels=c(papova_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_papova, classification=name)\n\n# Plot\ng_papova_genera &lt;- g_comp_base + \n  geom_col(data=papova_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papovaviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_papova_genera\n\n\n\n\n\n\n\nIn RNA reads, Pisoniviricetes are conversely dominated by a single genus, Sobemovirus, across all samples, and specifically a single species. Disappointingly, this is Ryegrass mottle virus, a plant pathogen.\n\nCode# Get all read counts in class\npisoni_taxid &lt;- 2732506\npisoni_desc_taxids_old &lt;- pisoni_taxid\npisoni_desc_taxids_new &lt;- unique(c(pisoni_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% pisoni_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(pisoni_desc_taxids_new) &gt; length(pisoni_desc_taxids_old)){\n  pisoni_desc_taxids_old &lt;- pisoni_desc_taxids_new\n  pisoni_desc_taxids_new &lt;- unique(c(pisoni_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% pisoni_desc_taxids_old) %&gt;% pull(taxid)))\n}\npisoni_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% pisoni_desc_taxids_new) %&gt;%\n  mutate(p_reads_pisoni = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\npisoni_genera &lt;- pisoni_counts %&gt;% filter(rank == \"S\", na_type == \"RNA\")\npisoni_genera_major_tab &lt;- pisoni_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_pisoni_max = max(p_reads_pisoni), .groups=\"drop\") %&gt;%\n  filter(p_reads_pisoni_max &gt;= 0.04)\npisoni_genera_major_list &lt;- pisoni_genera_major_tab %&gt;% pull(name)\npisoni_genera_major &lt;- pisoni_genera %&gt;% \n  filter(name %in% pisoni_genera_major_list) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_pisoni)\npisoni_genera_minor &lt;- pisoni_genera_major %&gt;% \n  group_by(sample, room, na_type, sample_type) %&gt;%\n  summarize(p_reads_pisoni_major = sum(p_reads_pisoni), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_pisoni = 1-p_reads_pisoni_major) %&gt;%\n  select(name, taxid, sample, room, na_type, sample_type, p_reads_pisoni)\npisoni_genera_display &lt;- bind_rows(pisoni_genera_major, pisoni_genera_minor) %&gt;%\n  arrange(desc(p_reads_pisoni)) %&gt;% \n  mutate(name = factor(name, levels=c(pisoni_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_pisoni, classification=name)\n\n# Plot\ng_pisoni_genera &lt;- g_comp_base + \n  geom_col(data=pisoni_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Pisoniviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_pisoni_genera\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. Using the same workflow I used for Prussin et al, I identified 118 RNA read pairs and 1269 DNA read pairs as putatively human viral: 0.004% and 0.017% of surviving reads, respectively.\n\nCodehv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- read_tsv(hv_reads_filtered_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% \n  arrange(na_type, sample_type)\nn_hv_filtered &lt;- hv_reads_filtered %&gt;% \n  group_by(sample, room, na_type, sample_type) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% group_by(na_type) %&gt;% \n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCodemrg &lt;- hv_reads_filtered %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  group_by(sample, na_type) %&gt;% arrange(desc(adj_score_fwd), desc(adj_score_rev)) %&gt;%\n  mutate(seq_num = row_number(),\n         adj_score_max = pmax(adj_score_fwd, adj_score_rev))\n# Import Bowtie2/Kraken data and perform filtering steps\ng_mrg &lt;- ggplot(mrg, aes(x=adj_score_fwd, y=adj_score_rev)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  facet_grid(na_type~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\n\n\nCodeg_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~kraken_label, scales=\"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_0\n\n\n\n\n\n\n\nAs previously described, I ran BLASTN on these reads via a dedicated EC2 instance, using the same parameters I’ve used for previous datasets.\n\nCodemrg_fasta &lt;-  mrg %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;%\n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta, sep=\"\\n\")) %&gt;% paste(collapse=\"\\n\")\nwrite(mrg_fasta_out, file.path(data_dir, \"blast/putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\nblast_results_path &lt;- file.path(data_dir, \"blast/putative-viral.blast.gz\")\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE,\n                          col_names = blast_cols, col_types = cols(.default=\"c\"))\n# blast_results_path &lt;- file.path(data_dir, \"blast/putative-viral-best.blast.gz\")\n# blast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE)\n\n# Filter for best hit for each query/subject combination\nblast_results_best &lt;- blast_results %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n\n# Rank hits for each query and filter for high-ranking hits\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max, na_type)\nblast_results_assign &lt;- left_join(blast_results_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = (staxid == taxid),\n           taxid_match_kraken = (staxid == assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_results_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\nmrg_blast_rna &lt;- mrg_blast %&gt;% filter(na_type == \"RNA\")\nmrg_blast_dna &lt;- mrg_blast %&gt;% filter(na_type == \"DNA\")\n\n# Plot RNA\ng_mrg_blast_rna &lt;- mrg_blast_rna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"RNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_rna\n\n\n\n\n\n\nCode# Plot DNA\ng_mrg_blast_dna &lt;- mrg_blast_dna %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=viral_status_out)) +\n  geom_point(alpha=0.5, shape=16) +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,10), expand = c(0,0)) +\n  scale_color_brewer(palette = \"Set1\", name = \"Viral status\") +\n  facet_grid(viral_status_out~kraken_label, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + labs(title=\"DNA\") + \n  theme(aspect.ratio=1, plot.title = element_text(size=rel(2), hjust=0))\ng_mrg_blast_dna\n\n\n\n\n\n\n\n\nCodeg_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max)) + geom_histogram(binwidth=5,boundary=0,position=\"dodge\") + facet_grid(na_type~viral_status_out, scales = \"free_y\") + scale_x_continuous(name = \"Maximum adjusted alignment score\") + scale_y_continuous(name=\"# Read pairs\") + scale_fill_brewer(palette = \"Dark2\") + theme_base + geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\")\ng_hist_1\n\n\n\n\n\n\n\nThese results look good on visual inspection, and indeed precision and sensitivity are both very high. For a disjunctive score threshold of 20, my updated workflow achieves an F1 score of 98.2% for RNA sequences and 97.9% for DNA sequences.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, viral_status_out %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inc_special, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab, include_special=inc_special)\n  stats_conj &lt;- lapply(inrange, tss, conjunctive=TRUE) %&gt;% bind_rows\n  stats_disj &lt;- lapply(inrange, tss, conjunctive=FALSE) %&gt;% bind_rows\n  stats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n    pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n    mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\n  return(stats_all)\n}\ninc_special &lt;- FALSE\nstats_rna &lt;- range_f1(mrg_blast_rna, inc_special) %&gt;% mutate(na_type = \"RNA\")\nstats_dna &lt;- range_f1(mrg_blast_dna, inc_special) %&gt;% mutate(na_type = \"DNA\")\nstats_0 &lt;- bind_rows(stats_rna, stats_dna) %&gt;% mutate(attempt=0)\nthreshold_opt_0 &lt;- stats_0 %&gt;% group_by(conj_label,attempt,na_type) %&gt;% \n  filter(metric == \"f1\") %&gt;%\n  filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(data = threshold_opt_0, mapping = aes(xintercept=threshold),\n             color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Set3\") +\n  facet_grid(na_type~conj_label) +\n  theme_base\ng_stats_0\n\n\n\n\n\n\n\nLooking into the composition of different read groups, the most notable observations for me are the predominance of (1) HIV reads among low-scoring RNA “true-positives”, and (2) human mastadenovirus C among RNA true-positives in general:\n\nCodeviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\nviral_taxa$name[viral_taxa$taxid == 493803] &lt;- \"Merkel cell polyomavirus\"\nviral_taxa$name[viral_taxa$taxid == 427343] &lt;- \"Human papillomavirus 107\"\n\nmajor_threshold &lt;- 0.1\nfp &lt;- mrg_blast %&gt;% \n  group_by(na_type, viral_status_out, \n           highscore = adj_score_max &gt;= 20, taxid) %&gt;% count %&gt;% \n  group_by(na_type, viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p)) %&gt;%\n  mutate(name = ifelse(taxid == 194958, \"Porcine endogenous retrovirus A\", name))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(na_type, viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(na_type~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\nThere are only eight low-scoring true-positive RNA reads, three of which are HIV. These do seem to be real; at least, they don’t BLAST to anything else. Ironically the eight high-scoring “true-positive” HIV reads seem more likely to be fake; they also map to a range of cloning vectors:\n\nCode# Configure\nref_taxid_hiv &lt;- 11676\np_threshold &lt;- 0.3\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\nref_name_hiv &lt;- tax_names %&gt;% filter(staxid == ref_taxid_hiv) %&gt;% pull(name)\n\n# Get major matches\nfp_staxid &lt;- mrg_blast %&gt;% filter(taxid == ref_taxid_hiv) %&gt;%\n  group_by(na_type, highscore = adj_score_max &gt;= 20) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, na_type, \n           taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, na_type, taxid, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(na_type~status_display+score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_hiv, \" (taxid \", ref_taxid_hiv, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nA similar pattern is seen for the Human mastadenovirus C hits: low-scoring true-positives only map to adenoviruses, but high-scoring ones also map to a range of cloning vectors.\n\nCode# Configure\nref_taxid_masta &lt;- 129951\np_threshold &lt;- 0.1\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         3033760, \"Human adenovirus 89\",\n                         3043599, \"Human adenovirus 108\",\n                         3088344, \"Human adenovirus C108\",\n                         3102992, \"Cloning vector pBWH-C5\",\n                         3102994, \"Cloning vector pBWH-C5-pIX-Kan\",\n                         3102995, \"Cloning vector pBWH-C5-pIXZG\"\n                         )\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\nref_name_masta &lt;- tax_names %&gt;% filter(staxid == ref_taxid_masta) %&gt;% pull(name)\n\n# Get major matches\nfp_staxid &lt;- mrg_blast %&gt;% filter(taxid == ref_taxid_masta) %&gt;%\n  group_by(na_type, highscore = adj_score_max &gt;= 20) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, na_type, \n           taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, na_type, taxid, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(na_type~status_display+score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_masta, \" (taxid \", ref_taxid_masta, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.5), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nThe plots above only tell us the total number of reads that BLAST to each cloning vector; they don’t tell us how many reads map to any vector. In particular, they can’t distinguish between a situation where many reads map to a few cloning vectors, and one where a few reads map to many. This turns out to be about 35% of high-scoring HIV RNA reads and 55% of high-scoring mastadenovirus RNA reads:\n\nCodefp_hiv_vec &lt;- mrg_blast %&gt;% filter(taxid == ref_taxid_hiv) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\")  %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\") %&gt;%\n  group_by(vector = grepl(\"vector\", sname))\nfp_hiv_vec_match &lt;- fp_hiv_vec %&gt;%\n  group_by(viral_status_out, seq_id, na_type, highscore = adj_score_max &gt;= 20) %&gt;%\n  summarize(vector_match = any(vector), .groups = \"drop\")\nfp_hiv_vec_count &lt;- fp_hiv_vec_match %&gt;%\n  group_by(viral_status_out, na_type, highscore, vector_match) %&gt;% count %&gt;%\n  group_by(viral_status_out, na_type, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out,\n                                 \"True positive\", \"False positive\"))\n\n\ng_hiv_vec_count &lt;- ggplot(fp_hiv_vec_count, aes(y=score_display, x=p, fill=vector_match)) +\n  geom_col() +\n  scale_x_continuous(name=\"% Mapped Reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  scale_fill_brewer(name=\"Maps to vector?\", palette = \"Dark2\") +\n  facet_grid(na_type~status_display, scales=\"free\") +\n  labs(title=paste0(ref_name_hiv, \" (taxid \", ref_taxid_hiv, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hiv_vec_count\n\n\n\n\n\n\n\n\nCodefp_masta_vec &lt;- mrg_blast %&gt;% filter(taxid == ref_taxid_masta) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\")  %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid=staxid), by=\"taxid\") %&gt;%\n  group_by(vector = grepl(\"vector\", sname))\nfp_masta_vec_match &lt;- fp_masta_vec %&gt;%\n  group_by(viral_status_out, seq_id, na_type, highscore = adj_score_max &gt;= 20) %&gt;%\n  summarize(vector_match = any(vector), .groups = \"drop\")\nfp_masta_vec_count &lt;- fp_masta_vec_match %&gt;%\n  group_by(viral_status_out, na_type, highscore, vector_match) %&gt;% count %&gt;%\n  group_by(viral_status_out, na_type, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out,\n                                 \"True positive\", \"False positive\"))\n\n\ng_masta_vec_count &lt;- ggplot(fp_masta_vec_count, aes(y=score_display, x=p, fill=vector_match)) +\n  geom_col() +\n  scale_x_continuous(name=\"% Mapped Reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  scale_fill_brewer(name=\"Maps to vector?\", palette = \"Dark2\") +\n  facet_grid(na_type~status_display, scales=\"free\") +\n  labs(title=paste0(ref_name_masta, \" (taxid \", ref_taxid_masta, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_masta_vec_count\n\n\n\n\n\n\n\nThis is more concerning for the mastadenovirus reads, which make up a nontrivial fraction of all high-scoring RNA “true-positives”. As in my last post, though, it’s hard to know what to take from this; many vectors are based on lentiviral or adenoviral backbones, and it’s not always easy to distinguish between true virus and virus-based vector. I’m going to stick with my pipeline as it is for now, but it’s worth keeping this in mind in interpreting the results.\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, room, na_type, sample_type, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | hit_hv | adj_score_max &gt;= 20)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_total &lt;- read_counts %&gt;% group_by(na_type, sample_type) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\", room = \"All rooms\")\nread_counts_agg &lt;- read_counts %&gt;% arrange(sample) %&gt;%\n  bind_rows(read_counts_total) %&gt;% \n  mutate(sample = fct_inorder(sample),\n         p_reads_hv = n_reads_hv/n_reads_raw)\n\n\nIn non-control samples, applying a disjunctive cutoff at S=20 identifies 97 RNA reads and 1204 DNA reads as human-viral. This gives an overall relative HV abundance of \\(1.21 \\times 10^{-5}\\) for RNA reads and \\(1.50 \\times 10^{-4}\\) for DNA reads. For DNA reads, HV RA in the negative controls is about 10x lower than the non-control samples; however, for RNA reads, the HV RA in the negative controls is actually higher than in the non-controls. This, combined with the very low absolute read count, suggests that we shouldn’t take the HV results for RNA reads too seriously.\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=room, color=na_type)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_grid(.~sample_type, scales = \"free\", space = \"free_x\") +\n  scale_color_na() + theme_rotate\ng_phv_agg\n\n\n\n\n\n\n\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                  \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE)\n\n\n# Plot\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nI’m going to focus on the DNA reads here, since as discussed above I expect the RNA reads to be noisy and not very informative – which seems to in fact be the case.\nAt the family level, as in Prussin, we see that Papillomaviridae, and Polyomaviridae dominate DNA reads in most samples, with Poxviridae and Genomoviridae showing significant presence in several samples. The first of these is primarily Betapapillomavirus and Gammapapillomavirus, with some Alphapapillomavirus; the second is primarily Alphapolyomavirus and Deltapolyomavirus. All of these are made up of several different viruses at the species level. Genomoviridae is represented primarily by Gemykibivirus, while Poxviridae is represented by small numbers of reads from several species, including vaccinia, cowpox and tanapox. (There’s also one variola read, which would be alarming, but BLASTN thinks it isn’t real.)\nCompared to Prussin, the most striking difference is the absence of herpesvirus reads; whereas HCMV was dominant in the Prussin data, it barely shows up here except in the controls.\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\n\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.06\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, room, sample_type, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, room, sample_type, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, room, sample_type, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\")\ng_hv_family\n\n\n\n\n\n\n\n\nCodethreshold_major_genus &lt;- 0.05\n\n# Count reads for each human-viral genus\nhv_genus_counts &lt;- hv_reads_genus %&gt;% filter(na_type == \"DNA\") %&gt;%\n  group_by(sample, room, sample_type, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, room, sample_type, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_genus_major_tab &lt;- hv_genus_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_genus)\nhv_genus_counts_major &lt;- hv_genus_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_genus_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, room, sample_type, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_genus_major_tab$name, \"Other\")))\nhv_genus_counts_display &lt;- hv_genus_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_genus &lt;- g_comp_base + \n  geom_col(data=hv_genus_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\")\ng_hv_genus\n\n\n\n\n\n\n\n\nCodethreshold_major_species &lt;- 0.1\n\n# Count reads for each human-viral species\nhv_species_counts &lt;- hv_reads_species %&gt;% filter(na_type == \"DNA\") %&gt;%\n  group_by(sample, room, sample_type, na_type, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, room, sample_type, na_type) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_species_major_tab &lt;- hv_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_species)\nhv_species_counts_major &lt;- hv_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_species_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, room, sample_type, na_type, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_species_major_tab$name, \"Other\")))\nhv_species_counts_display &lt;- hv_species_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_species &lt;- g_comp_base + \n  geom_col(data=hv_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  guides(fill=guide_legend(ncol=3))\ng_hv_species\n\n\n\n\n\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nn_path_genera &lt;- hv_reads_genus %&gt;% \n  group_by(sample, room, na_type, sample_type, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"room\", \"na_type\", \"sample_type\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(na_type, sample_type,\n           name, taxid, genome_type) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", date=\"All dates\",\n         p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=na_type)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_na() +\n  facet_grid(genome_type~sample_type, scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\nOne immediate observation, which I think helps demonstrate the unreliability of the RNA data here, is the almost complete absence of RNA-virus genera of interest. It’s very suspicious for putative HIV reads to be almost as abundant as Enterovirus reads (which, remember, includes rhinovirus), while coronaviruses and influenza are totally absent. The DNA reads seem much more similar to Prussin, though I’m still surprised HCMV and Mastadenovirus are so rare.\nConclusion\nThis is the second, and by far the smaller, of the air-sampling datasets I’ve analyzed with this pipeline so far. Many of the high-level findings were similar to Prussin, including high relative abundance of human reads, low total viral reads, an absence of enteric viruses, and high abundance of papillomaviruses among human-infecting viruses. The biggest difference in the DNA data was the almost complete lack of cytomegalovirus, compared to its very high abundance in the Prussin data.\nGiven how small this dataset is (16M reads on a MiSeq, compared to ~450M for Prussin and 1.3B for the upcoming Leung et al. dataset), I don’t think too much weight should be placed on these findings, but given how rare air-MGS datasets are it’s nice to have another one."
  },
  {
    "objectID": "notebooks/2024-04-12_leung.html",
    "href": "notebooks/2024-04-12_leung.html",
    "title": "Workflow analysis of Leung et al. (2021)",
    "section": "",
    "text": "The last in our current run of air sampling datasets is Leung et al. (2021), a study of active air samples collected in public transit systems from six cities (Denver, Hong Kong, London, NYC, Oslo, Stockholm) from June to September 2017.\nSamples from Denver originated from their rail and bus system; all other samples originated from metro systems. Collection took place during working days and working hours. Air samples were collected with the SASS 3100 Dry Air Samplers (filtration) for 30 min at a flowrate of 300 L/min using electret microfibrous filters. Filters were stationed at 1.5m above floor level, facing downward (to avoid direct deposition).\nThis was a DNA-sequencing study, focused on the bacterial microbiome and resistome. Sample processing followed an ideosyncratic protocol, where samples were pelleted and the pellet and supernatant were processed separately before being recombined for NA extraction and sequencing; I don’t have a great understanding of how this is expected to affect the viral fraction. Samples were sequenced with Illumina HiSeqX 2x150bp.\nThe raw data\nIn total, the Leung dataset comprised 293 samples:\n\nCode# Importing the data is a bit more complicated this time as the samples are split across three pipeline runs\ndata_dir_base &lt;- \"../data/2024-04-12_leung\"\ndata_dirs &lt;- paste(data_dir_base, c(1,2,3), sep=\"/\")\n\n# Define geo relationships for filling in\ngeo &lt;- tribble(~region, ~country, ~city,\n               \"Asia\", \"Hong Kong\", \"Hong Kong\",\n               \"Europe\", \"Norway\", \"Oslo\",\n               \"Europe\", \"Sweden\", \"Stockholm\",\n               \"Europe\", \"United Kingdom\", \"London\",\n               \"North America\", \"USA\", \"New York City\",\n               \"North America\", \"USA\", \"Denver\")\n\n# Data input paths\nlibraries_paths &lt;- file.path(data_dirs, \"sample-metadata.csv\")\nbasic_stats_paths &lt;- file.path(data_dirs, \"qc_basic_stats.tsv.gz\")\nadapter_stats_paths &lt;- file.path(data_dirs, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_paths &lt;- file.path(data_dirs, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_paths &lt;- file.path(data_dirs, \"qc_quality_sequence_stats.tsv.gz\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- lapply(libraries_paths, read_csv, show_col_types = FALSE) %&gt;%\n  bind_rows\nlibraries &lt;- libraries_raw %&gt;%\n  # Fix missing entries\n  mutate(city = ifelse(is.na(city), sub(\", .*\", \"\", location), city)) %&gt;%\n  left_join(geo, by=\"city\", suffix = c(\"\", \"_new\")) %&gt;%\n  mutate(region = ifelse(region == \"uncalculated\", region_new, region),\n         country = ifelse(country == \"uncalculated\", country_new, country)) %&gt;%\n  select(-country_new, -region_new) %&gt;%\n  # Add sample aliases\n  arrange(city, date, location) %&gt;%\n  group_by(city, date) %&gt;%\n  mutate(sample_count = row_number(),\n         date_alias = paste(as.character(date), sample_count, sep=\"_\"),\n         sample_alias = paste(city, date_alias, sep=\"_\"))\n\ncount_city &lt;- libraries %&gt;% group_by(region, country, city) %&gt;% \n  count(name=\"n_samples\")\ncount_city\n\n\n  \n\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;% arrange(sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_paths)\nadapter_stats &lt;- import_basic_paired(adapter_stats_paths)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_paths)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_paths)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% ungroup %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThese 293 samples yielded 0.39M-7.86M (mean 4.57M) reads per sample, for a total of 1.34B read pairs (402 gigabases of sequence). Read qualities were high at the 5’ end but dropped off significantly in some samples, in definite need of cleaning. Adapter levels were high. With the exception of a couple of early samples, inferred duplication levels were low (mean 9.4%).\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample, city, date,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample:date), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_city &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\",\n                                  name=\"City\")\nscale_x_cdate &lt;- purrr::partial(scale_x_date, name=\"Collection Date\",\n                                date_breaks = \"1 month\", date_labels = \"%Y-%m-%d\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=date, y=value, fill=city, group=interaction(city,sample))) +\n  geom_col(position = \"dodge\") +\n  scale_x_cdate() +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_city() + \n  facet_grid(metric~., scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_rotate + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_city &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                   name=\"City\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=city, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_city() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. Read loss during cleaning was highly variable but averaged 11%, with a further ~7% lost during deduplication and ~0.3% during ribodepletion.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, date, city, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage, City=city) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=city, group=sample)) +\n  scale_color_city() +\n  facet_wrap(~city, scales=\"free\", ncol=3) +\n  theme_kit + theme(legend.position = \"none\")\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=city, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_city() +\n  facet_wrap(~city, scales=\"free\", ncol=3) +\n  theme_kit + theme(legend.position = \"none\")\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and improving read qualities:\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=city, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_city() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels, which fell from an average of 9.4% to 1.7% for DNA reads:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_paths &lt;- file.path(data_dirs, \"bracken_counts.tsv\")\nbracken &lt;- lapply(bracken_paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", \n              values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, date, date_alias, city, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"date\", \"date_alias\", \"city\"),\n              names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample, date, date_alias, city,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:city), \n                               names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\n\n# Summarize composition\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(city, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=date_alias, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_wrap(~city, scales = \"free\") +\n  theme_kit + theme(axis.text.x = element_blank())\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = read_comp_long, position = \"stack\") +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[c(6,7,9)]\ng_comp_minor &lt;- g_comp_base + geom_col(data=read_comp_minor, position = \"stack\") +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, city) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, city) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(city, classification, read_fraction=display) %&gt;%\n  arrange(city, classification)\np_reads_summ\n\n\n  \n\n\n\nIn many respects, these resemble the Prussin data: high human fraction (mean 19.6%), high bacterial fraction (mean 18.2%), high unclassified fraction (mean 43.9%), low viral fraction (mean 0.01%). One notable difference is that archaeal reads are more abundant (0.034% compared to 0.016% for Prussin).\nAs in Prussin, viral DNA reads were dominated by Caudoviricetes phages. Other viral classes that are prominent in at least some samples include Herviviricetes (herpesviruses), Papovaviricetes (polyomaviruses and papillomaviruses), Revtraviricetes (retroviruses + Hep B), and Naldaviricetes (mainly arthropod viruses). I’ll investigate the first three of this latter group in more depth, restricting in each case to samples where that family makes up at least 5% of viral reads.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir_base, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get paths to Kraken reports\nsamples &lt;- as.character(basic_stats_raw$sample)\nreport_dirs &lt;- file.path(data_dirs, \"kraken\")\nreport_paths &lt;- lapply(report_dirs, list.files, full.names = TRUE) %&gt;% unlist\nnames(report_paths) &lt;- str_extract(report_paths, \"SRR\\\\d*\")\n\n# Extract viral taxa\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\",\n               \"rank\", \"taxid\", \"name\")\nkraken_reports_raw &lt;- lapply(report_paths, read_tsv, col_names = col_names,\n                             show_col_types = FALSE)\nkraken_reports &lt;- lapply(names(kraken_reports_raw), \n                         function(x) kraken_reports_raw[[x]] %&gt;% \n                           mutate(sample = x)) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.05\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nPapovaviricetes are quite heterogeneous across samples, and frequently diverse within samples. Alphapolyomavirus and Alphapapillomavirus are the most abundant genera overall, but Betapapillomavirus, Gammapapillomavirus, Mupapillomavirus and others all have strong showings.\n\nCode# Get samples\npapova_taxid &lt;- 2732421\npapova_threshold &lt;- 0.05\npapova_samples &lt;- viral_classes %&gt;% filter(taxid == papova_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\npapova_desc_taxids_old &lt;- papova_taxid\npapova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(papova_desc_taxids_new) &gt; length(papova_desc_taxids_old)){\n  papova_desc_taxids_old &lt;- papova_desc_taxids_new\n  papova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\npapova_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% papova_desc_taxids_new,\n         sample %in% papova_samples) %&gt;%\n  mutate(p_reads_papova = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\npapova_genera &lt;- papova_counts %&gt;% filter(rank == \"G\")\npapova_genera_major_tab &lt;- papova_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_papova_max = max(p_reads_papova), .groups=\"drop\") %&gt;%\n  filter(p_reads_papova_max &gt;= papova_threshold)\npapova_genera_major_list &lt;- papova_genera_major_tab %&gt;% pull(name)\npapova_genera_major &lt;- papova_genera %&gt;% \n  filter(name %in% papova_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_papova)\npapova_genera_minor &lt;- papova_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_papova_major = sum(p_reads_papova), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_papova = 1-p_reads_papova_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_papova)\npapova_genera_display &lt;- bind_rows(papova_genera_major, papova_genera_minor) %&gt;%\n  arrange(desc(p_reads_papova)) %&gt;% \n  mutate(name = factor(name, levels=c(papova_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_papova, classification=name)\n\n# Plot\ng_papova_genera &lt;- g_comp_base + \n  geom_col(data=papova_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papovaviricetes Reads\", limits=c(0,1.02), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_papova_genera\n\n\n\n\n\n\n\nOnly a few samples showed at least 5% prevalence of Herviviricetes, but those that did were typically dominated by one or a small number of species that varied between samples. Of these, human alphaherpesvirus 1 appeared in the most samples, but several other species were prominent in at least one sample:\n\nCode# Get samples\nhervi_taxid &lt;- 2731363\nhervi_threshold &lt;- 0.05\nhervi_samples &lt;- viral_classes %&gt;% filter(taxid == hervi_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\nhervi_desc_taxids_old &lt;- hervi_taxid\nhervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(hervi_desc_taxids_new) &gt; length(hervi_desc_taxids_old)){\n  hervi_desc_taxids_old &lt;- hervi_desc_taxids_new\n  hervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\nhervi_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% hervi_desc_taxids_new,\n         sample %in% hervi_samples) %&gt;%\n  mutate(p_reads_hervi = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nhervi_genera &lt;- hervi_counts %&gt;% filter(rank == \"S\")\nhervi_genera_major_tab &lt;- hervi_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_hervi_max = max(p_reads_hervi), .groups=\"drop\") %&gt;%\n  filter(p_reads_hervi_max &gt;= hervi_threshold)\nhervi_genera_major_list &lt;- hervi_genera_major_tab %&gt;% pull(name)\nhervi_genera_major &lt;- hervi_genera %&gt;% \n  filter(name %in% hervi_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_hervi)\nhervi_genera_minor &lt;- hervi_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_hervi_major = sum(p_reads_hervi), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_hervi = 1-p_reads_hervi_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_hervi)\nhervi_genera_display &lt;- bind_rows(hervi_genera_major, hervi_genera_minor) %&gt;%\n  arrange(desc(p_reads_hervi)) %&gt;% \n  mutate(name = factor(name, levels=c(hervi_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_hervi, classification=name)\n\n# Plot\ng_hervi_genera &lt;- g_comp_base + \n  geom_col(data=hervi_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Herviviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_hervi_genera\n\n\n\n\n\n\n\nFinally, for Revtraviricetes, most samples were dominated by porcine type-C oncovirus, while one was dominated by an avian retrovirus. The last showed significant levels of two murine viruses plus HIV. I’m suspicious of many of these.\n\nCode# Get samples\nrevtra_taxid &lt;- 2732514\nrevtra_threshold &lt;- 0.05\nrevtra_samples &lt;- viral_classes %&gt;% filter(taxid == revtra_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\nrevtra_desc_taxids_old &lt;- revtra_taxid\nrevtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(revtra_desc_taxids_new) &gt; length(revtra_desc_taxids_old)){\n  revtra_desc_taxids_old &lt;- revtra_desc_taxids_new\n  revtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\nrevtra_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% revtra_desc_taxids_new,\n         sample %in% revtra_samples) %&gt;%\n  mutate(p_reads_revtra = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nrevtra_genera &lt;- revtra_counts %&gt;% filter(rank == \"S\")\nrevtra_genera_major_tab &lt;- revtra_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_revtra_max = max(p_reads_revtra), .groups=\"drop\") %&gt;%\n  filter(p_reads_revtra_max &gt;= revtra_threshold)\nrevtra_genera_major_list &lt;- revtra_genera_major_tab %&gt;% pull(name)\nrevtra_genera_major &lt;- revtra_genera %&gt;% \n  filter(name %in% revtra_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_revtra)\nrevtra_genera_minor &lt;- revtra_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_revtra_major = sum(p_reads_revtra), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_revtra = 1-p_reads_revtra_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_revtra)\nrevtra_genera_display &lt;- bind_rows(revtra_genera_major, revtra_genera_minor) %&gt;%\n  arrange(desc(p_reads_revtra)) %&gt;% \n  mutate(name = factor(name, levels=c(revtra_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_revtra, classification=name)\n\n# Plot\ng_revtra_genera &lt;- g_comp_base + \n  geom_col(data=revtra_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% revtraviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_revtra_genera\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. Using the same workflow I used for Prussin et al, I identified 24,278 read pairs as putatively human viral: 0.002% of reads surviving to that stage in the pipeline.\n\nCode# Import HV read data\nhv_reads_filtered_paths &lt;- file.path(data_dirs, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_paths, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  inner_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, date, date_alias, city, seq_id) %&gt;% count %&gt;%\n  group_by(sample, date, date_alias, city) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\ng_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_hist_0\n\n\n\n\n\n\n\nAs previously described, I ran BLASTN on these reads via a dedicated EC2 instance, using the same parameters I’ve used for previous datasets.\n\nCodemrg_fasta &lt;-  mrg %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;%\n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_sep &lt;- bind_rows(select(mrg_fasta, header=header1, seq=seq1),\n                           select(mrg_fasta, header=header2, seq=seq2)) %&gt;%\n  filter(!is.na(seq))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta_sep, sep=\"\\n\")) %&gt;% \n  paste(collapse=\"\\n\")\nblast_dir &lt;- file.path(data_dir_base, \"blast\")\ndir.create(blast_dir, showWarnings = FALSE)\nwrite(mrg_fasta_out, file.path(blast_dir, \"putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\n# blast_results_path &lt;- file.path(data_dir_base, \"blast/putative-viral.blast.gz\")\n# blast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\n# blast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE,\n#                           col_names = blast_cols, col_types = cols(.default=\"c\"))\nblast_results_path &lt;- file.path(data_dir_base, \"blast/putative-viral-best.blast.gz\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE)\n\n# Filter for best hit for each query/subject combination\nblast_results_best &lt;- blast_results %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n# write_tsv(blast_results_best, file.path(data_dir_base, \"blast/putative-viral-best.blast.gz\"))\n\n# Rank hits for each query and filter for high-ranking hits\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_results_assign &lt;- inner_join(blast_results_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_results_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot RNA\ng_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max, fill=viral_status_out)) + \n  geom_histogram(binwidth=5,boundary=0) + \n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base\ng_hist_1\n\n\n\n\n\n\n\nFor a disjunctive score threshold of 20, the workflow achieves a measured F1 score of 98.0%.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nLooking into the composition of different read groups, the notable observation for me is the high prevalence of Pigeon torque teno virus among high-scoring false positives, with 77 such read pairs. BLAST maps these not to viruses but to their most common hosts, i.e. assorted species of pigeon. That said, the number of false positive PTTV reads is substantially exceeded by the number of true-positive PTTV reads (1883), which do map to appropriate viruses according to BLAST, so the presence of a comparatively small number of false positives seems unlikely to cause too much distortion.\n\nCodemajor_threshold &lt;- 0.04\n\n# Add missing viral taxa\nviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\nviral_taxa$name[viral_taxa$taxid == 493803] &lt;- \"Merkel cell polyomavirus\"\nviral_taxa$name[viral_taxa$taxid == 427343] &lt;- \"Human papillomavirus 107\"\nviral_taxa$name[viral_taxa$taxid == 194958] &lt;- \"Porcine endogenous retrovirus A\"\nviral_taxa$name[viral_taxa$taxid == 340907] &lt;- \"Papiine alphaherpesvirus 2\"\nviral_taxa$name[viral_taxa$taxid == 194959] &lt;- \"Porcine endogenous retrovirus B\"\n\n\n# Prepare data\nfp &lt;- mrg_blast %&gt;% \n  group_by(viral_status_out, highscore, taxid_best) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  rename(taxid = taxid_best) %&gt;%\n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Plot\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(.~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\n\nCode# Configure\nref_taxid_ptt &lt;- 2233536\np_threshold &lt;- 0.3\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\",\n                         177155, \"Streptopelia turtur\",\n                         187126, \"Nesoenas mayeri\"\n                         )\ntax_names &lt;- tax_names_new %&gt;% filter(! staxid %in% tax_names$staxid) %&gt;%\n  bind_rows(tax_names) %&gt;% arrange(staxid)\nref_name_ptt &lt;- tax_names %&gt;% filter(staxid == ref_taxid_ptt) %&gt;% pull(name)\n\n# Get major matches\nmrg_staxid &lt;- mrg_blast %&gt;% filter(taxid_best == ref_taxid_ptt) %&gt;%\n    group_by(highscore, viral_status_out) %&gt;% mutate(n_seq = n())\nfp_staxid &lt;- mrg_staxid %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid_best=staxid), by=\"taxid_best\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, \n           taxid_best, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, taxid_best, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(status_display~score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_ptt, \" (taxid \", ref_taxid_ptt, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, date_alias, city, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | hit_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_city &lt;- read_counts %&gt;% group_by(city) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\", date_alias = \"All dates\")\nread_counts_total &lt;- read_counts_city %&gt;% group_by(sample, date_alias) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(city = \"All cities\")\nread_counts_agg &lt;- read_counts_city %&gt;% arrange(city) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         city = fct_inorder(city))\n\n\nApplying a disjunctive cutoff at S=20 identifies 23,191 read pairs as human-viral. This gives an overall relative HV abundance of \\(1.73 \\times 10^{-5}\\).\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=city, color=city)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  #facet_grid(.~sample_type, scales = \"free\", space = \"free_x\") +\n  scale_color_city() + theme_rotate\ng_phv_agg\n\n\n\n\n\n\n\nThis is lower than for DNA reads from other air-sampling datasets I’ve analyzed, but not drastically so:\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Leung\", 1.73e-5, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nAt the family level, most samples across all cities are dominated by Papillomaviridae, Herpesviridae, Anelloviridae, Polyomaviridae, and to a lesser extent Poxviridae:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\n\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\n\nIn investigating individual viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads.\nAs usual, Papillomaviridae reads are divided among many different viral species. In this case, Betapapillomavirus 1 and 2 are the most prevalent across samples, but many other alpha-, beta-, gamma-, and mupapillomaviruses are highly prevalent in at least some samples.\n\nCodethreshold_major_species &lt;- 0.4\ntaxid_papilloma &lt;- 151340\n\n# Get set of Papillomaviridae reads\npapilloma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_papilloma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npapilloma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_papilloma, sample %in% papilloma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each Papillomaviridae species\npapilloma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% papilloma_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_papilloma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npapilloma_species_major_tab &lt;- papilloma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_papilloma == max(p_reads_papilloma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_papilloma)) %&gt;% \n  filter(p_reads_papilloma &gt; threshold_major_species)\npapilloma_species_counts_major &lt;- papilloma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% papilloma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_papilloma = sum(n_reads_hv),\n            p_reads_papilloma = sum(p_reads_papilloma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(papilloma_species_major_tab$name, \"Other\")))\npapilloma_species_counts_display &lt;- papilloma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_papilloma, classification = name_display)\n\n# Plot\ng_papilloma_species &lt;- g_comp_base + \n  geom_col(data=papilloma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papillomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Papillomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_papilloma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npapilloma_species_collate &lt;- papilloma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_papilloma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn terms of total reads across samples, herpesviruses are dominated by Epstein-Barr virus (Human gammaherpesvirus 4), HSV-1 (Human alphaherpesvirus 1), and human cytomegalovirus (Human betaherpesvirus 5). However, numerous other herpesviruses are also present.\n\nCodethreshold_major_species &lt;- 0.4\ntaxid_herpes &lt;- viral_taxa %&gt;% filter(name == \"Herpesviridae\") %&gt;% pull(taxid)\n\n# Get set of herpesviridae reads\nherpes_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_herpes) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nherpes_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_herpes, sample %in% herpes_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each herpesviridae species\nherpes_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn sharp contrast to the above, my pipeline classifies the great majority of anellovirus reads in all samples into a single species, torque teno virus. Looking online, it looks like there are a lot of “torque teno viruses” within Anelloviridae – for example, Wikipedia says that the genus Alphatorquevirus contains &gt;20 numbered torque teno viruses – so I’m not sure exactly which virus this refers to.\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_anello &lt;- viral_taxa %&gt;% filter(name == \"Anelloviridae\") %&gt;% pull(taxid)\n\n# Get set of anelloviridae reads\nanello_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_anello) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nanello_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_anello, sample %in% anello_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each anelloviridae species\nanello_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% anello_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_anello = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nanello_species_major_tab &lt;- anello_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_anello == max(p_reads_anello)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_anello)) %&gt;% \n  filter(p_reads_anello &gt; threshold_major_species)\nanello_species_counts_major &lt;- anello_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% anello_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_anello = sum(n_reads_hv),\n            p_reads_anello = sum(p_reads_anello), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(anello_species_major_tab$name, \"Other\")))\nanello_species_counts_display &lt;- anello_species_counts_major %&gt;%\n  rename(p_reads = p_reads_anello, classification = name_display)\n\n# Plot\ng_anello_species &lt;- g_comp_base + \n  geom_col(data=anello_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Anelloviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Anelloviridae reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_anello_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nanello_species_collate &lt;- anello_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_anello), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nPolyomaviruses are intermediate; most viruses are dominated by a single species, Alphapolyomavirus quintihominis, but several other viruses in the family are also present.\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_polyoma &lt;- viral_taxa %&gt;% filter(name == \"Polyomaviridae\") %&gt;% pull(taxid)\n\n# Get set of polyomaviridae reads\n# Get set of polyomaviridae reads\npolyoma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_polyoma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npolyoma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_polyoma, sample %in% polyoma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each polyomaviridae species\npolyoma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% polyoma_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_polyoma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npolyoma_species_major_tab &lt;- polyoma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_polyoma == max(p_reads_polyoma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_polyoma)) %&gt;% \n  filter(p_reads_polyoma &gt; threshold_major_species)\npolyoma_species_counts_major &lt;- polyoma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% polyoma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_polyoma = sum(n_reads_hv),\n            p_reads_polyoma = sum(p_reads_polyoma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(polyoma_species_major_tab$name, \"Other\")))\npolyoma_species_counts_display &lt;- polyoma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_polyoma, classification = name_display)\n\n# Plot\ng_polyoma_species &lt;- g_comp_base + \n  geom_col(data=polyoma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Polyomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Polyomaviridae reads\") +\n  guides(fill=guide_legend(ncol=2)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_polyoma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npolyoma_species_collate &lt;- polyoma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_polyoma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nFinally, poxvirus reads in most samples are dominated by molluscum contagiosum virus (which I expect to be real), followed by Orf virus (which I expect to be fake). These expectations are borne out by BLAST alignments (below).\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_pox &lt;- viral_taxa %&gt;% filter(name == \"Poxviridae\") %&gt;% pull(taxid)\n\n# Get set of poxviridae reads\n# Get set of poxviridae reads\npox_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_pox) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npox_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_pox, sample %in% pox_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each poxviridae species\npox_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% pox_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_pox = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npox_species_major_tab &lt;- pox_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_pox == max(p_reads_pox)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_pox)) %&gt;% \n  filter(p_reads_pox &gt; threshold_major_species)\npox_species_counts_major &lt;- pox_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% pox_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_pox = sum(n_reads_hv),\n            p_reads_pox = sum(p_reads_pox), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(pox_species_major_tab$name, \"Other\")))\npox_species_counts_display &lt;- pox_species_counts_major %&gt;%\n  rename(p_reads = p_reads_pox, classification = name_display)\n\n# Plot\ng_pox_species &lt;- g_comp_base + \n  geom_col(data=pox_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Poxviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Poxviridae reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_pox_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npox_species_collate &lt;- pox_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_pox), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCode# Configure\nref_taxids_hv &lt;- c(10279, 10258)\nref_names_hv &lt;- sapply(ref_taxids_hv, function(x) viral_taxa %&gt;% filter(taxid == x) %&gt;% pull(name) %&gt;% first)\np_threshold &lt;- 0.1\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         257877, \"Macaca thibetana thibetana\",\n                         256321, \"Lentiviral transfer vector pHsCXW\",\n                         419242, \"Shuttle vector pLvCmvMYOCDHA\",\n                         419243, \"Shuttle vector pLvCmvLacZ\",\n                         421868, \"Cloning vector pLvCmvLacZ.Gfp\",\n                         421869, \"Cloning vector pLvCmvMyocardin.Gfp\",\n                         426303, \"Lentiviral vector pNL-GFP-RRE(SA)\",\n                         436015, \"Lentiviral transfer vector pFTMGW\",\n                         454257, \"Shuttle vector pLvCmvMYOCD2aHA\",\n                         476184, \"Shuttle vector pLV.mMyoD::ERT2.eGFP\",\n                         476185, \"Shuttle vector pLV.hMyoD.eGFP\",\n                         591936, \"Piliocolobus tephrosceles\",\n                         627481, \"Lentiviral transfer vector pFTM3GW\",\n                         680261, \"Self-inactivating lentivirus vector pLV.C-EF1a.cyt-bGal.dCpG\",\n                         2952778, \"Expression vector pLV[Exp]-EGFP:T2A:Puro-EF1A\",\n                         3022699, \"Vector PAS_122122\",\n                         3025913, \"Vector pSIN-WP-mPGK-GDNF\",\n                         3105863, \"Vector pLKO.1-ZsGreen1\",\n                         3105864, \"Vector pLKO.1-ZsGreen1 mouse Wfs1 shRNA\",\n                         3108001, \"Cloning vector pLVSIN-CMV_Neo_v4.0\",\n                         3109234, \"Vector pTwist+Kan+High\",\n                         3117662, \"Cloning vector pLV[Exp]-CBA&gt;P301L\",\n                         3117663, \"Cloning vector pLV[Exp]-CBA&gt;P301L:T2A:mRuby3\",\n                         3117664, \"Cloning vector pLV[Exp]-CBA&gt;hMAPT[NM_005910.6](ns):T2A:mRuby3\",\n                         3117665, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3\",\n                         3117666, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3/NFAT3 fusion protein\",\n                         3117667, \"Cloning vector pLV[Exp]-Neo-mPGK&gt;{EGFP-hSEPT6}\",\n                         438045, \"Xenotropic MuLV-related virus\",\n                         447135, \"Myodes glareolus\",\n                         590745, \"Mus musculus mobilized endogenous polytropic provirus\",\n                         181858, \"Murine AIDS virus-related provirus\",\n                         356663, \"Xenotropic MuLV-related virus VP35\",\n                         356664, \"Xenotropic MuLV-related virus VP42\",\n                         373193, \"Xenotropic MuLV-related virus VP62\",\n                         286419, \"Canis lupus dingo\",\n                         415978, \"Sus scrofa scrofa\",\n                         494514, \"Vulpes lagopus\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\n# Get matches\nhv_blast_staxids &lt;- hv_reads_species %&gt;% filter(taxid %in% ref_taxids_hv) %&gt;%\n  group_by(taxid) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names %&gt;% rename(sname=name), by=\"staxid\")\n\n# Count matches\nhv_blast_counts &lt;- hv_blast_staxids %&gt;%\n  group_by(taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;% mutate(p=n/n_seq)\n\n# Subset to major matches\nhv_blast_counts_major &lt;- hv_blast_counts %&gt;% \n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  arrange(desc(p)) %&gt;% group_by(taxid) %&gt;%\n  filter(row_number() &lt;= 25) %&gt;%\n  mutate(name_display = ifelse(name == ref_names_hv[1], \"MCV\", name))\n\n# Plot\ng_hv_blast &lt;- ggplot(hv_blast_counts_major, mapping=aes(x=p, y=sname)) +\n  geom_col() +\n  facet_grid(name_display~., scales=\"free_y\", space=\"free_y\") +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), \n                     breaks=seq(0,1,0.2), expand=c(0,0)) +\n  theme_base + theme(axis.title.y = element_blank())\ng_hv_blast\n\n\n\n\n\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nn_path_genera &lt;- hv_reads_genus %&gt;% \n  group_by(sample, date_alias, city, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"date_alias\", \"city\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type, city) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", date=\"All dates\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=city)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_city() +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\nConclusion\nThis is the third, largest, and final of this tranche of air-sampling datasets that I’ve run through this pipeline. Many of the high-level findings were similar to Prussin and Rosario, including high relative abundance of human reads, low total viral reads, an absence of enteric viruses, and high abundance of papillomaviruses among human-infecting viruses.\nIn the future, I’ll do a more in-depth comparative analysis across different datasets to compare the abundance of different viruses. For now, though, there are some major updates to the pipeline I want to make before I do any more public analyses."
  },
  {
    "objectID": "notebooks/2024-04-19_leung.html",
    "href": "notebooks/2024-04-19_leung.html",
    "title": "Workflow analysis of Leung et al. (2021)",
    "section": "",
    "text": "The last in our current run of air sampling datasets is Leung et al. (2021), a study of active air samples collected in public transit systems from six cities (Denver, Hong Kong, London, NYC, Oslo, Stockholm) from June to September 2017.\nSamples from Denver originated from their rail and bus system; all other samples originated from metro systems. Collection took place during working days and working hours. Air samples were collected with the SASS 3100 Dry Air Samplers (filtration) for 30 min at a flowrate of 300 L/min using electret microfibrous filters. Filters were stationed at 1.5m above floor level, facing downward (to avoid direct deposition).\nThis was a DNA-sequencing study, focused on the bacterial microbiome and resistome. Sample processing followed an ideosyncratic protocol, where samples were pelleted and the pellet and supernatant were processed separately before being recombined for NA extraction and sequencing; I don’t have a great understanding of how this is expected to affect the viral fraction. Samples were sequenced with Illumina HiSeqX 2x150bp.\nThe raw data\nIn total, the Leung dataset comprised 293 samples:\n\nCode# Importing the data is a bit more complicated this time as the samples are split across three pipeline runs\ndata_dir_base &lt;- \"../data/2024-04-12_leung\"\ndata_dirs &lt;- paste(data_dir_base, c(1,2,3), sep=\"/\")\n\n# Define geo relationships for filling in\ngeo &lt;- tribble(~region, ~country, ~city,\n               \"Asia\", \"Hong Kong\", \"Hong Kong\",\n               \"Europe\", \"Norway\", \"Oslo\",\n               \"Europe\", \"Sweden\", \"Stockholm\",\n               \"Europe\", \"United Kingdom\", \"London\",\n               \"North America\", \"USA\", \"New York City\",\n               \"North America\", \"USA\", \"Denver\")\n\n# Data input paths\nlibraries_paths &lt;- file.path(data_dirs, \"sample-metadata.csv\")\nbasic_stats_paths &lt;- file.path(data_dirs, \"qc_basic_stats.tsv.gz\")\nadapter_stats_paths &lt;- file.path(data_dirs, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_paths &lt;- file.path(data_dirs, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_paths &lt;- file.path(data_dirs, \"qc_quality_sequence_stats.tsv.gz\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- lapply(libraries_paths, read_csv, show_col_types = FALSE) %&gt;%\n  bind_rows\nlibraries &lt;- libraries_raw %&gt;%\n  # Fix missing entries\n  mutate(city = ifelse(is.na(city), sub(\", .*\", \"\", location), city)) %&gt;%\n  left_join(geo, by=\"city\", suffix = c(\"\", \"_new\")) %&gt;%\n  mutate(region = ifelse(region == \"uncalculated\", region_new, region),\n         country = ifelse(country == \"uncalculated\", country_new, country)) %&gt;%\n  select(-country_new, -region_new) %&gt;%\n  # Add sample aliases\n  arrange(city, date, location) %&gt;%\n  group_by(city, date) %&gt;%\n  mutate(sample_count = row_number(),\n         date_alias = paste(as.character(date), sample_count, sep=\"_\"),\n         sample_alias = paste(city, date_alias, sep=\"_\"))\n\ncount_city &lt;- libraries %&gt;% group_by(region, country, city) %&gt;% \n  count(name=\"n_samples\")\ncount_city\n\n\n  \n\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;% arrange(sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_paths)\nadapter_stats &lt;- import_basic_paired(adapter_stats_paths)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_paths)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_paths)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% ungroup %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThese 293 samples yielded 0.39M-7.86M (mean 4.57M) reads per sample, for a total of 1.34B read pairs (402 gigabases of sequence). Read qualities were high at the 5’ end but dropped off significantly in some samples, in definite need of cleaning. Adapter levels were high. With the exception of a couple of early samples, inferred duplication levels were low (mean 9.4%).\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample, city, date,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample:date), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_city &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\",\n                                  name=\"City\")\nscale_x_cdate &lt;- purrr::partial(scale_x_date, name=\"Collection Date\",\n                                date_breaks = \"1 month\", date_labels = \"%Y-%m-%d\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=date, y=value, fill=city, group=interaction(city,sample))) +\n  geom_col(position = \"dodge\") +\n  scale_x_cdate() +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_city() + \n  facet_grid(metric~., scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_rotate + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_city &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                   name=\"City\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=city, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_city() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. Read loss during cleaning was highly variable but averaged 11%, with a further ~7% lost during deduplication and ~0.3% during ribodepletion.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, date, city, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage, City=city) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=city, group=sample)) +\n  scale_color_city() +\n  facet_wrap(~city, scales=\"free\", ncol=3) +\n  theme_kit + theme(legend.position = \"none\")\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=city, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_city() +\n  facet_wrap(~city, scales=\"free\", ncol=3) +\n  theme_kit + theme(legend.position = \"none\")\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and improving read qualities:\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=city, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_city() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels, which fell from an average of 9.4% to 1.7% for DNA reads:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCode# Import Bracken data\nbracken_paths &lt;- file.path(data_dirs, \"bracken_counts.tsv\")\nbracken &lt;- lapply(bracken_paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows\ntotal_assigned &lt;- bracken %&gt;% group_by(sample) %&gt;% summarize(\n  name = \"Total\",\n  kraken_assigned_reads = sum(kraken_assigned_reads),\n  added_reads = sum(added_reads),\n  new_est_reads = sum(new_est_reads),\n  fraction_total_reads = sum(fraction_total_reads)\n)\nbracken_spread &lt;- bracken %&gt;% select(name, sample, new_est_reads) %&gt;%\n  mutate(name = tolower(name)) %&gt;%\n  pivot_wider(id_cols = \"sample\", names_from = \"name\", \n              values_from = \"new_est_reads\")\n\n# Count reads\nread_counts_preproc &lt;- basic_stats %&gt;% \n  select(sample, date, date_alias, city, stage, n_read_pairs) %&gt;%\n  pivot_wider(id_cols = c(\"sample\", \"date\", \"date_alias\", \"city\"),\n              names_from=\"stage\", values_from=\"n_read_pairs\")\nread_counts &lt;- read_counts_preproc %&gt;%\n  inner_join(total_assigned %&gt;% select(sample, new_est_reads), by = \"sample\") %&gt;%\n  rename(assigned = new_est_reads) %&gt;%\n  inner_join(bracken_spread, by=\"sample\")\n\n# Assess composition\nread_comp &lt;- transmute(read_counts, sample, date, date_alias, city,\n                       n_filtered = raw_concat-cleaned,\n                       n_duplicate = cleaned-dedup,\n                       n_ribosomal = (dedup-ribo_initial) + (ribo_initial-ribo_secondary),\n                       n_unassigned = ribo_secondary-assigned,\n                       n_bacterial = bacteria,\n                       n_archaeal = archaea,\n                       n_viral = viruses,\n                       n_human = eukaryota)\nread_comp_long &lt;- pivot_longer(read_comp, -(sample:city), \n                               names_to = \"classification\",\n                               names_prefix = \"n_\", values_to = \"n_reads\") %&gt;%\n  mutate(classification = fct_inorder(str_to_sentence(classification))) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\n\n# Summarize composition\nread_comp_summ &lt;- read_comp_long %&gt;% \n  group_by(city, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=date_alias, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Collection Date\") +\n  facet_wrap(~city, scales = \"free\") +\n  theme_kit + theme(axis.text.x = element_blank())\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = read_comp_long, position = \"stack\") +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- read_comp_long %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[c(6,7,9)]\ng_comp_minor &lt;- g_comp_base + geom_col(data=read_comp_minor, position = \"stack\") +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- read_comp_long %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, city) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, city) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(city, classification, read_fraction=display) %&gt;%\n  arrange(city, classification)\np_reads_summ\n\n\n  \n\n\n\nIn many respects, these resemble the Prussin data: high human fraction (mean 19.6%), high bacterial fraction (mean 18.2%), high unclassified fraction (mean 43.9%), low viral fraction (mean 0.01%). One notable difference is that archaeal reads are more abundant (0.034% compared to 0.016% for Prussin).\nAs in Prussin, viral DNA reads were dominated by Caudoviricetes phages. Other viral classes that are prominent in at least some samples include Herviviricetes (herpesviruses), Papovaviricetes (polyomaviruses and papillomaviruses), Revtraviricetes (retroviruses + Hep B), and Naldaviricetes (mainly arthropod viruses). I’ll investigate the first three of this latter group in more depth, restricting in each case to samples where that family makes up at least 5% of viral reads.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir_base, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get paths to Kraken reports\nsamples &lt;- as.character(basic_stats_raw$sample)\nreport_dirs &lt;- file.path(data_dirs, \"kraken\")\nreport_paths &lt;- lapply(report_dirs, list.files, full.names = TRUE) %&gt;% unlist\nnames(report_paths) &lt;- str_extract(report_paths, \"SRR\\\\d*\")\n\n# Extract viral taxa\ncol_names &lt;- c(\"pc_reads_total\", \"n_reads_clade\", \"n_reads_direct\",\n               \"rank\", \"taxid\", \"name\")\nkraken_reports_raw &lt;- lapply(report_paths, read_tsv, col_names = col_names,\n                             show_col_types = FALSE)\nkraken_reports &lt;- lapply(names(kraken_reports_raw), \n                         function(x) kraken_reports_raw[[x]] %&gt;% \n                           mutate(sample = x)) %&gt;% bind_rows\nkraken_reports_viral &lt;- filter(kraken_reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.05\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nPapovaviricetes are quite heterogeneous across samples, and frequently diverse within samples. Alphapolyomavirus and Alphapapillomavirus are the most abundant genera overall, but Betapapillomavirus, Gammapapillomavirus, Mupapillomavirus and others all have strong showings.\n\nCode# Get samples\npapova_taxid &lt;- 2732421\npapova_threshold &lt;- 0.05\npapova_samples &lt;- viral_classes %&gt;% filter(taxid == papova_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\npapova_desc_taxids_old &lt;- papova_taxid\npapova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(papova_desc_taxids_new) &gt; length(papova_desc_taxids_old)){\n  papova_desc_taxids_old &lt;- papova_desc_taxids_new\n  papova_desc_taxids_new &lt;- unique(c(papova_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% papova_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\npapova_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% papova_desc_taxids_new,\n         sample %in% papova_samples) %&gt;%\n  mutate(p_reads_papova = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\npapova_genera &lt;- papova_counts %&gt;% filter(rank == \"G\")\npapova_genera_major_tab &lt;- papova_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_papova_max = max(p_reads_papova), .groups=\"drop\") %&gt;%\n  filter(p_reads_papova_max &gt;= papova_threshold)\npapova_genera_major_list &lt;- papova_genera_major_tab %&gt;% pull(name)\npapova_genera_major &lt;- papova_genera %&gt;% \n  filter(name %in% papova_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_papova)\npapova_genera_minor &lt;- papova_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_papova_major = sum(p_reads_papova), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_papova = 1-p_reads_papova_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_papova)\npapova_genera_display &lt;- bind_rows(papova_genera_major, papova_genera_minor) %&gt;%\n  arrange(desc(p_reads_papova)) %&gt;% \n  mutate(name = factor(name, levels=c(papova_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_papova, classification=name)\n\n# Plot\ng_papova_genera &lt;- g_comp_base + \n  geom_col(data=papova_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papovaviricetes Reads\", limits=c(0,1.02), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_papova_genera\n\n\n\n\n\n\n\nOnly a few samples showed at least 5% prevalence of Herviviricetes, but those that did were typically dominated by one or a small number of species that varied between samples. Of these, human alphaherpesvirus 1 appeared in the most samples, but several other species were prominent in at least one sample:\n\nCode# Get samples\nhervi_taxid &lt;- 2731363\nhervi_threshold &lt;- 0.05\nhervi_samples &lt;- viral_classes %&gt;% filter(taxid == hervi_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\nhervi_desc_taxids_old &lt;- hervi_taxid\nhervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(hervi_desc_taxids_new) &gt; length(hervi_desc_taxids_old)){\n  hervi_desc_taxids_old &lt;- hervi_desc_taxids_new\n  hervi_desc_taxids_new &lt;- unique(c(hervi_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% hervi_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\nhervi_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% hervi_desc_taxids_new,\n         sample %in% hervi_samples) %&gt;%\n  mutate(p_reads_hervi = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nhervi_genera &lt;- hervi_counts %&gt;% filter(rank == \"S\")\nhervi_genera_major_tab &lt;- hervi_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_hervi_max = max(p_reads_hervi), .groups=\"drop\") %&gt;%\n  filter(p_reads_hervi_max &gt;= hervi_threshold)\nhervi_genera_major_list &lt;- hervi_genera_major_tab %&gt;% pull(name)\nhervi_genera_major &lt;- hervi_genera %&gt;% \n  filter(name %in% hervi_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_hervi)\nhervi_genera_minor &lt;- hervi_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_hervi_major = sum(p_reads_hervi), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_hervi = 1-p_reads_hervi_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_hervi)\nhervi_genera_display &lt;- bind_rows(hervi_genera_major, hervi_genera_minor) %&gt;%\n  arrange(desc(p_reads_hervi)) %&gt;% \n  mutate(name = factor(name, levels=c(hervi_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_hervi, classification=name)\n\n# Plot\ng_hervi_genera &lt;- g_comp_base + \n  geom_col(data=hervi_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Herviviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_hervi_genera\n\n\n\n\n\n\n\nFinally, for Revtraviricetes, most samples were dominated by porcine type-C oncovirus, while one was dominated by an avian retrovirus. The last showed significant levels of two murine viruses plus HIV. I’m suspicious of many of these.\n\nCode# Get samples\nrevtra_taxid &lt;- 2732514\nrevtra_threshold &lt;- 0.05\nrevtra_samples &lt;- viral_classes %&gt;% filter(taxid == revtra_taxid) %&gt;% filter(p_reads_viral &gt; 0.05) %&gt;% pull(sample) %&gt;% unique\n\n# Get all taxa in class\nrevtra_desc_taxids_old &lt;- revtra_taxid\nrevtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\nwhile (length(revtra_desc_taxids_new) &gt; length(revtra_desc_taxids_old)){\n  revtra_desc_taxids_old &lt;- revtra_desc_taxids_new\n  revtra_desc_taxids_new &lt;- unique(c(revtra_desc_taxids_old, viral_taxa %&gt;% filter(parent_taxid %in% revtra_desc_taxids_old) %&gt;% pull(taxid)))\n}\n\n# Get read counts\nrevtra_counts &lt;- kraken_reports_viral_cleaned %&gt;%\n  filter(taxid %in% revtra_desc_taxids_new,\n         sample %in% revtra_samples) %&gt;%\n  mutate(p_reads_revtra = n_reads_clade/n_reads_clade[1])\n\n# Get genus composition\nrevtra_genera &lt;- revtra_counts %&gt;% filter(rank == \"S\")\nrevtra_genera_major_tab &lt;- revtra_genera %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_revtra_max = max(p_reads_revtra), .groups=\"drop\") %&gt;%\n  filter(p_reads_revtra_max &gt;= revtra_threshold)\nrevtra_genera_major_list &lt;- revtra_genera_major_tab %&gt;% pull(name)\nrevtra_genera_major &lt;- revtra_genera %&gt;% \n  filter(name %in% revtra_genera_major_list) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_revtra)\nrevtra_genera_minor &lt;- revtra_genera_major %&gt;% \n  group_by(sample, date_alias, city) %&gt;%\n  summarize(p_reads_revtra_major = sum(p_reads_revtra), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_revtra = 1-p_reads_revtra_major) %&gt;%\n  select(name, taxid, sample, date_alias, city, p_reads_revtra)\nrevtra_genera_display &lt;- bind_rows(revtra_genera_major, revtra_genera_minor) %&gt;%\n  arrange(desc(p_reads_revtra)) %&gt;% \n  mutate(name = factor(name, levels=c(revtra_genera_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_revtra, classification=name)\n\n# Plot\ng_revtra_genera &lt;- g_comp_base + \n  geom_col(data=revtra_genera_display, position = \"stack\") +\n  scale_y_continuous(name=\"% revtraviricetes Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral genus\") +\n  guides(fill=guide_legend(ncol=3))\ng_revtra_genera\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. Using the same workflow I used for Prussin et al, I identified 24,278 read pairs as putatively human viral: 0.002% of reads surviving to that stage in the pipeline.\n\nCode# Import HV read data\nhv_reads_filtered_paths &lt;- file.path(data_dirs, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_paths, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  inner_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, date, date_alias, city, seq_id) %&gt;% count %&gt;%\n  group_by(sample, date, date_alias, city) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\ng_hist_0 &lt;- ggplot(mrg, aes(x=adj_score_max)) + \n  geom_histogram(binwidth=5,boundary=0) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_hist_0\n\n\n\n\n\n\n\nAs previously described, I ran BLASTN on these reads via a dedicated EC2 instance, using the same parameters I’ve used for previous datasets.\n\nCodemrg_fasta &lt;-  mrg %&gt;%\n  mutate(seq_head = paste0(\"&gt;\", seq_id)) %&gt;%\n  ungroup %&gt;%\n  select(header1=seq_head, seq1=query_seq_fwd, \n         header2=seq_head, seq2=query_seq_rev) %&gt;%\n  mutate(header1=paste0(header1, \"_1\"), header2=paste0(header2, \"_2\"))\nmrg_fasta_sep &lt;- bind_rows(select(mrg_fasta, header=header1, seq=seq1),\n                           select(mrg_fasta, header=header2, seq=seq2)) %&gt;%\n  filter(!is.na(seq))\nmrg_fasta_out &lt;- do.call(paste, c(mrg_fasta_sep, sep=\"\\n\")) %&gt;% \n  paste(collapse=\"\\n\")\nblast_dir &lt;- file.path(data_dir_base, \"blast\")\ndir.create(blast_dir, showWarnings = FALSE)\nwrite(mrg_fasta_out, file.path(blast_dir, \"putative-viral.fasta\"))\n\n\n\nCode# Import BLAST results\n# blast_results_path &lt;- file.path(data_dir_base, \"blast/putative-viral.blast.gz\")\n# blast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\n# blast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE,\n#                           col_names = blast_cols, col_types = cols(.default=\"c\"))\nblast_results_path &lt;- file.path(data_dir_base, \"blast/putative-viral-best.blast.gz\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE)\n\n# Filter for best hit for each query/subject combination\nblast_results_best &lt;- blast_results %&gt;% group_by(qseqid, staxid) %&gt;% \n  filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1)\n# write_tsv(blast_results_best, file.path(data_dir_base, \"blast/putative-viral-best.blast.gz\"))\n\n# Rank hits for each query and filter for high-ranking hits\nblast_results_ranked &lt;- blast_results_best %&gt;% \n  group_by(qseqid) %&gt;% mutate(rank = dense_rank(desc(bitscore)))\nblast_results_highrank &lt;- blast_results_ranked %&gt;% filter(rank &lt;= 5) %&gt;%\n    mutate(read_pair = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=-1), \n         seq_id = str_split(qseqid, \"_\") %&gt;% sapply(nth, n=1)) %&gt;%\n    mutate(bitscore = as.numeric(bitscore))\n\n# Summarize by read pair and taxid\nblast_results_paired &lt;- blast_results_highrank %&gt;%\n  group_by(seq_id, staxid) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore),\n            n_reads = n(), .groups = \"drop\")\n\n# Add viral status\nblast_results_viral &lt;- mutate(blast_results_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_results_assign &lt;- inner_join(blast_results_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_results_out &lt;- blast_results_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_results_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot RNA\ng_hist_1 &lt;- ggplot(mrg_blast, aes(x=adj_score_max, fill=viral_status_out)) + \n  geom_histogram(binwidth=5,boundary=0) + \n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\") +\n  theme_base\ng_hist_1\n\n\n\n\n\n\n\nFor a disjunctive score threshold of 20, the workflow achieves a measured F1 score of 98.0%.\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nLooking into the composition of different read groups, the notable observation for me is the high prevalence of Pigeon torque teno virus among high-scoring false positives, with 77 such read pairs. BLAST maps these not to viruses but to their most common hosts, i.e. assorted species of pigeon. That said, the number of false positive PTTV reads is substantially exceeded by the number of true-positive PTTV reads (1883), which do map to appropriate viruses according to BLAST, so the presence of a comparatively small number of false positives seems unlikely to cause too much distortion.\n\nCodemajor_threshold &lt;- 0.04\n\n# Add missing viral taxa\nviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\nviral_taxa$name[viral_taxa$taxid == 493803] &lt;- \"Merkel cell polyomavirus\"\nviral_taxa$name[viral_taxa$taxid == 427343] &lt;- \"Human papillomavirus 107\"\nviral_taxa$name[viral_taxa$taxid == 194958] &lt;- \"Porcine endogenous retrovirus A\"\nviral_taxa$name[viral_taxa$taxid == 340907] &lt;- \"Papiine alphaherpesvirus 2\"\nviral_taxa$name[viral_taxa$taxid == 194959] &lt;- \"Porcine endogenous retrovirus B\"\n\n\n# Prepare data\nfp &lt;- mrg_blast %&gt;% \n  group_by(viral_status_out, highscore, taxid_best) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  rename(taxid = taxid_best) %&gt;%\n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Plot\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(.~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\n\nCode# Configure\nref_taxid_ptt &lt;- 2233536\np_threshold &lt;- 0.3\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\",\n                         177155, \"Streptopelia turtur\",\n                         187126, \"Nesoenas mayeri\"\n                         )\ntax_names &lt;- tax_names_new %&gt;% filter(! staxid %in% tax_names$staxid) %&gt;%\n  bind_rows(tax_names) %&gt;% arrange(staxid)\nref_name_ptt &lt;- tax_names %&gt;% filter(staxid == ref_taxid_ptt) %&gt;% pull(name)\n\n# Get major matches\nmrg_staxid &lt;- mrg_blast %&gt;% filter(taxid_best == ref_taxid_ptt) %&gt;%\n    group_by(highscore, viral_status_out) %&gt;% mutate(n_seq = n())\nfp_staxid &lt;- mrg_staxid %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid_best=staxid), by=\"taxid_best\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, \n           taxid_best, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, taxid_best, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(status_display~score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_ptt, \" (taxid \", ref_taxid_ptt, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, date_alias, city, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | hit_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_city &lt;- read_counts %&gt;% group_by(city) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\", date_alias = \"All dates\")\nread_counts_total &lt;- read_counts_city %&gt;% group_by(sample, date_alias) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(city = \"All cities\")\nread_counts_agg &lt;- read_counts_city %&gt;% arrange(city) %&gt;%\n  bind_rows(read_counts_total) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         city = fct_inorder(city))\n\n\nApplying a disjunctive cutoff at S=20 identifies 23,191 read pairs as human-viral. This gives an overall relative HV abundance of \\(1.73 \\times 10^{-5}\\).\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=city, color=city)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_x_discrete(name=\"Collection Date\") +\n  #facet_grid(.~sample_type, scales = \"free\", space = \"free_x\") +\n  scale_color_city() + theme_rotate\ng_phv_agg\n\n\n\n\n\n\n\nThis is lower than for DNA reads from other air-sampling datasets I’ve analyzed, but not drastically so:\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Leung\", 1.73e-5, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nAt the family level, most samples across all cities are dominated by Papillomaviridae, Herpesviridae, Anelloviridae, Polyomaviridae, and to a lesser extent Poxviridae:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\n\n\nmrg_hv_named &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\n\nIn investigating individual viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads.\nAs usual, Papillomaviridae reads are divided among many different viral species. In this case, Betapapillomavirus 1 and 2 are the most prevalent across samples, but many other alpha-, beta-, gamma-, and mupapillomaviruses are highly prevalent in at least some samples.\n\nCodethreshold_major_species &lt;- 0.4\ntaxid_papilloma &lt;- 151340\n\n# Get set of Papillomaviridae reads\npapilloma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_papilloma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npapilloma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_papilloma, sample %in% papilloma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each Papillomaviridae species\npapilloma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% papilloma_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_papilloma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npapilloma_species_major_tab &lt;- papilloma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_papilloma == max(p_reads_papilloma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_papilloma)) %&gt;% \n  filter(p_reads_papilloma &gt; threshold_major_species)\npapilloma_species_counts_major &lt;- papilloma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% papilloma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_papilloma = sum(n_reads_hv),\n            p_reads_papilloma = sum(p_reads_papilloma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(papilloma_species_major_tab$name, \"Other\")))\npapilloma_species_counts_display &lt;- papilloma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_papilloma, classification = name_display)\n\n# Plot\ng_papilloma_species &lt;- g_comp_base + \n  geom_col(data=papilloma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papillomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Papillomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_papilloma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npapilloma_species_collate &lt;- papilloma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_papilloma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn terms of total reads across samples, herpesviruses are dominated by Epstein-Barr virus (Human gammaherpesvirus 4), HSV-1 (Human alphaherpesvirus 1), and human cytomegalovirus (Human betaherpesvirus 5). However, numerous other herpesviruses are also present.\n\nCodethreshold_major_species &lt;- 0.4\ntaxid_herpes &lt;- viral_taxa %&gt;% filter(name == \"Herpesviridae\") %&gt;% pull(taxid)\n\n# Get set of herpesviridae reads\nherpes_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_herpes) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nherpes_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_herpes, sample %in% herpes_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each herpesviridae species\nherpes_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn sharp contrast to the above, my pipeline classifies the great majority of anellovirus reads in all samples into a single species, torque teno virus. Looking online, it looks like there are a lot of “torque teno viruses” within Anelloviridae – for example, Wikipedia says that the genus Alphatorquevirus contains &gt;20 numbered torque teno viruses – so I’m not sure exactly which virus this refers to.\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_anello &lt;- viral_taxa %&gt;% filter(name == \"Anelloviridae\") %&gt;% pull(taxid)\n\n# Get set of anelloviridae reads\nanello_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_anello) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nanello_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_anello, sample %in% anello_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each anelloviridae species\nanello_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% anello_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_anello = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nanello_species_major_tab &lt;- anello_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_anello == max(p_reads_anello)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_anello)) %&gt;% \n  filter(p_reads_anello &gt; threshold_major_species)\nanello_species_counts_major &lt;- anello_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% anello_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_anello = sum(n_reads_hv),\n            p_reads_anello = sum(p_reads_anello), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(anello_species_major_tab$name, \"Other\")))\nanello_species_counts_display &lt;- anello_species_counts_major %&gt;%\n  rename(p_reads = p_reads_anello, classification = name_display)\n\n# Plot\ng_anello_species &lt;- g_comp_base + \n  geom_col(data=anello_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Anelloviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Anelloviridae reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_anello_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nanello_species_collate &lt;- anello_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_anello), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nPolyomaviruses are intermediate; most viruses are dominated by a single species, Alphapolyomavirus quintihominis, but several other viruses in the family are also present.\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_polyoma &lt;- viral_taxa %&gt;% filter(name == \"Polyomaviridae\") %&gt;% pull(taxid)\n\n# Get set of polyomaviridae reads\n# Get set of polyomaviridae reads\npolyoma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_polyoma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npolyoma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_polyoma, sample %in% polyoma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each polyomaviridae species\npolyoma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% polyoma_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_polyoma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npolyoma_species_major_tab &lt;- polyoma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_polyoma == max(p_reads_polyoma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_polyoma)) %&gt;% \n  filter(p_reads_polyoma &gt; threshold_major_species)\npolyoma_species_counts_major &lt;- polyoma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% polyoma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_polyoma = sum(n_reads_hv),\n            p_reads_polyoma = sum(p_reads_polyoma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(polyoma_species_major_tab$name, \"Other\")))\npolyoma_species_counts_display &lt;- polyoma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_polyoma, classification = name_display)\n\n# Plot\ng_polyoma_species &lt;- g_comp_base + \n  geom_col(data=polyoma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Polyomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Polyomaviridae reads\") +\n  guides(fill=guide_legend(ncol=2)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_polyoma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npolyoma_species_collate &lt;- polyoma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_polyoma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nFinally, poxvirus reads in most samples are dominated by molluscum contagiosum virus (which I expect to be real), followed by Orf virus (which I expect to be fake). These expectations are borne out by BLAST alignments (below).\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_pox &lt;- viral_taxa %&gt;% filter(name == \"Poxviridae\") %&gt;% pull(taxid)\n\n# Get set of poxviridae reads\n# Get set of poxviridae reads\npox_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_pox) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npox_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_pox, sample %in% pox_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each poxviridae species\npox_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% pox_ids) %&gt;%\n  group_by(sample, date_alias, city, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date_alias, city) %&gt;%\n  mutate(p_reads_pox = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npox_species_major_tab &lt;- pox_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_pox == max(p_reads_pox)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_pox)) %&gt;% \n  filter(p_reads_pox &gt; threshold_major_species)\npox_species_counts_major &lt;- pox_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% pox_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date_alias, city, name_display) %&gt;%\n  summarize(n_reads_pox = sum(n_reads_hv),\n            p_reads_pox = sum(p_reads_pox), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(pox_species_major_tab$name, \"Other\")))\npox_species_counts_display &lt;- pox_species_counts_major %&gt;%\n  rename(p_reads = p_reads_pox, classification = name_display)\n\n# Plot\ng_pox_species &lt;- g_comp_base + \n  geom_col(data=pox_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Poxviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Poxviridae reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_pox_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npox_species_collate &lt;- pox_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_pox), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCode# Configure\nref_taxids_hv &lt;- c(10279, 10258)\nref_names_hv &lt;- sapply(ref_taxids_hv, function(x) viral_taxa %&gt;% filter(taxid == x) %&gt;% pull(name) %&gt;% first)\np_threshold &lt;- 0.1\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         257877, \"Macaca thibetana thibetana\",\n                         256321, \"Lentiviral transfer vector pHsCXW\",\n                         419242, \"Shuttle vector pLvCmvMYOCDHA\",\n                         419243, \"Shuttle vector pLvCmvLacZ\",\n                         421868, \"Cloning vector pLvCmvLacZ.Gfp\",\n                         421869, \"Cloning vector pLvCmvMyocardin.Gfp\",\n                         426303, \"Lentiviral vector pNL-GFP-RRE(SA)\",\n                         436015, \"Lentiviral transfer vector pFTMGW\",\n                         454257, \"Shuttle vector pLvCmvMYOCD2aHA\",\n                         476184, \"Shuttle vector pLV.mMyoD::ERT2.eGFP\",\n                         476185, \"Shuttle vector pLV.hMyoD.eGFP\",\n                         591936, \"Piliocolobus tephrosceles\",\n                         627481, \"Lentiviral transfer vector pFTM3GW\",\n                         680261, \"Self-inactivating lentivirus vector pLV.C-EF1a.cyt-bGal.dCpG\",\n                         2952778, \"Expression vector pLV[Exp]-EGFP:T2A:Puro-EF1A\",\n                         3022699, \"Vector PAS_122122\",\n                         3025913, \"Vector pSIN-WP-mPGK-GDNF\",\n                         3105863, \"Vector pLKO.1-ZsGreen1\",\n                         3105864, \"Vector pLKO.1-ZsGreen1 mouse Wfs1 shRNA\",\n                         3108001, \"Cloning vector pLVSIN-CMV_Neo_v4.0\",\n                         3109234, \"Vector pTwist+Kan+High\",\n                         3117662, \"Cloning vector pLV[Exp]-CBA&gt;P301L\",\n                         3117663, \"Cloning vector pLV[Exp]-CBA&gt;P301L:T2A:mRuby3\",\n                         3117664, \"Cloning vector pLV[Exp]-CBA&gt;hMAPT[NM_005910.6](ns):T2A:mRuby3\",\n                         3117665, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3\",\n                         3117666, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3/NFAT3 fusion protein\",\n                         3117667, \"Cloning vector pLV[Exp]-Neo-mPGK&gt;{EGFP-hSEPT6}\",\n                         438045, \"Xenotropic MuLV-related virus\",\n                         447135, \"Myodes glareolus\",\n                         590745, \"Mus musculus mobilized endogenous polytropic provirus\",\n                         181858, \"Murine AIDS virus-related provirus\",\n                         356663, \"Xenotropic MuLV-related virus VP35\",\n                         356664, \"Xenotropic MuLV-related virus VP42\",\n                         373193, \"Xenotropic MuLV-related virus VP62\",\n                         286419, \"Canis lupus dingo\",\n                         415978, \"Sus scrofa scrofa\",\n                         494514, \"Vulpes lagopus\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\n# Get matches\nhv_blast_staxids &lt;- hv_reads_species %&gt;% filter(taxid %in% ref_taxids_hv) %&gt;%\n  group_by(taxid) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_results_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names %&gt;% rename(sname=name), by=\"staxid\")\n\n# Count matches\nhv_blast_counts &lt;- hv_blast_staxids %&gt;%\n  group_by(taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;% mutate(p=n/n_seq)\n\n# Subset to major matches\nhv_blast_counts_major &lt;- hv_blast_counts %&gt;% \n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  arrange(desc(p)) %&gt;% group_by(taxid) %&gt;%\n  filter(row_number() &lt;= 25) %&gt;%\n  mutate(name_display = ifelse(name == ref_names_hv[1], \"MCV\", name))\n\n# Plot\ng_hv_blast &lt;- ggplot(hv_blast_counts_major, mapping=aes(x=p, y=sname)) +\n  geom_col() +\n  facet_grid(name_display~., scales=\"free_y\", space=\"free_y\") +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), \n                     breaks=seq(0,1,0.2), expand=c(0,0)) +\n  theme_base + theme(axis.title.y = element_blank())\ng_hv_blast\n\n\n\n\n\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nn_path_genera &lt;- hv_reads_genus %&gt;% \n  group_by(sample, date_alias, city, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"date_alias\", \"city\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type, city) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", date=\"All dates\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=city)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_city() +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\nConclusion\nThis is the third, largest, and final of this tranche of air-sampling datasets that I’ve run through this pipeline. Many of the high-level findings were similar to Prussin and Rosario, including high relative abundance of human reads, low total viral reads, an absence of enteric viruses, and high abundance of papillomaviruses among human-infecting viruses.\nIn the future, I’ll do a more in-depth comparative analysis across different datasets to compare the abundance of different viruses. For now, though, there are some major updates to the pipeline I want to make before I do any more public analyses."
  },
  {
    "objectID": "notebooks/2024-04-30_brinch.html",
    "href": "notebooks/2024-04-30_brinch.html",
    "title": "Workflow analysis of Brinch et al. (2020)",
    "section": "",
    "text": "In this entry, I’m analyzing Brinch et al. (2020), a large DNA-sequencing study of raw influent samples from three Copenhagen treatment plants. 12-hour composite samples were collected “at irregular intervals” between 2015 and 2018; the sample processing protocol isn’t described in great detail, but samples were centrifuged and the pellet retained for extraction, a procedure that (as in Bengtsson-Palme) we expect to select against viruses. About half the samples were sequenced on an Illumina MiSeq, the rest on a NextSeq; “the change of sequencing platform was circumstantial and was not part of the experiment design.” The platform used varied in temporal blocks (blue = NextSeq):\n\nThe raw data\nThe Brinch dataset contained sample information from 322 samples, composed as follows:\n\nCode# Importing the data is a bit more complicated this time as the samples are split across two pipeline runs\ndata_dir_base &lt;- \"../data/2024-04-30_brinch\"\ndata_dirs &lt;- paste(data_dir_base, c(1,2), sep=\"/\")\n\n# Data input paths\nlibraries_paths &lt;- file.path(data_dirs, \"sample-metadata.csv\")\nbasic_stats_paths &lt;- file.path(data_dirs, \"qc_basic_stats.tsv.gz\")\nadapter_stats_paths &lt;- file.path(data_dirs, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_paths &lt;- file.path(data_dirs, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_paths &lt;- file.path(data_dirs, \"qc_quality_sequence_stats.tsv.gz\")\ninstrument_path &lt;- file.path(data_dir_base, \"instruments.csv\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- lapply(libraries_paths, read_csv, show_col_types = FALSE) %&gt;% bind_rows\ninstruments &lt;- read_csv(instrument_path, show_col_types = FALSE) %&gt;%\n  mutate(instrument = sub(\"^Illumina \", \"\", instrument))\nlibraries &lt;- libraries_raw %&gt;%\n  arrange(location) %&gt;% mutate(location = fct_inorder(location)) %&gt;%\n  mutate(year = year(date),\n         month = gsub(\"-\\\\d+$\", \"\", date)) %&gt;%\n  arrange(location, date, sample) %&gt;%\n  mutate(sample=fct_inorder(sample),\n         location_alias = ifelse(location == \"Amager\", \"RL\",\n                                 ifelse(location == \"Valby\", \"RD\", \"RA\"))) %&gt;%\n  group_by(location, month) %&gt;%\n  mutate(sample_alias = paste(location_alias, month, row_number(), sep=\"_\"),\n         sample_alias = fct_inorder(sample_alias)) %&gt;%\n  left_join(instruments, by=\"library\")\n         \n# Make table\ncount_samples &lt;- libraries %&gt;% group_by(location, year) %&gt;% count %&gt;%\n  pivot_wider(names_from = \"year\", values_from=\"n\") %&gt;%\n  rename(Location = location)\ncount_samples\n\n\n  \n\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;%\n    arrange(location, sample_alias, sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_paths)\nadapter_stats &lt;- import_basic_paired(adapter_stats_paths)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_paths)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_paths)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% group_by(instrument) %&gt;% \n  summarize(n_lib = n(), rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThe 169 NextSeq libraries yielded 3.6M-63M (mean 23.8M) read pairs per sample, while the 153 MiSeq libraries yielded 10-7.4M (mean 2.3M) read pairs. The total number of read pairs across all instruments was 4.4B read pairs (1.27 terabases of sequence), of which 93% of read pairs (89% of bases) came from NextSeq runs.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample, date, location, instrument,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample:instrument), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_loc &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", name=\"Location\")\nscale_fill_ins &lt;- purrr::partial(scale_fill_brewer, palette=\"Dark2\", name=\"Instrument\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=date, y=value, fill=instrument, group=interaction(location,date))) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_ins() + \n  facet_grid(metric~location, scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_kit + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\nIn most samples, read qualities were high at the 5’ end of the read but tailed off substantially over the course of the read, especially in long MiSeq reads. A minority of MiSeq samples had poor read quality across the length of the read. Adapter levels were fairly low, but significant enough to require trimming. Duplicate levels were low (0-11%, mean 3%) in MiSeq libraries (unsurprising given their small size) but moderate (3-59%, mean 28%) in the larger NextSeq libraries.\n\nCode# Set up plotting templates\nscale_color_loc &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\", name=\"Location\")\nscale_color_ins &lt;- purrr::partial(scale_color_brewer, palette=\"Dark2\", name=\"Instrument\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=instrument, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_ins() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\nrmax &lt;- 240\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,1), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,rmax,20), expand=c(0,0)) +\n  facet_grid(.~adapter) + theme(axis.text.x = element_text(angle=45, hjust=1))\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(0,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,rmax,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. As expected given the observed difference in duplication levels, few reads were lost during deduplication in MiSeq libraries, while a somewhat larger (but still not huge) number were lost in NextSeq libraries. Very few reads were lost during ribodepletion, as expected for DNA sequencing libraries.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, sample_alias, location, date, instrument, \n         percent_duplicates, n_read_pairs, stage) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  group_by(Instrument=instrument, Stage=stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=instrument, group=sample)) +\n  scale_color_ins() +\n  facet_wrap(~location, scales=\"free\", ncol=3) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=instrument, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_ins() +\n  facet_wrap(~location, scales=\"free\", ncol=3) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and mostly successful at improving read qualities, though some libraries still dipped below my preferred quality thresholds later in the read:\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=instrument, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_ins() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels, which fell from an average of 28% to 8% in NextSeq libraries and from 3% to 1% in MiSeq samples:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(instrument,stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCodeclassifications &lt;- c(\"Filtered\", \"Duplicate\", \"Ribosomal\", \"Unassigned\",\n                     \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# Import composition data\ncomp_paths &lt;- file.path(data_dirs, \"taxonomic_composition.tsv.gz\")\ncomp &lt;- lapply(comp_paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n  left_join(libraries, by=\"sample\") %&gt;%\n  mutate(classification = factor(classification, levels = classifications),\n         n_reads = replace_na(n_reads, 0)) %&gt;%\n  group_by(sample) %&gt;% mutate(p_reads = n_reads/sum(n_reads))\n  \n# Summarize composition\nread_comp_summ &lt;- comp %&gt;% \n  group_by(location, instrument, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=sample, y=p_reads, fill=classification)) +\n  facet_wrap(instrument~location, scales = \"free_x\", ncol=3,\n             labeller = label_wrap_gen(multi_line=FALSE, width=20)) +\n  theme_xblank\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = comp, position = \"stack\", width=1) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\ncomp_minor &lt;- comp %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + geom_col(data=comp_minor, position = \"stack\", width=1) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- comp %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, instrument) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, instrument) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(instrument, classification, read_fraction=display) %&gt;%\n  arrange(instrument, classification)\np_reads_summ\n\n\n  \n\n\n\nIn most samples, the majority of reads were either filtered, duplicates, or unassigned. Among assigned reads, the vast majority in most samples were bacterial, which is unsurprising given the sample processing protocols used. A few samples showed elevated levels of human reads, which might be real or due to experimenter contamination. Viral fraction averaged 0.41% in MiSeq reads and 0.34% in NextSeq reads.\nAs is common for DNA data, viral reads were overwhelmingly dominated by Caudoviricetes phages:\n\nCode# Get Kraken reports\nreports_paths &lt;- file.path(data_dirs, \"kraken_reports.tsv.gz\")\nreports &lt;- lapply(reports_paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows\n\n# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir_base, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.02\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, instrument, location, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, instrument, location) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, instrument, location, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. A total of 17,448 reads were identified as putatively human-viral:\n\nCode# Import HV read data\nhv_reads_filtered_paths &lt;- file.path(data_dirs, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_paths, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  left_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, location, instrument, seq_id) %&gt;% count %&gt;%\n  group_by(sample, location, instrument) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\n# Plot results\ngeom_vhist &lt;- purrr::partial(geom_histogram, binwidth=5, boundary=0)\ng_vhist_base &lt;- ggplot(mapping=aes(x=adj_score_max)) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_vhist_0 &lt;- g_vhist_base + geom_vhist(data=mrg)\ng_vhist_0\n\n\n\n\n\n\n\nBLASTing these reads against nt:\n\nCode# Import paired BLAST results\nblast_paired_paths &lt;- file.path(data_dirs, \"hv_hits_blast_paired.tsv.gz\")\nblast_paired &lt;- lapply(blast_paired_paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows\n\n# Add viral status\nblast_viral &lt;- mutate(blast_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_assign &lt;- inner_join(blast_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_out &lt;- blast_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot RNA\ng_vhist_1 &lt;- g_vhist_base + geom_vhist(data=mrg_blast, mapping=aes(fill=viral_status_out)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\")\ng_vhist_1\n\n\n\n\n\n\n\nApplying my standard disjunctive score threshold of 20 achieved precision, sensitivity and F1 scores all &gt;99%:\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold (Disjunctive)\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nLooking into the composition of different read groups, a plurality of high-scoring false positives map to human alphaherpesvirus 1 strain RH2 according to Bowtie2. BLASTN maps these sequences to a variety of bacterial taxa, especially E. coli and various Klebsiella species. I’m already screening against one strain of E. coli, so if I wanted to tackle this I’d need to add more strains or another species, probably Klebsiella pneumoniae. However, in this case the scores are good enough that I don’t think it’s worth it.\n\nCodemajor_threshold &lt;- 0.04\n\n# Add missing viral taxa\nviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\nviral_taxa$name[viral_taxa$taxid == 493803] &lt;- \"Merkel cell polyomavirus\"\nviral_taxa$name[viral_taxa$taxid == 427343] &lt;- \"Human papillomavirus 107\"\nviral_taxa$name[viral_taxa$taxid == 194958] &lt;- \"Porcine endogenous retrovirus A\"\nviral_taxa$name[viral_taxa$taxid == 340907] &lt;- \"Papiine alphaherpesvirus 2\"\nviral_taxa$name[viral_taxa$taxid == 194959] &lt;- \"Porcine endogenous retrovirus B\"\n\n\n# Prepare data\nfp &lt;- mrg_blast %&gt;% \n  group_by(viral_status_out, highscore, taxid_best) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  rename(taxid = taxid_best) %&gt;%\n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Plot\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(.~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\n\nCode# Configure\nref_taxid_rh2 &lt;- 946522\np_threshold &lt;- 0.3\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\",\n                         177155, \"Streptopelia turtur\",\n                         187126, \"Nesoenas mayeri\",\n                         244366, \"Klebsiella variicola\",\n                         208224, \"Enterobacter kobei\"\n                         )\ntax_names &lt;- tax_names_new %&gt;% filter(! staxid %in% tax_names$staxid) %&gt;%\n  bind_rows(tax_names) %&gt;% arrange(staxid)\nref_name_rh2 &lt;- tax_names %&gt;% filter(staxid == ref_taxid_rh2) %&gt;% pull(name)\n\n# Get major matches\nmrg_staxid &lt;- mrg_blast %&gt;% filter(taxid_best == ref_taxid_rh2) %&gt;%\n    group_by(highscore, viral_status_out) %&gt;% mutate(n_seq = n())\nfp_staxid &lt;- mrg_staxid %&gt;%\n  left_join(blast_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid_best=staxid), by=\"taxid_best\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, \n           taxid_best, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, taxid_best, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(status_display~score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_rh2, \" (taxid \", ref_taxid_rh2, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, location, instrument, date, year, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_grp &lt;- read_counts %&gt;% group_by(location, year) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\", year = as.character(year))\nread_counts_yr &lt;- read_counts_grp %&gt;% group_by(sample, year) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(location = \"All locations\")\nread_counts_loc &lt;- read_counts_grp %&gt;%\n  group_by(sample, location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(year = \"All years\")\nread_counts_tot &lt;- read_counts_loc %&gt;% group_by(sample, year) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(location = \"All locations\")\nread_counts_agg &lt;- bind_rows(read_counts_grp, read_counts_yr,\n                             read_counts_loc, read_counts_tot) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         location = factor(location, levels = c(levels(libraries$location), \"All locations\")),\n         year = factor(year, levels = c(as.character(2015:2018), \"All years\")))\n\n\nApplying a disjunctive cutoff at S=20 identifies 16,967 read pairs as human-viral. This gives an overall relative HV abundance of \\(3.88 \\times 10^{-6}\\). Measured RAs are somewhat lower on average in 2015, and higher than average in 2016, but overall year-on-year variation isn’t huge on a log scale:\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=year, color=location)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\", limits=c(1e-6, 1e-5)) +\n  scale_color_loc() + theme_kit\ng_phv_agg\n\n\n\n\n\n\n\nThis overall RA is somewhat lower than most RNA datasets I’ve previously analyzed, and on the low end for DNA datasets as well, probably due to the antiviral selection of the sample processing protocol used:\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE,\n                   \"Leung\", 1.73e-5, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Brinch\", 3.88e-6, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nIn investigating the taxonomy of human-infecting virus reads, I restricted my analysis to samples with more than 5 HV read pairs total across all viruses, to reduce noise arising from extremely low HV read counts in some samples. 15 samples, 4 from influent and 11 from primary sludge, met this criterion.\nAt the family level, most samples across all locations were overwhelmingly dominated by Adenoviridae, followed distantly by Polyomaviridae, Papillomaviridae and Poxviridae:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\nviral_taxa$name[viral_taxa$taxid == 333930] &lt;- \"Gammapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 337048] &lt;- \"Alphapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337041] &lt;- \"Alphapapillomavirus 9\"\nviral_taxa$name[viral_taxa$taxid == 337049] &lt;- \"Alphapapillomavirus 11\"\nviral_taxa$name[viral_taxa$taxid == 337044] &lt;- \"Alphapapillomavirus 5\"\n\n\n# Filter samples and add viral taxa information\nsamples_keep &lt;- read_counts %&gt;% filter(n_reads_hv &gt; 5) %&gt;% pull(sample)\nmrg_hv_named &lt;- mrg_hv %&gt;% filter(sample %in% samples_keep) %&gt;% left_join(viral_taxa, by=\"taxid\") \n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.08\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, location, instrument, year, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument, year) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, year, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nAdenoviruses were in turn overwhelmingly dominated by human mastadenovirus F:\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_adeno &lt;- 10508\n\n# Get set of adenoviridae reads\nadeno_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_adeno) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nadeno_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_adeno, sample %in% adeno_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each adenoviridae species\nadeno_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% adeno_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_adeno = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nadeno_species_major_tab &lt;- adeno_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_adeno == max(p_reads_adeno)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_adeno)) %&gt;% \n  filter(p_reads_adeno &gt; threshold_major_species)\nadeno_species_counts_major &lt;- adeno_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% adeno_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_adeno = sum(n_reads_hv),\n            p_reads_adeno = sum(p_reads_adeno), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(adeno_species_major_tab$name, \"Other\")))\nadeno_species_counts_display &lt;- adeno_species_counts_major %&gt;%\n  rename(p_reads = p_reads_adeno, classification = name_display)\n\n# Plot\ng_adeno_species &lt;- g_comp_base + \n  geom_col(data=adeno_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Adenoviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Adenoviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_adeno_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nadeno_species_collate &lt;- adeno_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_adeno), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCode# Configure\nref_taxids_hv &lt;- c(130309)\nref_names_hv &lt;- sapply(ref_taxids_hv, function(x) viral_taxa %&gt;% filter(taxid == x) %&gt;% pull(name) %&gt;% first)\np_threshold &lt;- 0.1\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir_base, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         257877, \"Macaca thibetana thibetana\",\n                         256321, \"Lentiviral transfer vector pHsCXW\",\n                         419242, \"Shuttle vector pLvCmvMYOCDHA\",\n                         419243, \"Shuttle vector pLvCmvLacZ\",\n                         421868, \"Cloning vector pLvCmvLacZ.Gfp\",\n                         421869, \"Cloning vector pLvCmvMyocardin.Gfp\",\n                         426303, \"Lentiviral vector pNL-GFP-RRE(SA)\",\n                         436015, \"Lentiviral transfer vector pFTMGW\",\n                         454257, \"Shuttle vector pLvCmvMYOCD2aHA\",\n                         476184, \"Shuttle vector pLV.mMyoD::ERT2.eGFP\",\n                         476185, \"Shuttle vector pLV.hMyoD.eGFP\",\n                         591936, \"Piliocolobus tephrosceles\",\n                         627481, \"Lentiviral transfer vector pFTM3GW\",\n                         680261, \"Self-inactivating lentivirus vector pLV.C-EF1a.cyt-bGal.dCpG\",\n                         2952778, \"Expression vector pLV[Exp]-EGFP:T2A:Puro-EF1A\",\n                         3022699, \"Vector PAS_122122\",\n                         3025913, \"Vector pSIN-WP-mPGK-GDNF\",\n                         3105863, \"Vector pLKO.1-ZsGreen1\",\n                         3105864, \"Vector pLKO.1-ZsGreen1 mouse Wfs1 shRNA\",\n                         3108001, \"Cloning vector pLVSIN-CMV_Neo_v4.0\",\n                         3109234, \"Vector pTwist+Kan+High\",\n                         3117662, \"Cloning vector pLV[Exp]-CBA&gt;P301L\",\n                         3117663, \"Cloning vector pLV[Exp]-CBA&gt;P301L:T2A:mRuby3\",\n                         3117664, \"Cloning vector pLV[Exp]-CBA&gt;hMAPT[NM_005910.6](ns):T2A:mRuby3\",\n                         3117665, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3\",\n                         3117666, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3/NFAT3 fusion protein\",\n                         3117667, \"Cloning vector pLV[Exp]-Neo-mPGK&gt;{EGFP-hSEPT6}\",\n                         438045, \"Xenotropic MuLV-related virus\",\n                         447135, \"Myodes glareolus\",\n                         590745, \"Mus musculus mobilized endogenous polytropic provirus\",\n                         181858, \"Murine AIDS virus-related provirus\",\n                         356663, \"Xenotropic MuLV-related virus VP35\",\n                         356664, \"Xenotropic MuLV-related virus VP42\",\n                         373193, \"Xenotropic MuLV-related virus VP62\",\n                         286419, \"Canis lupus dingo\",\n                         415978, \"Sus scrofa scrofa\",\n                         494514, \"Vulpes lagopus\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\n# Get matches\nhv_blast_staxids &lt;- hv_reads_species %&gt;% filter(taxid %in% ref_taxids_hv) %&gt;%\n  group_by(taxid) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names %&gt;% rename(sname=name), by=\"staxid\")\n\n# Count matches\nhv_blast_counts &lt;- hv_blast_staxids %&gt;%\n  group_by(taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;% mutate(p=n/n_seq)\n\n# Subset to major matches\nhv_blast_counts_major &lt;- hv_blast_counts %&gt;% \n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  arrange(desc(p)) %&gt;% group_by(taxid) %&gt;%\n  filter(row_number() &lt;= 25) %&gt;%\n  mutate(name_display = name)\n\n# Plot\ng_hv_blast &lt;- ggplot(hv_blast_counts_major, mapping=aes(x=p, y=sname)) +\n  geom_col() +\n  facet_grid(name_display~., scales=\"free_y\", space=\"free_y\") +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), \n                     breaks=seq(0,1,0.2), expand=c(0,0)) +\n  theme_base + theme(axis.title.y = element_blank())\ng_hv_blast\n\n\n\n\n\n\n\nIn investigating more minor viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads:\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_polyoma &lt;- 151341\n\n# Get set of polyomaviridae reads\npolyoma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_polyoma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npolyoma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_polyoma, sample %in% polyoma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each polyomaviridae species\npolyoma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% polyoma_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_polyoma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npolyoma_species_major_tab &lt;- polyoma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_polyoma == max(p_reads_polyoma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_polyoma)) %&gt;% \n  filter(p_reads_polyoma &gt; threshold_major_species)\npolyoma_species_counts_major &lt;- polyoma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% polyoma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_polyoma = sum(n_reads_hv),\n            p_reads_polyoma = sum(p_reads_polyoma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(polyoma_species_major_tab$name, \"Other\")))\npolyoma_species_counts_display &lt;- polyoma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_polyoma, classification = name_display)\n\n# Plot\ng_polyoma_species &lt;- g_comp_base + \n  geom_col(data=polyoma_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Polyomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Polyomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_polyoma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npolyoma_species_collate &lt;- polyoma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_polyoma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.35\ntaxid_papilloma &lt;- 151340\n\n# Get set of papillomaviridae reads\npapilloma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_papilloma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npapilloma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_papilloma, sample %in% papilloma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each papillomaviridae species\npapilloma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% papilloma_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_papilloma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npapilloma_species_major_tab &lt;- papilloma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_papilloma == max(p_reads_papilloma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_papilloma)) %&gt;% \n  filter(p_reads_papilloma &gt; threshold_major_species)\npapilloma_species_counts_major &lt;- papilloma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% papilloma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_papilloma = sum(n_reads_hv),\n            p_reads_papilloma = sum(p_reads_papilloma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(papilloma_species_major_tab$name, \"Other\")))\npapilloma_species_counts_display &lt;- papilloma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_papilloma, classification = name_display)\n\n# Plot\ng_papilloma_species &lt;- g_comp_base + \n  geom_col(data=papilloma_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Papillomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Papillomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_papilloma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npapilloma_species_collate &lt;- papilloma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_papilloma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_pox &lt;- 10240\n\n# Get set of poxviridae reads\npox_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_pox) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npox_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_pox, sample %in% pox_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each poxviridae species\npox_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% pox_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_pox = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npox_species_major_tab &lt;- pox_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_pox == max(p_reads_pox)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_pox)) %&gt;% \n  filter(p_reads_pox &gt; threshold_major_species)\npox_species_counts_major &lt;- pox_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% pox_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_pox = sum(n_reads_hv),\n            p_reads_pox = sum(p_reads_pox), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(pox_species_major_tab$name, \"Other\")))\npox_species_counts_display &lt;- pox_species_counts_major %&gt;%\n  rename(p_reads = p_reads_pox, classification = name_display)\n\n# Plot\ng_pox_species &lt;- g_comp_base + \n  geom_col(data=pox_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Poxviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Poxviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_pox_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npox_species_collate &lt;- pox_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_pox), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_herpes &lt;- 10292\n\n# Get set of herpesviridae reads\nherpes_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_herpes) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nherpes_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_herpes, sample %in% herpes_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each herpesviridae species\nherpes_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn the latter case, some subset of human alphaherpesvirus 1 reads will be arising from strain RH2, which we know to be dominated by false positive hits. Digging deeper, we see that these are essentially all of the alphaherpesvirus 1 reads. Hence, these herpesvirus read counts need to be treated with significant skepticism, at least until the RH2 problem is fixed.\n\nCode# Count reads for each herpesviridae taxon\nherpes_species_counts &lt;- mrg_hv_named %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, location, instrument, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, instrument) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, instrument, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry. Note that, as Simplexvirus includes human alphaherpesvirus 1, that count should be treated with significant caution until the RH2 problem is fixed.\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nmrg_hv_named_all &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\nhv_reads_genus_all &lt;- raise_rank(mrg_hv_named_all, viral_taxa, \"genus\")\nn_path_genera &lt;- hv_reads_genus_all %&gt;% \n  group_by(sample, location, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"location\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type, location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=location)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_loc() +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\nConclusion\nThis is the first major DNA dataset from wastewater that I’ve analyzed using this pipeline, and at first I was nervous about how it would perform. Most of the optimization I’ve done on the HV detection pipeline has been on RNA datasets, so I was concerned that we would see major problems with false positives that we haven’t encountered with other datasets. In the event, however, things went well. There were some preventable false positives (especially from human alphaherpesvirus 1 strain RH2) but they were rare enough relative to correct assignments that the pipeline’s overall performance scores were very high. Overall I wouldn’t trust the performance of this pipeline on DNA data much less than on RNA.\nIn terms of the results themselves, it was interesting to see the remarkable preponderance of adenoviruses, and especially human mastadenovirus F, among human-infecting viruses. This isn’t a pattern we’ve seen in RNA data, and it will be interesting to see if it persists in other DNA datasets."
  },
  {
    "objectID": "notebooks/2024-05-01_bengtsson-palme.html",
    "href": "notebooks/2024-05-01_bengtsson-palme.html",
    "title": "Workflow analysis of Bengtsson-Palme et al. (2016)",
    "section": "",
    "text": "In this entry, I’m analyzing Bengtsson-Palme et al. (2016), a study of grab samples taken from three treatment plants in Sweden in September 2012. This was a DNA-sequencing study focused on investigating AMR in sewage communities. Influent, effluent and sludge samples were taken from each location, with a total of 70 samples across all sites: 20 influent, 10 effluent and 40 sludge. Liquid samples were filtered through a 1mm seive, then centrifuged, retaining the pellet (note that we expect this to select against viruses), which was then resuspended and underwent DNA extraction. Sludge samples weren’t concentrated, but went directly to DNA extraction. Samples were sequenced on an Illumina HiSeq 2500, producing 2x101bp reads.\nThe raw data\nThe sample composition of the Bengtsson-Palme dataset was as follows:\n\nCode# Importing the data is a bit more complicated this time as the samples are split across three pipeline runs\ndata_dir &lt;- \"../data/2024-04-30_bengtsson-palme\"\n\n# Data input paths\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv.gz\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv.gz\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- lapply(libraries_path, read_csv, show_col_types = FALSE) %&gt;% bind_rows\nlibraries &lt;- libraries_raw %&gt;%\n  # Process sample types\n  mutate(sample_group = ifelse(grepl(\"Inlet\", sample_type), \"Influent\",\n                               ifelse(grepl(\"Primary\", sample_type), \"Sludge (Primary/Surplus)\",\n                                      ifelse(grepl(\"Surplus\", sample_type), \"Sludge (Primary/Surplus)\",\n                                             ifelse(grepl(\"Digested\", sample_type), \"Sludge (Other)\",\n                                                    ifelse(grepl(\"Kemikond\", sample_type), \"Sludge (Other)\",\n                                                           \"Effluent\"))))),\n         sample_group = factor(sample_group, levels = c(\"Influent\", \"Effluent\", \"Sludge (Primary/Surplus)\", \n                                                        \"Sludge (Other)\")),\n         sludge = grepl(\"Sludge\", sample_group)) %&gt;%\n  arrange(location, sample_group, sample) %&gt;%\n  mutate(location = fct_inorder(location),\n         sample = fct_inorder(sample))\n\n# Make table\ncount_samples &lt;- libraries %&gt;% group_by(sample_group, location) %&gt;% count %&gt;%\n  pivot_wider(names_from = \"location\", values_from=\"n\") %&gt;%\n  rename(`Sample Type`=sample_group)\ncount_samples\n\n\n  \n\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;%\n      arrange(location, sample_group, sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_path)\nadapter_stats &lt;- import_basic_paired(adapter_stats_path)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_path)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_path)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% ungroup %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThese 70 samples yielded 23.7M-61.3M (mean 38.7M) reads per sample, for a total of 2.7B read pairs (539 gigabases of sequence). Read qualities were mostly high but tailed off towards the 3’ end, requiring some trimming. Adapter levels were high. Inferred duplication levels were low in sludge samples (1-12%, mean 4%) but much higher in liquid samples (22-90%, mean 46%), implying lower available sequence diversity in the latter sample groups.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample, sample_group, location,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample:location), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_st &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", name=\"Sample Type\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=sample, y=value, fill=sample_group, group=interaction(sample_group,sample))) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_st() + \n  facet_grid(metric~location, scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_xblank + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_st &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                   name=\"Sample Type\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=sample_group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_st() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. As expected given the observed difference in duplication levels, many more reads were lost during deduplication in liquid samples than sludge samples. Conversely, trimming and filtering consistently removed more reads in sludge than in liquid samples, though the effect was less dramatic than for deduplication. Very few reads were lost during ribodepletion, as expected for DNA sequencing libraries.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, location, sample_group, sludge, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  group_by(`Sludge?`=sludge, Stage=stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, color=sample_group, group=sample)) +\n  scale_color_st() +\n  facet_wrap(~location, scales=\"free\", ncol=3) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, aes(x=stage, color=sample_group, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  scale_color_st() +\n  facet_wrap(~location, scales=\"free\", ncol=3) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and improving read qualities:\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=sample_group, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_st() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels, which fell from an average of 45% to 5% in liquid samples and from 6% to 3% in sludge samples:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(sludge,stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCodeclassifications &lt;- c(\"Filtered\", \"Duplicate\", \"Ribosomal\", \"Unassigned\",\n                     \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# Import composition data\ncomp_path &lt;- file.path(data_dir, \"taxonomic_composition.tsv.gz\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  left_join(libraries, by=\"sample\") %&gt;%\n  mutate(classification = factor(classification, levels = classifications))\n  \n\n# Summarize composition\nread_comp_summ &lt;- comp %&gt;% \n  group_by(location, sample_group, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=sample, y=p_reads, fill=classification)) +\n  facet_wrap(location~sample_group, scales = \"free_x\", ncol=4,\n             labeller = label_wrap_gen(multi_line=FALSE, width=20)) +\n  theme_xblank\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = comp, position = \"stack\", width=1) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\ncomp_minor &lt;- comp %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_col(data=comp_minor, position = \"stack\", width=1) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- comp %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, sample_group) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, sample_group) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(sample_group, classification, read_fraction=display) %&gt;%\n  arrange(sample_group, classification)\np_reads_summ\n\n\n  \n\n\n\nIn all sample types, the majority of reads were either filtered, duplicates, or unassigned. Among assigned reads, the vast majority were bacterial, which is unsurprising given the sample processing protocols used. Total viral fraction averaged 0.011% in influent and primary sludge, and considerably lower in effluent and treated sludge. The human fraction was also low, averaging 0.033% across all sample types. Interestingly, treated sludge showed far higher fractions of archaeal reads than other sample types, possibly due to the anaerobic conditions experienced during sludge treatment.\nAs is common for DNA data, viral reads were overwhelmingly dominated by Caudoviricetes phages:\n\nCode# Get Kraken reports\nreports_path &lt;- file.path(data_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE)\n\n# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.02\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, sample_group, location, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, sample_group, location) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, sample_group, location, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. A grand total of 565 reads were identified as putatively human-viral, with many samples showing fewer than 5 total HV read pairs. Even this total likely overstates total human-viral presence, however, as more than 300 of these reads had only low alignment scores to their putative viral sources:\n\nCode# Import HV read data\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_path, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  left_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, location, sample_group, seq_id) %&gt;% count %&gt;%\n  group_by(sample, location, sample_group) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\n# Plot results\ngeom_vhist &lt;- purrr::partial(geom_histogram, binwidth=5, boundary=0)\ng_vhist_base &lt;- ggplot(mapping=aes(x=adj_score_max)) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_vhist_0 &lt;- g_vhist_base + geom_vhist(data=mrg)\ng_vhist_0\n\n\n\n\n\n\n\nBLASTing these reads against nt:\n\nCode# Import paired BLAST results\nblast_paired_path &lt;- file.path(data_dir, \"hv_hits_blast_paired.tsv.gz\")\nblast_paired &lt;- read_tsv(blast_paired_path, show_col_types = FALSE)\n\n# Add viral status\nblast_viral &lt;- mutate(blast_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_assign &lt;- inner_join(blast_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_out &lt;- blast_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_vhist_1 &lt;- g_vhist_base + geom_vhist(data=mrg_blast, mapping=aes(fill=viral_status_out)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\")\ng_vhist_1\n\n\n\n\n\n\n\nIn order to achieve decent performance metrics under these conditions, I needed to exclude low-scoring reads with Kraken hits as well as those without. Doing this at my normal disjunctive score threshold of 20 gave passable precision (93%) but poor sensitivity (79%), leading to a poor overall F1 score (85%):\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nLooking into the composition of different read groups, the bulk of high-scoring false positives map to human alphaherpesvirus 1 strain RH2 according to Bowtie2. BLASTN maps these sequences to a variety of bacterial taxa, especially E. coli and various Klebsiella species.\n\nCodemajor_threshold &lt;- 0.05\n\n# Add missing viral taxa\nviral_taxa$name[viral_taxa$taxid == 211787] &lt;- \"Human papillomavirus type 92\"\nviral_taxa$name[viral_taxa$taxid == 509154] &lt;- \"Porcine endogenous retrovirus C\"\nviral_taxa$name[viral_taxa$taxid == 493803] &lt;- \"Merkel cell polyomavirus\"\nviral_taxa$name[viral_taxa$taxid == 427343] &lt;- \"Human papillomavirus 107\"\nviral_taxa$name[viral_taxa$taxid == 194958] &lt;- \"Porcine endogenous retrovirus A\"\nviral_taxa$name[viral_taxa$taxid == 340907] &lt;- \"Papiine alphaherpesvirus 2\"\nviral_taxa$name[viral_taxa$taxid == 194959] &lt;- \"Porcine endogenous retrovirus B\"\n\n\n# Prepare data\nfp &lt;- mrg_blast %&gt;% \n  group_by(viral_status_out, highscore, taxid_best) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  rename(taxid = taxid_best) %&gt;%\n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p))\nfp_major_tab &lt;- fp %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp_major_list &lt;- fp_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp_major &lt;- fp %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Plot\ng_fp &lt;- ggplot(fp_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(.~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp\n\n\n\n\n\n\n\n\nCode# Configure\nref_taxid_rh2 &lt;- 946522\np_threshold &lt;- 0.3\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\",\n                         177155, \"Streptopelia turtur\",\n                         187126, \"Nesoenas mayeri\",\n                         244366, \"Klebsiella variicola\",\n                         )\ntax_names &lt;- tax_names_new %&gt;% filter(! staxid %in% tax_names$staxid) %&gt;%\n  bind_rows(tax_names) %&gt;% arrange(staxid)\nref_name_rh2 &lt;- tax_names %&gt;% filter(staxid == ref_taxid_rh2) %&gt;% pull(name)\n\n# Get major matches\nmrg_staxid &lt;- mrg_blast %&gt;% filter(taxid_best == ref_taxid_rh2) %&gt;%\n    group_by(highscore, viral_status_out) %&gt;% mutate(n_seq = n())\nfp_staxid &lt;- mrg_staxid %&gt;%\n  left_join(blast_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names, by=\"staxid\") %&gt;% rename(sname=name) %&gt;%\n  left_join(tax_names %&gt;% rename(taxid_best=staxid), by=\"taxid_best\")\nfp_staxid_count &lt;- fp_staxid %&gt;%\n  group_by(viral_status_out, highscore, \n           taxid_best, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;%\n  group_by(viral_status_out, highscore, taxid_best, name) %&gt;%\n  mutate(p=n/n_seq)\nfp_staxid_count_major &lt;- fp_staxid_count %&gt;%\n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  mutate(score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \n                                 \"True positive\", \"False positive\"))\n\n# Plot\ng &lt;- ggplot(fp_staxid_count_major, aes(x=p, y=sname)) + \n  geom_col() + \n  facet_grid(status_display~score_display, scales=\"free\",\n             labeller = label_wrap_gen(multi_line = FALSE)) +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), breaks=seq(0,1,0.2),\n                     expand=c(0,0)) +\n  labs(title=paste0(ref_name_rh2, \" (taxid \", ref_taxid_rh2, \")\")) +\n  theme_base + theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng\n\n\n\n\n\n\n\nThis is the second DNA wastewater dataset I’ve run (along with Brinch) where alphaherpesvirus 1 strain RH2 represents a large fraction of high-scoring false-positives. In both datasets, all of these reads are mapped by Bowtie to a single reference genome, with ID AB618031.1. This is just one of over 70 HSV-1 genomes present in our reference database. As such, I decided to try removing this genome from the database and re-running the analysis to see if this reduced the number of high-scoring false positives.\nRepeating the analysis with this modification reduces the number of putative HV reads by 33, increases precision from 93% to 98%, and eliminates high-scoring false-positives mapping to human alphaherpesvirus 1:\n\nCodedata_dir_2 &lt;- file.path(data_dir, \"take2\")\n\n# Import HV read data\nhv_reads_filtered_2_path &lt;- file.path(data_dir_2, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered_2 &lt;- lapply(hv_reads_filtered_2_path, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  left_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered_2 &lt;- hv_reads_filtered_2 %&gt;%\n  group_by(sample, location, sample_group, seq_id) %&gt;% count %&gt;%\n  group_by(sample, location, sample_group) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ_2 &lt;- n_hv_filtered_2 %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n# Process read data\nmrg2 &lt;- hv_reads_filtered_2 %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\n\n\nCode# Import paired BLAST results\nblast_paired_2_path &lt;- file.path(data_dir_2, \"hv_hits_blast_paired.tsv.gz\")\nblast_paired_2 &lt;- read_tsv(blast_paired_2_path, show_col_types = FALSE)\n\n# Add viral status\nblast_viral_2 &lt;- mutate(blast_paired_2, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmrg2_assign &lt;- mrg2 %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_assign_2 &lt;- inner_join(blast_viral, mrg2_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_out_2 &lt;- blast_assign_2 %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n# Merge BLAST results with unenriched read data\nmrg2_blast &lt;- full_join(mrg2, blast_out_2, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_vhist_2 &lt;- g_vhist_base + geom_vhist(data=mrg2_blast, mapping=aes(fill=viral_status_out)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\")\ng_vhist_2\n\n\n\n\n\n\n\n\nCodestats_1 &lt;- range_f1(mrg2_blast)\ng_stats_1 &lt;- ggplot(stats_1, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_1\n\n\n\n\n\n\nCodestats_1 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\n\nCode# Prepare data\nfp2 &lt;- mrg2_blast %&gt;% \n  group_by(viral_status_out, highscore, taxid_best) %&gt;% count %&gt;% \n  group_by(viral_status_out, highscore) %&gt;% mutate(p=n/sum(n)) %&gt;% \n  rename(taxid = taxid_best) %&gt;%\n  left_join(viral_taxa, by=\"taxid\") %&gt;%\n  arrange(desc(p))\nfp2_major_tab &lt;- fp2 %&gt;% filter(p &gt; major_threshold) %&gt;% arrange(desc(p))\nfp2_major_list &lt;- fp2_major_tab %&gt;% pull(name) %&gt;% sort %&gt;% unique %&gt;% c(., \"Other\")\nfp2_major &lt;- fp2 %&gt;% mutate(major = p &gt; major_threshold) %&gt;% \n  mutate(name_display = ifelse(major, name, \"Other\")) %&gt;%\n  group_by(viral_status_out, highscore, name_display) %&gt;% \n  summarize(n=sum(n), p=sum(p), .groups = \"drop\")  %&gt;%\n  mutate(name_display = factor(name_display, levels = fp2_major_list),\n         score_display = ifelse(highscore, \"S &gt;= 20\", \"S &lt; 20\"),\n         status_display = ifelse(viral_status_out, \"True positive\", \"False positive\"))\n\n# Plot\ng_fp2 &lt;- ggplot(fp2_major, aes(x=score_display, y=p, fill=name_display)) +\n  geom_col(position=\"stack\") +\n  scale_x_discrete(name = \"True positive?\") +\n  scale_y_continuous(name = \"% reads\", limits = c(0,1.01), \n                     breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_fill_manual(values = palette_viral, name = \"Viral\\ntaxon\") +\n  facet_grid(.~status_display) +\n  guides(fill=guide_legend(ncol=3)) +\n  theme_kit\ng_fp2\n\n\n\n\n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, location, sample_group, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg2 %&gt;% mutate(hv_status = assigned_hv | hit_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_grp &lt;- read_counts %&gt;% group_by(location, sample_group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\")\nread_counts_st &lt;- read_counts_grp %&gt;% group_by(sample, sample_group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(location = \"All locations\")\nread_counts_loc &lt;- read_counts_grp %&gt;%\n  group_by(sample, location) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample_group = \"All sample types\")\nread_counts_tot &lt;- read_counts_loc %&gt;% group_by(sample, sample_group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(location = \"All locations\")\nread_counts_agg &lt;- bind_rows(read_counts_grp, read_counts_st,\n                             read_counts_loc, read_counts_tot) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         location = factor(location, levels = c(levels(libraries$location), \"All locations\")),\n         sample_group = factor(sample_group, levels = c(levels(libraries$sample_group), \"All sample types\")))\n\n\nApplying a disjunctive cutoff at S=20 identifies 230 read pairs as human-viral. This gives an overall relative HV abundance of \\(8.50 \\times 10^{-8}\\). While very low across all sample types, HV RA was noticeably higher in influent and primary sludge than in sample types that had undergone more extensive processing (effluent and processed sludge):\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=sample_group, color=location)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_brewer(name=\"Location\", palette=\"Dark2\") + theme_kit\ng_phv_agg\n\n\n\n\n\n\n\nThis is by far the lowest HV relative abundance I’ve seen across any of the datasets I’ve analyzed:\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE,\n                   \"Leung\", 1.73e-5, \"DNA\", FALSE,\n                   \"Brinch\", 3.88e-6, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Bengtsson-Palme\", 8.86e-8, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nIn investigating the taxonomy of human-infecting virus reads, I restricted my analysis to samples with more than 5 HV read pairs total across all viruses, to reduce noise arising from extremely low HV read counts in some samples. 13 samples, 3 from influent and 10 from primary sludge, met this criterion.\nAt the family level, most samples across all locations were dominated by Poxviridae and Adenoviridae, with Herpesviridae, Papillomaviridae and Picornaviridae also making a significant appearance in at least some samples:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\nviral_taxa$name[viral_taxa$taxid == 333930] &lt;- \"Gammapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 337048] &lt;- \"Alphapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337041] &lt;- \"Alphapapillomavirus 9\"\nviral_taxa$name[viral_taxa$taxid == 337049] &lt;- \"Alphapapillomavirus 11\"\nviral_taxa$name[viral_taxa$taxid == 337044] &lt;- \"Alphapapillomavirus 5\"\n\n# Filter samples and add viral taxa information\nsamples_keep &lt;- read_counts %&gt;% filter(n_reads_hv &gt; 5) %&gt;% pull(sample)\nmrg_hv_named &lt;- mrg_hv %&gt;% filter(sample %in% samples_keep) %&gt;% left_join(viral_taxa, by=\"taxid\") \n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.08\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn investigating individual viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads:\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_pox &lt;- 10240\n\n# Get set of poxviridae reads\npox_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_pox) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npox_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_pox, sample %in% pox_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each poxviridae species\npox_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% pox_ids) %&gt;%\n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_pox = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npox_species_major_tab &lt;- pox_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_pox == max(p_reads_pox)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_pox)) %&gt;% \n  filter(p_reads_pox &gt; threshold_major_species)\npox_species_counts_major &lt;- pox_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% pox_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_pox = sum(n_reads_hv),\n            p_reads_pox = sum(p_reads_pox), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(pox_species_major_tab$name, \"Other\")))\npox_species_counts_display &lt;- pox_species_counts_major %&gt;%\n  rename(p_reads = p_reads_pox, classification = name_display)\n\n# Plot\ng_pox_species &lt;- g_comp_base + \n  geom_col(data=pox_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Poxviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Poxviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_pox_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npox_species_collate &lt;- pox_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_pox), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_adeno &lt;- 10508\n\n# Get set of adenoviridae reads\nadeno_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_adeno) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nadeno_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_adeno, sample %in% adeno_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each adenoviridae species\nadeno_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% adeno_ids) %&gt;%\n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_adeno = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nadeno_species_major_tab &lt;- adeno_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_adeno == max(p_reads_adeno)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_adeno)) %&gt;% \n  filter(p_reads_adeno &gt; threshold_major_species)\nadeno_species_counts_major &lt;- adeno_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% adeno_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_adeno = sum(n_reads_hv),\n            p_reads_adeno = sum(p_reads_adeno), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(adeno_species_major_tab$name, \"Other\")))\nadeno_species_counts_display &lt;- adeno_species_counts_major %&gt;%\n  rename(p_reads = p_reads_adeno, classification = name_display)\n\n# Plot\ng_adeno_species &lt;- g_comp_base + \n  geom_col(data=adeno_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Adenoviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Adenoviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_adeno_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nadeno_species_collate &lt;- adeno_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_adeno), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_herpes &lt;- 10292\n\n# Get set of herpesviridae reads\nherpes_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_herpes) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nherpes_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_herpes, sample %in% herpes_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each herpesviridae species\nherpes_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_papilloma &lt;- 151340\n\n# Get set of papillomaviridae reads\npapilloma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_papilloma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npapilloma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_papilloma, sample %in% papilloma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each papillomaviridae species\npapilloma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% papilloma_ids) %&gt;%\n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_papilloma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npapilloma_species_major_tab &lt;- papilloma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_papilloma == max(p_reads_papilloma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_papilloma)) %&gt;% \n  filter(p_reads_papilloma &gt; threshold_major_species)\npapilloma_species_counts_major &lt;- papilloma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% papilloma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_papilloma = sum(n_reads_hv),\n            p_reads_papilloma = sum(p_reads_papilloma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(papilloma_species_major_tab$name, \"Other\")))\npapilloma_species_counts_display &lt;- papilloma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_papilloma, classification = name_display)\n\n# Plot\ng_papilloma_species &lt;- g_comp_base + \n  geom_col(data=papilloma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Papillomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Papillomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_papilloma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npapilloma_species_collate &lt;- papilloma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_papilloma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_picorna &lt;- 12058\n\n# Get set of picornaviridae reads\npicorna_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_picorna) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npicorna_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_picorna, sample %in% picorna_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each picornaviridae species\npicorna_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% picorna_ids) %&gt;%\n  group_by(sample, location, sample_group, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, location, sample_group) %&gt;%\n  mutate(p_reads_picorna = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npicorna_species_major_tab &lt;- picorna_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_picorna == max(p_reads_picorna)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_picorna)) %&gt;% \n  filter(p_reads_picorna &gt; threshold_major_species)\npicorna_species_counts_major &lt;- picorna_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% picorna_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, location, sample_group, name_display) %&gt;%\n  summarize(n_reads_picorna = sum(n_reads_hv),\n            p_reads_picorna = sum(p_reads_picorna), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(picorna_species_major_tab$name, \"Other\")))\npicorna_species_counts_display &lt;- picorna_species_counts_major %&gt;%\n  rename(p_reads = p_reads_picorna, classification = name_display)\n\n# Plot\ng_picorna_species &lt;- g_comp_base + \n  geom_col(data=picorna_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Picornaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Picornaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_picorna_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npicorna_species_collate &lt;- picorna_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_picorna), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nmrg_hv_named_all &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\nhv_reads_genus_all &lt;- raise_rank(mrg_hv_named_all, viral_taxa, \"genus\")\nn_path_genera &lt;- hv_reads_genus_all %&gt;% \n  group_by(sample, location, sample_group, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"location\", \"sample_group\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type, sample_group) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", location=\"All locations\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=sample_group)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_st() +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\n\n\n\n\n\n\nConclusion\nThis dataset is a cautionary tale about the dangers of looking for viruses in samples that were processed with something else in mind. The sample preparation methods used here would be expected to select against viruses, and it shows, with by far the lowest overall human-viral RA of any dataset I’ve investigated so far. Even so, there were some interesting results, such as the higher level of human viruses in influent and primary sludge compared to more processed sample types."
  },
  {
    "objectID": "notebooks/2024-05-01_ng.html",
    "href": "notebooks/2024-05-01_ng.html",
    "title": "Workflow analysis of Ng et al. (2019)",
    "section": "",
    "text": "Continuing my analysis of datasets from the P2RA preprint, I analyzed the data from Ng et al. (2019), a study that used DNA sequencing of wastewater samples to characterize the bacterial microbiota and resistome in Singapore. This study used processing methods I haven’t seen before:\n\nAll samples passed through “a filter” on-site at the WWTP prior to further processing in lab.\nSamples concentrated to 400ml using a Hemoflow dialyzer “via standard bloodline tubing”.\nEluted concentrates then further concentrated by passing through a 0.22um filter and retaining the retentate (NB: this is anti-selecting for viruses).\nSludge samples were instead centrifuged and the pellet kept for further analysis.\nAfter concentration, samples underwent DNA extraction with the PowerSoil DNA Isolation Kit, then underwent library prep and Illumina sequencing with an Illumina HiSeq2500 (2x250bp).\n\nSince this was a bacteria-focused study that used processing methods we expect to select against viruses, we wouldn’t expect to see high viral relative abundances here. Nevertheless, it’s worth seeing what we can see.\nThe raw data\nSamples were collected from six different locations in the treatment plant on six different dates (from October 2016 to August 2017) for a total of 36 samples:\n\n\nCode# Importing the data is a bit more complicated this time as the samples are split across three pipeline runs\ndata_dir &lt;- \"../data/2024-05-01_ng\"\n\n# Data input paths\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv.gz\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv.gz\")\n\n# Import libraries and extract metadata from sample names\nlocs &lt;- c(\"INF\", \"PST\", \"SLUDGE\", \"SST\", \"MBR\", \"WW\")\nlibraries_raw &lt;- lapply(libraries_path, read_csv, show_col_types = FALSE) %&gt;%\n  bind_rows\nlibraries &lt;- libraries_raw %&gt;%\n  mutate(sample_type_long = gsub(\" \\\\(.*\", \"\", sample_type),\n         sample_type_short = ifelse(sample_type_long == \"Influent\", \"INF\",\n                                    sub(\".*\\\\((.*)\\\\)\", \"\\\\1\", sample_type)),\n         sample_type_short = factor(sample_type_short, levels=locs)) %&gt;%\n  arrange(sample_type_short, date) %&gt;%\n  mutate(sample_type_long = fct_inorder(sample_type_long),\n         sample = fct_inorder(sample)) %&gt;%\n  arrange(date) %&gt;%\n  mutate(date = fct_inorder(date))\n\n# Make table\ncount_samples &lt;- libraries %&gt;% group_by(sample_type_long, sample_type_short) %&gt;%\n  count %&gt;%\n  rename(`Sample Type`=sample_type_long, Abbreviation=sample_type_short)\ncount_samples\n\n\n  \n\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;%\n      arrange(sample_type_short, date, sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_path)\nadapter_stats &lt;- import_basic_paired(adapter_stats_path)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_path)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_path)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% ungroup %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\nThese 36 samples yielded 26.6M-74.1M (mean 46.1M) reads per sample, for a total of 1.7B read pairs (830 gigabases of sequence). Read qualities were mostly high but tailed off towards the 3’ end, requiring some trimming. Adapter levels were fairly low but still in need of some trimming. Inferred duplication levels were variable (1-64%, mean 31%), with libraries with lower read depth showing much lower duplicate levels.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample, sample_type_short, date,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample:date), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\nscale_fill_st &lt;- purrr::partial(scale_fill_brewer, palette=\"Set1\", name=\"Sample Type\")\ng_basic &lt;- ggplot(basic_stats_raw_metrics, \n                  aes(x=sample, y=value, fill=sample_type_short, \n                      group=interaction(sample_type_short,sample))) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  scale_fill_st() + \n  facet_grid(metric~., scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_xblank + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\nscale_color_st &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                   name=\"Sample Type\")\ng_qual_raw &lt;- ggplot(mapping=aes(color=sample_type_short, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_st() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,1), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nThe average fraction of reads lost at each stage in the preprocessing pipeline is shown in the following table. As expected given the observed difference in duplication levels, many more reads were lost during deduplication in liquid samples than sludge samples. Conversely, trimming and filtering consistently removed more reads in sludge than in liquid samples, though the effect was less dramatic than for deduplication. Very few reads were lost during ribodepletion, as expected for DNA sequencing libraries.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, sample_type_short, date, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  group_by(`Sample Type`=sample_type_short, Stage=stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_base &lt;- ggplot(mapping=aes(x=stage, color=sample_type_short, group=sample)) +\n  scale_color_st() +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_base +\n  geom_line(aes(y=n_read_pairs), data=basic_stats) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- g_stage_base +\n  geom_line(aes(y=p_reads_lost_abs_marginal), data=n_reads_rel) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100)\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and improving read qualities:\n\nCodeg_qual &lt;- ggplot(mapping=aes(color=sample_type_short, linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_color_st() + scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels, which fell from an average of 31% to 6.5%:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_base +\n  geom_line(aes(y=percent_duplicates), data=basic_stats) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_base + \n  geom_line(aes(y=mean_seq_len), data=basic_stats) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCodeclassifications &lt;- c(\"Filtered\", \"Duplicate\", \"Ribosomal\", \"Unassigned\",\n                     \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# Import composition data\ncomp_path &lt;- file.path(data_dir, \"taxonomic_composition.tsv.gz\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  left_join(libraries, by=\"sample\") %&gt;%\n  mutate(classification = factor(classification, levels = classifications))\n  \n\n# Summarize composition\nread_comp_summ &lt;- comp %&gt;% \n  group_by(sample_type_short, classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=sample, y=p_reads, fill=classification)) +\n  facet_wrap(~sample_type_short, scales = \"free_x\", ncol=3,\n             labeller = label_wrap_gen(multi_line=FALSE, width=20)) +\n  theme_xblank\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = comp, position = \"stack\", width=1) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\ncomp_minor &lt;- comp %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_col(data=comp_minor, position = \"stack\", width=1) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- comp %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample, sample_type_short) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification, sample_type_short) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(`Sample Type`=sample_type_short, Classification=classification, \n         `Read Fraction`=display) %&gt;%\n  arrange(`Sample Type`, Classification)\np_reads_summ\n\n\n  \n\n\n\nAs in previous DNA datasets, the vast majority of classified reads were bacterial in origin. The fraction of virus reads varied substantially between sample types, averaging &lt;0.01% in influent and final effluent but closer to 0.05% in other sample types. Interestingly (though not particularly relevantly for this analysis), the fraction of archaeal reads was much higher in influent than other sample types, in contrast to Bengtsson-Palme where it was highest in sludge.\nAs is common for DNA data, viral reads were overwhelmingly dominated by Caudoviricetes phages, though one wet-well sample contained a substantial fraction of Alsuviricetes (a class of mainly plant pathogens that includes Virgaviridae):\n\nCode# Get Kraken reports\nreports_path &lt;- file.path(data_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE)\n\n# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.02\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, sample_type_short, date, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample, sample_type_short, date) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, sample_type_short, date, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. A grand total of 527 reads were identified as putatively human-viral, with half of samples showing 5 or fewer total HV read pairs.\n\nCode# Import HV read data\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_path, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  left_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, date, sample_type_short, seq_id) %&gt;% count %&gt;%\n  group_by(sample, date, sample_type_short) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\n# Plot results\ngeom_vhist &lt;- purrr::partial(geom_histogram, binwidth=5, boundary=0)\ng_vhist_base &lt;- ggplot(mapping=aes(x=adj_score_max)) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_vhist_0 &lt;- g_vhist_base + geom_vhist(data=mrg)\ng_vhist_0\n\n\n\n\n\n\n\nBLASTing these reads against nt, we find that the pipeline performs well, with only a single high-scoring false-positive read:\n\nCode# Import paired BLAST results\nblast_paired_path &lt;- file.path(data_dir, \"hv_hits_blast_paired.tsv.gz\")\nblast_paired &lt;- read_tsv(blast_paired_path, show_col_types = FALSE)\n\n# Add viral status\nblast_viral &lt;- mutate(blast_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_assign &lt;- inner_join(blast_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_out &lt;- blast_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_vhist_1 &lt;- g_vhist_base + geom_vhist(data=mrg_blast, mapping=aes(fill=viral_status_out)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\")\ng_vhist_1\n\n\n\n\n\n\n\nMy usual disjunctive score threshold of 20 gave precision, sensitivity, and F1 scores all &gt;97%:\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, sample_type_short, date, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_grp &lt;- read_counts %&gt;% group_by(date, sample_type_short) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\")\nread_counts_st &lt;- read_counts_grp %&gt;% group_by(sample, sample_type_short) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(date = \"All dates\")\nread_counts_date &lt;- read_counts_grp %&gt;%\n  group_by(sample, date) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample_type_short = \"All sample types\")\nread_counts_tot &lt;- read_counts_date %&gt;% group_by(sample, sample_type_short) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(date = \"All dates\")\nread_counts_agg &lt;- bind_rows(read_counts_grp, read_counts_st,\n                             read_counts_date, read_counts_tot) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         date = factor(date, levels = c(levels(libraries$date), \"All dates\")),\n         sample_type_short = factor(sample_type_short, levels = c(levels(libraries$sample_type_short), \"All sample types\")))\n\n\nApplying a disjunctive cutoff at S=20 identifies 482 read pairs as human-viral. This gives an overall relative HV abundance of \\(2.90 \\times 10^{-7}\\); on the low end across all datasets I’ve analyzed, though higher than for Bengtsson-Palme:\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=date, color=sample_type_short)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  scale_color_st() + theme_kit\ng_phv_agg\n\n\n\n\n\n\n\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE,\n                   \"Leung\", 1.73e-5, \"DNA\", FALSE,\n                   \"Brinch\", 3.88e-6, \"DNA\", FALSE,\n                   \"Bengtsson-Palme\", 8.86e-8, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Ng\", 2.90e-7, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nIn investigating the taxonomy of human-infecting virus reads, I restricted my analysis to samples with more than 5 HV read pairs total across all viruses, to reduce noise arising from extremely low HV read counts in some samples. 13 samples met this criterion.\nAt the family level, most samples were overwhelmingly dominated by Adenoviridae, with Picornaviridae, Polyomaviridae and Papillomaviridae making up most of the rest:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\nviral_taxa$name[viral_taxa$taxid == 333930] &lt;- \"Gammapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 337048] &lt;- \"Alphapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337041] &lt;- \"Alphapapillomavirus 9\"\nviral_taxa$name[viral_taxa$taxid == 337049] &lt;- \"Alphapapillomavirus 11\"\nviral_taxa$name[viral_taxa$taxid == 337044] &lt;- \"Alphapapillomavirus 5\"\n\n# Filter samples and add viral taxa information\nsamples_keep &lt;- read_counts %&gt;% filter(n_reads_hv &gt; 5) %&gt;% pull(sample)\nmrg_hv_named &lt;- mrg_hv %&gt;% filter(sample %in% samples_keep, hv_status) %&gt;% left_join(viral_taxa, by=\"taxid\") \n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.02\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, date, sample_type_short, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, sample_type_short) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, date, sample_type_short, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn investigating individual viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads:\n\nCodethreshold_major_species &lt;- 0.05\ntaxid_adeno &lt;- 10508\n\n# Get set of adenoviridae reads\nadeno_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_adeno) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nadeno_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_adeno, sample %in% adeno_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each adenoviridae species\nadeno_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% adeno_ids) %&gt;%\n  group_by(sample, date, sample_type_short, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, sample_type_short) %&gt;%\n  mutate(p_reads_adeno = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nadeno_species_major_tab &lt;- adeno_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_adeno == max(p_reads_adeno)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_adeno)) %&gt;% \n  filter(p_reads_adeno &gt; threshold_major_species)\nadeno_species_counts_major &lt;- adeno_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% adeno_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date, sample_type_short, name_display) %&gt;%\n  summarize(n_reads_adeno = sum(n_reads_hv),\n            p_reads_adeno = sum(p_reads_adeno), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(adeno_species_major_tab$name, \"Other\")))\nadeno_species_counts_display &lt;- adeno_species_counts_major %&gt;%\n  rename(p_reads = p_reads_adeno, classification = name_display)\n\n# Plot\ng_adeno_species &lt;- g_comp_base + \n  geom_col(data=adeno_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Adenoviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Adenoviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_adeno_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nadeno_species_collate &lt;- adeno_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_adeno), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_picorna &lt;- 12058\n\n# Get set of picornaviridae reads\npicorna_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_picorna) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npicorna_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_picorna, sample %in% picorna_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each picornaviridae species\npicorna_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% picorna_ids) %&gt;%\n  group_by(sample, date, sample_type_short, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, sample_type_short) %&gt;%\n  mutate(p_reads_picorna = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npicorna_species_major_tab &lt;- picorna_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_picorna == max(p_reads_picorna)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_picorna)) %&gt;% \n  filter(p_reads_picorna &gt; threshold_major_species)\npicorna_species_counts_major &lt;- picorna_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% picorna_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date, sample_type_short, name_display) %&gt;%\n  summarize(n_reads_picorna = sum(n_reads_hv),\n            p_reads_picorna = sum(p_reads_picorna), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(picorna_species_major_tab$name, \"Other\")))\npicorna_species_counts_display &lt;- picorna_species_counts_major %&gt;%\n  rename(p_reads = p_reads_picorna, classification = name_display)\n\n# Plot\ng_picorna_species &lt;- g_comp_base + \n  geom_col(data=picorna_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Picornaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Picornaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_picorna_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npicorna_species_collate &lt;- picorna_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_picorna), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_polyoma &lt;- 151341\n\n# Get set of polyomaviridae reads\npolyoma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_polyoma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npolyoma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_polyoma, sample %in% polyoma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each polyomaviridae species\npolyoma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% polyoma_ids) %&gt;%\n  group_by(sample, date, sample_type_short, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample, date, sample_type_short) %&gt;%\n  mutate(p_reads_polyoma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npolyoma_species_major_tab &lt;- polyoma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_polyoma == max(p_reads_polyoma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_polyoma)) %&gt;% \n  filter(p_reads_polyoma &gt; threshold_major_species)\npolyoma_species_counts_major &lt;- polyoma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% polyoma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, date, sample_type_short, name_display) %&gt;%\n  summarize(n_reads_polyoma = sum(n_reads_hv),\n            p_reads_polyoma = sum(p_reads_polyoma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(polyoma_species_major_tab$name, \"Other\")))\npolyoma_species_counts_display &lt;- polyoma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_polyoma, classification = name_display)\n\n# Plot\ng_polyoma_species &lt;- g_comp_base + \n  geom_col(data=polyoma_species_counts_display, position = \"stack\") +\n  scale_y_continuous(name=\"% Polyomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Polyomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_polyoma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npolyoma_species_collate &lt;- polyoma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_polyoma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nmrg_hv_named_all &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\nhv_reads_genus_all &lt;- raise_rank(mrg_hv_named_all, viral_taxa, \"genus\")\nn_path_genera &lt;- hv_reads_genus_all %&gt;% \n  group_by(sample, date, sample_type_short, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\", \"date\", \"sample_type_short\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type, sample_type_short) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", location=\"All locations\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral, color=sample_type_short)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  scale_color_st() +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\n\n\n\n\n\n\nConclusion\nThis is another dataset with very low HV abundance, arising from lab methods intended to maximize bacterial abundance at the expense of other taxa. Nevertheless, this dataset had higher HV relative abundance than the last one. Interestingly, all three wastewater DNA datasets analyzed so far have exhibited a strong predominance of adenoviruses, and especially human mastadenovirus F, among human-infecting viruses. We’ll see if this pattern persists in the other DNA wastewater datasets I have in the queue."
  },
  {
    "objectID": "notebooks/2024-05-01_maritz.html",
    "href": "notebooks/2024-05-01_maritz.html",
    "title": "Workflow analysis of Maritz et al. (2019)",
    "section": "",
    "text": "Continuing my analysis of datasets from the P2RA preprint, I analyzed the data from Maritz et al. (2019), a study that used DNA sequencing of wastewater samples to characterize protist diversity and temporal diversity in New York City. Samples for this study underwent direct DNA extraction without a dedicated concentration step, then underwent library prep and Illumina sequencing on a HiSeq Rapid Run (2x250bp).\nThe raw data\n16 samples were collected from 14 treatment plants in NYC in November 2014. These samples yielded 8.6M-18.3M (mean 10.8M) reads per sample, for a total of 172M read pairs (84 gigabases of sequence). Read qualities were mostly high; adapter levels were moderate; inferred duplication levels were low.\n\nCode# Importing the data is a bit more complicated this time as the samples are split across three pipeline runs\ndata_dir &lt;- \"../data/2024-05-01_maritz\"\n\n# Data input paths\nlibraries_path &lt;- file.path(data_dir, \"sample-metadata.csv\")\nbasic_stats_path &lt;- file.path(data_dir, \"qc_basic_stats.tsv.gz\")\nadapter_stats_path &lt;- file.path(data_dir, \"qc_adapter_stats.tsv.gz\")\nquality_base_stats_path &lt;- file.path(data_dir, \"qc_quality_base_stats.tsv.gz\")\nquality_seq_stats_path &lt;- file.path(data_dir, \"qc_quality_sequence_stats.tsv.gz\")\n\n# Import libraries and extract metadata from sample names\nlibraries_raw &lt;- lapply(libraries_path, read_csv, show_col_types = FALSE) %&gt;%\n  bind_rows\nlibraries &lt;- libraries_raw %&gt;%\n  mutate(sample = fct_inorder(sample))\n\n\n\nCode# Import QC data\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"ribo_secondary\")\nimport_basic &lt;- function(paths){\n  lapply(paths, read_tsv, show_col_types = FALSE) %&gt;% bind_rows %&gt;%\n    inner_join(libraries, by=\"sample\") %&gt;%\n    arrange(sample) %&gt;%\n    mutate(stage = factor(stage, levels = stages),\n           sample = fct_inorder(sample))\n}\nimport_basic_paired &lt;- function(paths){\n  import_basic(paths) %&gt;% arrange(read_pair) %&gt;% \n    mutate(read_pair = fct_inorder(as.character(read_pair)))\n}\nbasic_stats &lt;- import_basic(basic_stats_path)\nadapter_stats &lt;- import_basic_paired(adapter_stats_path)\nquality_base_stats &lt;- import_basic_paired(quality_base_stats_path)\nquality_seq_stats &lt;- import_basic_paired(quality_seq_stats_path)\n\n# Filter to raw data\nbasic_stats_raw &lt;- basic_stats %&gt;% filter(stage == \"raw_concat\")\nadapter_stats_raw &lt;- adapter_stats %&gt;% filter(stage == \"raw_concat\")\nquality_base_stats_raw &lt;- quality_base_stats %&gt;% filter(stage == \"raw_concat\")\nquality_seq_stats_raw &lt;- quality_seq_stats %&gt;% filter(stage == \"raw_concat\")\n\n# Get key values for readout\nraw_read_counts &lt;- basic_stats_raw %&gt;% ungroup %&gt;% \n  summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),\n            rmean=mean(n_read_pairs), \n            rtot = sum(n_read_pairs),\n            btot = sum(n_bases_approx),\n            dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\n\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(sample,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(sample), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric))\n\n# Set up plot templates\ng_basic &lt;- ggplot(basic_stats_raw_metrics, aes(x=sample, y=value)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(expand=c(0,0)) +\n  expand_limits(y=c(0,100)) +\n  facet_grid(metric~., scales = \"free\", space=\"free_x\", switch=\"y\") +\n  theme_kit + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face=\"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\n\nCode# Set up plotting templates\ng_qual_raw &lt;- ggplot(mapping=aes(linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,1), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw\n\n\n\n\n\n\n\nPreprocessing\nAbout 6% of reads on average were lost during cleaning, and a further 2% during deduplication. Very few reads were lost during ribodepletion, as expected for DNA sequencing libraries.\n\nCoden_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, \n         percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = replace_na(n_read_pairs / lag(n_read_pairs), 0),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = replace_na(p_reads_lost_abs - lag(p_reads_lost_abs), 0))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  group_by(Stage=stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_base &lt;- ggplot(mapping=aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_base +\n  geom_line(aes(y=n_read_pairs), data=basic_stats) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- g_stage_base +\n  geom_line(aes(y=p_reads_lost_abs_marginal), data=n_reads_rel) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100)\ng_reads_rel\n\n\n\n\n\n\n\nData cleaning was very successful at removing adapters and improving read qualities:\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, \n                         group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_seq\n\n\n\n\n\n\n\nAccording to FASTQC, cleaning + deduplication was very effective at reducing measured duplicate levels in the few samples that required it:\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_base +\n  geom_line(aes(y=percent_duplicates), data=basic_stats) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_base + \n  geom_line(aes(y=mean_seq_len), data=basic_stats) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nHigh-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken (using the Standard 16 database) and summarized the results with Bracken. Combining these results with the read counts above gives us a breakdown of the inferred composition of the samples:\n\nCodeclassifications &lt;- c(\"Filtered\", \"Duplicate\", \"Ribosomal\", \"Unassigned\",\n                     \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# Import composition data\ncomp_path &lt;- file.path(data_dir, \"taxonomic_composition.tsv.gz\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  left_join(libraries, by=\"sample\") %&gt;%\n  mutate(classification = factor(classification, levels = classifications))\n  \n\n# Summarize composition\nread_comp_summ &lt;- comp %&gt;% \n  group_by(classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n    p_reads = n_reads/sum(n_reads),\n    pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=sample, y=p_reads, fill=classification)) +\n  theme_kit\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_col(data = comp, position = \"stack\", width=1) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\")\ng_comp\n\n\n\n\n\n\nCode# Plot composition of minor components\ncomp_minor &lt;- comp %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_col(data=comp_minor, position = \"stack\", width=1) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\")\ng_comp_minor\n\n\n\n\n\n\n\n\nCodep_reads_summ_group &lt;- comp %&gt;%\n  mutate(classification = ifelse(classification %in% c(\"Filtered\", \"Duplicate\", \"Unassigned\"), \"Excluded\", as.character(classification)),\n         classification = fct_inorder(classification)) %&gt;%\n  group_by(classification, sample) %&gt;%\n  summarize(p_reads = sum(p_reads), .groups = \"drop\") %&gt;%\n  group_by(classification) %&gt;%\n  summarize(pc_min = min(p_reads)*100, pc_max = max(p_reads)*100, \n            pc_mean = mean(p_reads)*100, .groups = \"drop\")\np_reads_summ_prep &lt;- p_reads_summ_group %&gt;%\n  mutate(classification = fct_inorder(classification),\n         pc_min = pc_min %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_max = pc_max %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         pc_mean = pc_mean %&gt;% signif(digits=2) %&gt;% sapply(format, scientific=FALSE, trim=TRUE, digits=2),\n         display = paste0(pc_min, \"-\", pc_max, \"% (mean \", pc_mean, \"%)\"))\np_reads_summ &lt;- p_reads_summ_prep %&gt;%\n  select(Classification=classification, \n         `Read Fraction`=display) %&gt;%\n  arrange(Classification)\np_reads_summ\n\n\n  \n\n\n\nAs in previous DNA datasets, the vast majority of classified reads were bacterial in origin. Viral fraction averaged 0.13%, though one samples (NYC-08) reached almost 1%. As is common for DNA data, viral reads were overwhelmingly dominated by Caudoviricetes phages:\n\nCode# Get Kraken reports\nreports_path &lt;- file.path(data_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE)\n\n# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"viral-taxids.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything())\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.02\n\n# Identify major viral classes\nviral_classes_major_tab &lt;- viral_classes %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_classes_major_list &lt;- viral_classes_major_tab %&gt;% pull(name)\nviral_classes_major &lt;- viral_classes %&gt;% \n  filter(name %in% viral_classes_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_classes_minor &lt;- viral_classes_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_classes_display &lt;- bind_rows(viral_classes_major, viral_classes_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_classes_major_list, \"Other\")),\n         p_reads_viral = pmax(p_reads_viral, 0)) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name)\n\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_classes &lt;- g_comp_base + \n  geom_col(data=viral_classes_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\n  \ng_classes\n\n\n\n\n\n\n\nHuman-infecting virus reads: validation\nNext, I investigated the human-infecting virus read content of these unenriched samples. A grand total of 199 reads were identified as putatively human-viral:\n\nCode# Import HV read data\nhv_reads_filtered_path &lt;- file.path(data_dir, \"hv_hits_putative_filtered.tsv.gz\")\nhv_reads_filtered &lt;- lapply(hv_reads_filtered_path, read_tsv,\n                            show_col_types = FALSE) %&gt;%\n  bind_rows() %&gt;%\n  left_join(libraries, by=\"sample\")\n\n# Count reads\nn_hv_filtered &lt;- hv_reads_filtered %&gt;%\n  group_by(sample, seq_id) %&gt;% count %&gt;%\n  group_by(sample) %&gt;% count %&gt;% \n  inner_join(basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;% \n               select(sample, n_read_pairs), by=\"sample\") %&gt;% \n  rename(n_putative = n, n_total = n_read_pairs) %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads * 100)\nn_hv_filtered_summ &lt;- n_hv_filtered %&gt;% ungroup %&gt;%\n  summarize(n_putative = sum(n_putative), n_total = sum(n_total), \n            .groups=\"drop\") %&gt;% \n  mutate(p_reads = n_putative/n_total, pc_reads = p_reads*100)\n\n\n\nCode# Collapse multi-entry sequences\nrmax &lt;- purrr::partial(max, na.rm = TRUE)\ncollapse &lt;- function(x) ifelse(all(x == x[1]), x[1], paste(x, collapse=\"/\"))\nmrg &lt;- hv_reads_filtered %&gt;% \n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev, na.rm = TRUE)) %&gt;%\n  arrange(desc(adj_score_max)) %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(sample = collapse(sample),\n            genome_id = collapse(genome_id),\n            taxid_best = taxid[1],\n            taxid = collapse(as.character(taxid)),\n            best_alignment_score_fwd = rmax(best_alignment_score_fwd),\n            best_alignment_score_rev = rmax(best_alignment_score_rev),\n            query_len_fwd = rmax(query_len_fwd),\n            query_len_rev = rmax(query_len_rev),\n            query_seq_fwd = query_seq_fwd[!is.na(query_seq_fwd)][1],\n            query_seq_rev = query_seq_rev[!is.na(query_seq_rev)][1],\n            classified = rmax(classified),\n            assigned_name = collapse(assigned_name),\n            assigned_taxid_best = assigned_taxid[1],\n            assigned_taxid = collapse(as.character(assigned_taxid)),\n            assigned_hv = rmax(assigned_hv),\n            hit_hv = rmax(hit_hv),\n            encoded_hits = collapse(encoded_hits),\n            adj_score_fwd = rmax(adj_score_fwd),\n            adj_score_rev = rmax(adj_score_rev)\n            ) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\"))) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20)\n\n# Plot results\ngeom_vhist &lt;- purrr::partial(geom_histogram, binwidth=5, boundary=0)\ng_vhist_base &lt;- ggplot(mapping=aes(x=adj_score_max)) +\n  geom_vline(xintercept=20, linetype=\"dashed\", color=\"red\") +\n  facet_wrap(~kraken_label, labeller = labeller(kit = label_wrap_gen(20)), scales = \"free_y\") +\n  scale_x_continuous(name = \"Maximum adjusted alignment score\") + \n  scale_y_continuous(name=\"# Read pairs\") + \n  theme_base \ng_vhist_0 &lt;- g_vhist_base + geom_vhist(data=mrg)\ng_vhist_0\n\n\n\n\n\n\n\nBLASTing these reads against nt, we find that the pipeline performs well, with only a single high-scoring false-positive read:\n\nCode# Import paired BLAST results\nblast_paired_path &lt;- file.path(data_dir, \"hv_hits_blast_paired.tsv.gz\")\nblast_paired &lt;- read_tsv(blast_paired_path, show_col_types = FALSE)\n\n# Add viral status\nblast_viral &lt;- mutate(blast_paired, viral = staxid %in% viral_taxa$taxid) %&gt;%\n  mutate(viral_full = viral & n_reads == 2)\n\n# Compare to Kraken & Bowtie assignments\nmatch_taxid &lt;- function(taxid_1, taxid_2){\n  p1 &lt;- mapply(grepl, paste0(\"/\", taxid_1, \"$\"), taxid_2)\n  p2 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"/\"), taxid_2)\n  p3 &lt;- mapply(grepl, paste0(\"^\", taxid_1, \"$\"), taxid_2)\n  out &lt;- setNames(p1|p2|p3, NULL)\n  return(out)\n}\nmrg_assign &lt;- mrg %&gt;% select(sample, seq_id, taxid, assigned_taxid, adj_score_max)\nblast_assign &lt;- inner_join(blast_viral, mrg_assign, by=\"seq_id\") %&gt;%\n    mutate(taxid_match_bowtie = match_taxid(staxid, taxid),\n           taxid_match_kraken = match_taxid(staxid, assigned_taxid),\n           taxid_match_any = taxid_match_bowtie | taxid_match_kraken)\nblast_out &lt;- blast_assign %&gt;%\n  group_by(seq_id) %&gt;%\n  summarize(viral_status = ifelse(any(viral_full), 2,\n                                  ifelse(any(taxid_match_any), 2,\n                                             ifelse(any(viral), 1, 0))),\n            .groups = \"drop\")\n\n\n\nCode# Merge BLAST results with unenriched read data\nmrg_blast &lt;- full_join(mrg, blast_out, by=\"seq_id\") %&gt;%\n  mutate(viral_status = replace_na(viral_status, 0),\n         viral_status_out = ifelse(viral_status == 0, FALSE, TRUE))\n\n# Plot\ng_vhist_1 &lt;- g_vhist_base + geom_vhist(data=mrg_blast, mapping=aes(fill=viral_status_out)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Viral status\")\ng_vhist_1\n\n\n\n\n\n\n\nMy usual disjunctive score threshold of 20 gave precision, sensitivity, and F1 scores all &gt;96%:\n\nCodetest_sens_spec &lt;- function(tab, score_threshold){\n  tab_retained &lt;- tab %&gt;% \n    mutate(retain_score = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n           retain = assigned_hv | retain_score) %&gt;%\n    group_by(viral_status_out, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(viral_status_out != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(viral_status_out == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nrange_f1 &lt;- function(intab, inrange=15:45){\n  tss &lt;- purrr::partial(test_sens_spec, tab=intab)\n  stats &lt;- lapply(inrange, tss) %&gt;% bind_rows %&gt;%\n    pivot_longer(!threshold, names_to=\"metric\", values_to=\"value\")\n  return(stats)\n}\nstats_0 &lt;- range_f1(mrg_blast)\ng_stats_0 &lt;- ggplot(stats_0, aes(x=threshold, y=value, color=metric)) +\n  geom_vline(xintercept=20, color = \"red\", linetype = \"dashed\") +\n  geom_line() +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Adjusted Score Threshold\", expand = c(0,0)) +\n  scale_color_brewer(palette=\"Dark2\") +\n  theme_base\ng_stats_0\n\n\n\n\n\n\nCodestats_0 %&gt;% filter(threshold == 20) %&gt;% \n  select(Threshold=threshold, Metric=metric, Value=value)\n\n\n  \n\n\n\nHuman-infecting viruses: overall relative abundance\n\nCode# Get raw read counts\nread_counts_raw &lt;- basic_stats_raw %&gt;%\n  select(sample, n_reads_raw = n_read_pairs)\n\n# Get HV read counts\nmrg_hv &lt;- mrg %&gt;% mutate(hv_status = assigned_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;% left_join(read_counts_hv, by=\"sample\") %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0))\n\n# Aggregate\nread_counts_grp &lt;- read_counts %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_hv = sum(n_reads_hv), .groups=\"drop\") %&gt;%\n  mutate(sample= \"All samples\")\nread_counts_agg &lt;- bind_rows(read_counts, read_counts_grp) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/n_reads_raw,\n         sample = factor(sample, levels=c(levels(libraries$sample), \"All samples\")))\n\n\nApplying a disjunctive cutoff at S=20 identifies 162 read pairs as human-viral. This gives an overall relative HV abundance of \\(9.42 \\times 10^{-7}\\); higher than Ng and Bengtsson-Palme but lower than most other datasets I’ve analyzed with this pipeline:\n\nCode# Visualize\ng_phv_agg &lt;- ggplot(read_counts_agg, aes(x=sample)) +\n  geom_point(aes(y=p_reads_hv)) +\n  scale_y_log10(\"Relative abundance of human virus reads\") +\n  theme_kit\ng_phv_agg\n\n\n\n\n\n\n\n\nCode# Collate past RA values\nra_past &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                   \"Brumfield\", 5e-5, \"RNA\", FALSE,\n                   \"Brumfield\", 3.66e-7, \"DNA\", FALSE,\n                   \"Spurbeck\", 5.44e-6, \"RNA\", FALSE,\n                   \"Yang\", 3.62e-4, \"RNA\", FALSE,\n                   \"Rothman (unenriched)\", 1.87e-5, \"RNA\", FALSE,\n                   \"Rothman (panel-enriched)\", 3.3e-5, \"RNA\", TRUE,\n                   \"Crits-Christoph (unenriched)\", 1.37e-5, \"RNA\", FALSE,\n                   \"Crits-Christoph (panel-enriched)\", 1.26e-2, \"RNA\", TRUE,\n                   \"Prussin (non-control)\", 1.63e-5, \"RNA\", FALSE,\n                   \"Prussin (non-control)\", 4.16e-5, \"DNA\", FALSE,\n                   \"Rosario (non-control)\", 1.21e-5, \"RNA\", FALSE,\n                   \"Rosario (non-control)\", 1.50e-4, \"DNA\", FALSE,\n                   \"Leung\", 1.73e-5, \"DNA\", FALSE,\n                   \"Brinch\", 3.88e-6, \"DNA\", FALSE,\n                   \"Bengtsson-Palme\", 8.86e-8, \"DNA\", FALSE,\n                   \"Ng\", 2.90e-7, \"DNA\", FALSE\n)\n\n# Collate new RA values\nra_new &lt;- tribble(~dataset, ~ra, ~na_type, ~panel_enriched,\n                  \"Maritz\", 9.42e-7, \"DNA\", FALSE)\n\n\n# Plot\nscale_color_na &lt;- purrr::partial(scale_color_brewer, palette=\"Set1\",\n                                 name=\"Nucleic acid type\")\nra_comp &lt;- bind_rows(ra_past, ra_new) %&gt;% mutate(dataset = fct_inorder(dataset))\ng_ra_comp &lt;- ggplot(ra_comp, aes(y=dataset, x=ra, color=na_type)) +\n  geom_point() +\n  scale_color_na() +\n  scale_x_log10(name=\"Relative abundance of human virus reads\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_ra_comp\n\n\n\n\n\n\n\nHuman-infecting viruses: taxonomy and composition\nIn investigating the taxonomy of human-infecting virus reads, I restricted my analysis to samples with more than 5 HV read pairs total across all viruses, to reduce noise arising from extremely low HV read counts in some samples. 10 samples met this criterion.\nAt the family level, most samples were dominated by Adenoviridae, Polyomaviridae and Papillomaviridae. However, one sample, NYC-03, was overwhelmingly dominated by Herpesviridae:\n\nCode# Get viral taxon names for putative HV reads\nviral_taxa$name[viral_taxa$taxid == 249588] &lt;- \"Mamastrovirus\"\nviral_taxa$name[viral_taxa$taxid == 194960] &lt;- \"Kobuvirus\"\nviral_taxa$name[viral_taxa$taxid == 688449] &lt;- \"Salivirus\"\nviral_taxa$name[viral_taxa$taxid == 585893] &lt;- \"Picobirnaviridae\"\nviral_taxa$name[viral_taxa$taxid == 333922] &lt;- \"Betapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 334207] &lt;- \"Betapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 369960] &lt;- \"Porcine type-C oncovirus\"\nviral_taxa$name[viral_taxa$taxid == 333924] &lt;- \"Betapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 687329] &lt;- \"Anelloviridae\"\nviral_taxa$name[viral_taxa$taxid == 325455] &lt;- \"Gammapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 333750] &lt;- \"Alphapapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 694002] &lt;- \"Betacoronavirus\"\nviral_taxa$name[viral_taxa$taxid == 334202] &lt;- \"Mupapillomavirus\"\nviral_taxa$name[viral_taxa$taxid == 197911] &lt;- \"Alphainfluenzavirus\"\nviral_taxa$name[viral_taxa$taxid == 186938] &lt;- \"Respirovirus\"\nviral_taxa$name[viral_taxa$taxid == 333926] &lt;- \"Gammapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337051] &lt;- \"Betapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337043] &lt;- \"Alphapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 694003] &lt;- \"Betacoronavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 334204] &lt;- \"Mupapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 334208] &lt;- \"Betapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 333928] &lt;- \"Gammapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 337039] &lt;- \"Alphapapillomavirus 2\"\nviral_taxa$name[viral_taxa$taxid == 333929] &lt;- \"Gammapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 337042] &lt;- \"Alphapapillomavirus 7\"\nviral_taxa$name[viral_taxa$taxid == 334203] &lt;- \"Mupapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 333757] &lt;- \"Alphapapillomavirus 8\"\nviral_taxa$name[viral_taxa$taxid == 337050] &lt;- \"Alphapapillomavirus 6\"\nviral_taxa$name[viral_taxa$taxid == 333767] &lt;- \"Alphapapillomavirus 3\"\nviral_taxa$name[viral_taxa$taxid == 333754] &lt;- \"Alphapapillomavirus 10\"\nviral_taxa$name[viral_taxa$taxid == 687363] &lt;- \"Torque teno virus 24\"\nviral_taxa$name[viral_taxa$taxid == 687342] &lt;- \"Torque teno virus 3\"\nviral_taxa$name[viral_taxa$taxid == 687359] &lt;- \"Torque teno virus 20\"\nviral_taxa$name[viral_taxa$taxid == 194441] &lt;- \"Primate T-lymphotropic virus 2\"\nviral_taxa$name[viral_taxa$taxid == 334209] &lt;- \"Betapapillomavirus 5\"\nviral_taxa$name[viral_taxa$taxid == 194965] &lt;- \"Aichivirus B\"\nviral_taxa$name[viral_taxa$taxid == 333930] &lt;- \"Gammapapillomavirus 4\"\nviral_taxa$name[viral_taxa$taxid == 337048] &lt;- \"Alphapapillomavirus 1\"\nviral_taxa$name[viral_taxa$taxid == 337041] &lt;- \"Alphapapillomavirus 9\"\nviral_taxa$name[viral_taxa$taxid == 337049] &lt;- \"Alphapapillomavirus 11\"\nviral_taxa$name[viral_taxa$taxid == 337044] &lt;- \"Alphapapillomavirus 5\"\n\n# Filter samples and add viral taxa information\nsamples_keep &lt;- read_counts %&gt;% filter(n_reads_hv &gt; 5) %&gt;% pull(sample)\nmrg_hv_named &lt;- mrg_hv %&gt;% filter(sample %in% samples_keep, hv_status) %&gt;% left_join(viral_taxa, by=\"taxid\") \n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.02\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nIn investigating individual viral families, to avoid distortions from a few rare reads, I restricted myself to samples where that family made up at least 10% of human-viral reads:\n\nCodethreshold_major_species &lt;- 0.05\ntaxid_adeno &lt;- 10508\n\n# Get set of adenoviridae reads\nadeno_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_adeno) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nadeno_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_adeno, sample %in% adeno_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each adenoviridae species\nadeno_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% adeno_ids) %&gt;%\n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_adeno = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nadeno_species_major_tab &lt;- adeno_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_adeno == max(p_reads_adeno)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_adeno)) %&gt;% \n  filter(p_reads_adeno &gt; threshold_major_species)\nadeno_species_counts_major &lt;- adeno_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% adeno_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_adeno = sum(n_reads_hv),\n            p_reads_adeno = sum(p_reads_adeno), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(adeno_species_major_tab$name, \"Other\")))\nadeno_species_counts_display &lt;- adeno_species_counts_major %&gt;%\n  rename(p_reads = p_reads_adeno, classification = name_display)\n\n# Plot\ng_adeno_species &lt;- g_comp_base + \n  geom_col(data=adeno_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Adenoviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Adenoviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_adeno_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nadeno_species_collate &lt;- adeno_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_adeno), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_polyoma &lt;- 151341\n\n# Get set of polyomaviridae reads\npolyoma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_polyoma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npolyoma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_polyoma, sample %in% polyoma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each polyomaviridae species\npolyoma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% polyoma_ids) %&gt;%\n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_polyoma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npolyoma_species_major_tab &lt;- polyoma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_polyoma == max(p_reads_polyoma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_polyoma)) %&gt;% \n  filter(p_reads_polyoma &gt; threshold_major_species)\npolyoma_species_counts_major &lt;- polyoma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% polyoma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_polyoma = sum(n_reads_hv),\n            p_reads_polyoma = sum(p_reads_polyoma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(polyoma_species_major_tab$name, \"Other\")))\npolyoma_species_counts_display &lt;- polyoma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_polyoma, classification = name_display)\n\n# Plot\ng_polyoma_species &lt;- g_comp_base + \n  geom_col(data=polyoma_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Polyomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Polyomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_polyoma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npolyoma_species_collate &lt;- polyoma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_polyoma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_papilloma &lt;- 151340\n\n# Get set of papillomaviridae reads\npapilloma_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_papilloma) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\npapilloma_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_papilloma, sample %in% papilloma_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each papillomaviridae species\npapilloma_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% papilloma_ids) %&gt;%\n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_papilloma = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\npapilloma_species_major_tab &lt;- papilloma_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_papilloma == max(p_reads_papilloma)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_papilloma)) %&gt;% \n  filter(p_reads_papilloma &gt; threshold_major_species)\npapilloma_species_counts_major &lt;- papilloma_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% papilloma_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_papilloma = sum(n_reads_hv),\n            p_reads_papilloma = sum(p_reads_papilloma), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(papilloma_species_major_tab$name, \"Other\")))\npapilloma_species_counts_display &lt;- papilloma_species_counts_major %&gt;%\n  rename(p_reads = p_reads_papilloma, classification = name_display)\n\n# Plot\ng_papilloma_species &lt;- g_comp_base + \n  geom_col(data=papilloma_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Papillomaviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Papillomaviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_papilloma_species\n\n\n\n\n\n\nCode# Get most prominent species for text\npapilloma_species_collate &lt;- papilloma_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_papilloma), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\n\nCodethreshold_major_species &lt;- 0.1\ntaxid_herpes &lt;- 10292\n\n# Get set of herpesviridae reads\nherpes_samples &lt;- hv_family_counts %&gt;% filter(taxid == taxid_herpes) %&gt;%\n  filter(p_reads_hv &gt;= 0.1) %&gt;%\n  pull(sample)\nherpes_ids &lt;- hv_reads_family %&gt;% \n  filter(taxid == taxid_herpes, sample %in% herpes_samples) %&gt;%\n  pull(seq_id)\n\n# Count reads for each herpesviridae species\nherpes_species_counts &lt;- hv_reads_species %&gt;%\n  filter(seq_id %in% herpes_ids) %&gt;%\n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_herpes = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nherpes_species_major_tab &lt;- herpes_species_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_herpes == max(p_reads_herpes)) %&gt;% \n  filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_herpes)) %&gt;% \n  filter(p_reads_herpes &gt; threshold_major_species)\nherpes_species_counts_major &lt;- herpes_species_counts %&gt;%\n  mutate(name_display = ifelse(name %in% herpes_species_major_tab$name, \n                               name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_herpes = sum(n_reads_hv),\n            p_reads_herpes = sum(p_reads_herpes), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(herpes_species_major_tab$name, \"Other\")))\nherpes_species_counts_display &lt;- herpes_species_counts_major %&gt;%\n  rename(p_reads = p_reads_herpes, classification = name_display)\n\n# Plot\ng_herpes_species &lt;- g_comp_base + \n  geom_col(data=herpes_species_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% Herpesviridae Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral species\") +\n  labs(title=\"Species composition of Herpesviridae reads\") +\n  guides(fill=guide_legend(ncol=3)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\n\ng_herpes_species\n\n\n\n\n\n\nCode# Get most prominent species for text\nherpes_species_collate &lt;- herpes_species_counts %&gt;% group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv), p_reads_mean = mean(p_reads_herpes), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n\n\nI was a bit suspicious of this last result, given that it only occurred in one sample, but according to BLASTN, at least, these human gammaherpesvirus 4 (a.k.a. EBV) matches are real:\n\nCode# Configure\nref_taxids_hv &lt;- c(10376)\nref_names_hv &lt;- sapply(ref_taxids_hv, function(x) viral_taxa %&gt;% filter(taxid == x) %&gt;% pull(name) %&gt;% first)\np_threshold &lt;- 0.1\n\n# Get taxon names\ntax_names_path &lt;- file.path(data_dir, \"taxid-names.tsv.gz\")\ntax_names &lt;- read_tsv(tax_names_path, show_col_types = FALSE)\n\n# Add missing names\ntax_names_new &lt;- tribble(~staxid, ~name,\n                         3050295, \"Cytomegalovirus humanbeta5\",\n                         459231, \"FLAG-tagging vector pFLAG97-TSR\",\n                         257877, \"Macaca thibetana thibetana\",\n                         256321, \"Lentiviral transfer vector pHsCXW\",\n                         419242, \"Shuttle vector pLvCmvMYOCDHA\",\n                         419243, \"Shuttle vector pLvCmvLacZ\",\n                         421868, \"Cloning vector pLvCmvLacZ.Gfp\",\n                         421869, \"Cloning vector pLvCmvMyocardin.Gfp\",\n                         426303, \"Lentiviral vector pNL-GFP-RRE(SA)\",\n                         436015, \"Lentiviral transfer vector pFTMGW\",\n                         454257, \"Shuttle vector pLvCmvMYOCD2aHA\",\n                         476184, \"Shuttle vector pLV.mMyoD::ERT2.eGFP\",\n                         476185, \"Shuttle vector pLV.hMyoD.eGFP\",\n                         591936, \"Piliocolobus tephrosceles\",\n                         627481, \"Lentiviral transfer vector pFTM3GW\",\n                         680261, \"Self-inactivating lentivirus vector pLV.C-EF1a.cyt-bGal.dCpG\",\n                         2952778, \"Expression vector pLV[Exp]-EGFP:T2A:Puro-EF1A\",\n                         3022699, \"Vector PAS_122122\",\n                         3025913, \"Vector pSIN-WP-mPGK-GDNF\",\n                         3105863, \"Vector pLKO.1-ZsGreen1\",\n                         3105864, \"Vector pLKO.1-ZsGreen1 mouse Wfs1 shRNA\",\n                         3108001, \"Cloning vector pLVSIN-CMV_Neo_v4.0\",\n                         3109234, \"Vector pTwist+Kan+High\",\n                         3117662, \"Cloning vector pLV[Exp]-CBA&gt;P301L\",\n                         3117663, \"Cloning vector pLV[Exp]-CBA&gt;P301L:T2A:mRuby3\",\n                         3117664, \"Cloning vector pLV[Exp]-CBA&gt;hMAPT[NM_005910.6](ns):T2A:mRuby3\",\n                         3117665, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3\",\n                         3117666, \"Cloning vector pLV[Exp]-CBA&gt;mRuby3/NFAT3 fusion protein\",\n                         3117667, \"Cloning vector pLV[Exp]-Neo-mPGK&gt;{EGFP-hSEPT6}\",\n                         438045, \"Xenotropic MuLV-related virus\",\n                         447135, \"Myodes glareolus\",\n                         590745, \"Mus musculus mobilized endogenous polytropic provirus\",\n                         181858, \"Murine AIDS virus-related provirus\",\n                         356663, \"Xenotropic MuLV-related virus VP35\",\n                         356664, \"Xenotropic MuLV-related virus VP42\",\n                         373193, \"Xenotropic MuLV-related virus VP62\",\n                         286419, \"Canis lupus dingo\",\n                         415978, \"Sus scrofa scrofa\",\n                         494514, \"Vulpes lagopus\",\n                         3082113, \"Rangifer tarandus platyrhynchus\",\n                         3119969, \"Bubalus kerabau\")\ntax_names &lt;- bind_rows(tax_names, tax_names_new)\n\n# Get matches\nhv_blast_staxids &lt;- hv_reads_species %&gt;% filter(taxid %in% ref_taxids_hv) %&gt;%\n  group_by(taxid) %&gt;% mutate(n_seq = n()) %&gt;%\n  left_join(blast_paired, by=\"seq_id\") %&gt;%\n  mutate(staxid = as.integer(staxid)) %&gt;%\n  left_join(tax_names %&gt;% rename(sname=name), by=\"staxid\")\n\n# Count matches\nhv_blast_counts &lt;- hv_blast_staxids %&gt;%\n  group_by(taxid, name, staxid, sname, n_seq) %&gt;%\n  count %&gt;% mutate(p=n/n_seq)\n\n# Subset to major matches\nhv_blast_counts_major &lt;- hv_blast_counts %&gt;% \n  filter(n&gt;1, p&gt;p_threshold, !is.na(staxid)) %&gt;%\n  arrange(desc(p)) %&gt;% group_by(taxid) %&gt;%\n  filter(row_number() &lt;= 25) %&gt;%\n  mutate(name_display = ifelse(name == ref_names_hv[1], \"EBV\", name))\n\n# Plot\ng_hv_blast &lt;- ggplot(hv_blast_counts_major, mapping=aes(x=p, y=sname)) +\n  geom_col() +\n  facet_grid(name_display~., scales=\"free_y\", space=\"free_y\") +\n  scale_x_continuous(name=\"% mapped reads\", limits=c(0,1), \n                     breaks=seq(0,1,0.2), expand=c(0,0)) +\n  theme_base + theme(axis.title.y = element_blank())\ng_hv_blast\n\n\n\n\n\n\n\nFinally, here again are the overall relative abundances of the specific viral genera I picked out manually in my last entry:\n\nCode# Define reference genera\npath_genera_rna &lt;- c(\"Mamastrovirus\", \"Enterovirus\", \"Salivirus\", \"Kobuvirus\", \"Norovirus\", \"Sapovirus\", \"Rotavirus\", \"Alphacoronavirus\", \"Betacoronavirus\", \"Alphainfluenzavirus\", \"Betainfluenzavirus\", \"Lentivirus\")\npath_genera_dna &lt;- c(\"Mastadenovirus\", \"Alphapolyomavirus\", \"Betapolyomavirus\", \"Alphapapillomavirus\", \"Betapapillomavirus\", \"Gammapapillomavirus\", \"Orthopoxvirus\", \"Simplexvirus\",\n                     \"Lymphocryptovirus\", \"Cytomegalovirus\", \"Dependoparvovirus\")\npath_genera &lt;- bind_rows(tibble(name=path_genera_rna, genome_type=\"RNA genome\"),\n                         tibble(name=path_genera_dna, genome_type=\"DNA genome\")) %&gt;%\n  left_join(viral_taxa, by=\"name\")\n\n# Count in each sample\nmrg_hv_named_all &lt;- mrg_hv %&gt;% left_join(viral_taxa, by=\"taxid\")\nhv_reads_genus_all &lt;- raise_rank(mrg_hv_named_all, viral_taxa, \"genus\")\nn_path_genera &lt;- hv_reads_genus_all %&gt;% \n  group_by(sample, name, taxid) %&gt;% \n  count(name=\"n_reads_viral\") %&gt;% \n  inner_join(path_genera, by=c(\"name\", \"taxid\")) %&gt;%\n  left_join(read_counts_raw, by=c(\"sample\")) %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n# Pivot out and back to add zero lines\nn_path_genera_out &lt;- n_path_genera %&gt;% ungroup %&gt;% select(sample, name, n_reads_viral) %&gt;%\n  pivot_wider(names_from=\"name\", values_from=\"n_reads_viral\", values_fill=0) %&gt;%\n  pivot_longer(-sample, names_to=\"name\", values_to=\"n_reads_viral\") %&gt;%\n  left_join(read_counts_raw, by=\"sample\") %&gt;%\n  left_join(path_genera, by=\"name\") %&gt;%\n  mutate(p_reads_viral = n_reads_viral/n_reads_raw)\n\n## Aggregate across dates\nn_path_genera_stype &lt;- n_path_genera_out %&gt;% \n  group_by(name, taxid, genome_type) %&gt;%\n  summarize(n_reads_raw = sum(n_reads_raw),\n            n_reads_viral = sum(n_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(sample=\"All samples\", location=\"All locations\",\n         p_reads_viral = n_reads_viral/n_reads_raw,\n         na_type = \"DNA\")\n\n# Plot\ng_path_genera &lt;- ggplot(n_path_genera_stype,\n                        aes(y=name, x=p_reads_viral)) +\n  geom_point() +\n  scale_x_log10(name=\"Relative abundance\") +\n  facet_grid(genome_type~., scales=\"free_y\") +\n  theme_base + theme(axis.title.y = element_blank())\ng_path_genera\n\n\n\n\n\n\n\nConclusion\nI’ve had trouble with this dataset previously, so I was surprised at how well this analysis went. It seems the improvements I’ve made to the pipeline over the last couple of months have really had an effect. Like other DNA wastewater datasets I’ve looked at recently, this one (a) has very low HV relative abundance overall, and (b) shows a very high preponderance of human mastadenovirus F. I have one more DNA dataset from the P2RA study to analyze with this pipeline, so we’ll see if this pattern persists there."
  },
  {
    "objectID": "notebooks/2024-05-07_munk.html",
    "href": "notebooks/2024-05-07_munk.html",
    "title": "Workflow analysis of Munk et al. (2022)",
    "section": "",
    "text": "The final dataset from the P2RA dataset I want to analyze here is Munk et al. (2022), an enormous dataset of &gt;1,000 raw influent samples from 101 countries collected between 2016 and 2019. As in previous DNA studies like Bengtsson-Palme, samples were centrifuged and only the pellet was retained for sequencing, so we expect viral abundance to be low; nevertheless, this is the largest and most comprehensive DNA wastewater dataset we’ve been able to find to date, so it’s worth having a look at what’s in it. The pellet from each sample was resuspended, was homogenized with bead-beating, underwent DNA extraction and library prep, and was sequenced using Illumina technology; earlier samples were sequenced on an Illumina HiSeq3000, while later samples were sequenced on a NovaSeq6000, both with 2x150bp reads."
  },
  {
    "objectID": "notebooks/2024-05-07_munk.html#footnotes",
    "href": "notebooks/2024-05-07_munk.html#footnotes",
    "title": "Workflow analysis of Munk et al. (2022)",
    "section": "Footnotes",
    "text": "Footnotes\n\nI wasn’t able to quickly find any HDI datasets other than the most recent one, and it didn’t seem worth doing serious digging for this quick analysis.↩︎"
  },
  {
    "objectID": "notebooks/2024-06-11_batch.html",
    "href": "notebooks/2024-06-11_batch.html",
    "title": "Setting up AWS Batch to work with the NAO’s MGS workflow",
    "section": "",
    "text": "AWS Batch is an incredibly powerful tool for running Nextflow workflows in a highly parallelized and automated manner, but it’s also confusing to set up and run. In this entry, I try to break down the process in a way that makes it simpler for teammates and other interested parties to run our own workflows using Batch.\nThe below represents the decisions I made while setting up my first Batch workflow, and my recommendations for others trying to do the same. Others in significantly different circumstances would do well to try and understand the process more deeply before simply following everything I’ve written here."
  },
  {
    "objectID": "notebooks/2024-06-11_batch.html#footnotes",
    "href": "notebooks/2024-06-11_batch.html#footnotes",
    "title": "Setting up AWS Batch to work with the NAO’s MGS workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn more depth, you need the following actions to be enabled for the bucket in question for your IAM user or role: s3:ListBucket, s3:GetBucketLocation, s3:GetObject, s3:GetObjectAcl, s3:PutObject, s3:PutObjectAcl, s3:PutObjectTagging, and s3:DeleteObject. If you’re using a bucket specific to your user, all this is easier if you first have your administrator enable s3:GetBucketPolicy and s3:PutBucketPolicy for your user.↩︎\nIf you want even more IOPS, you can provision an io2 volume instead of gp3. However, that’s beyond the scope of this guide.↩︎\nIn the future, I’ll investigate running Batch with Fargate for Nextflow workflows. For now, using EC2 gives us greater control over configuration than Fargate, at the cost of additional setup complexity and occasional startup delays.↩︎\nOn the latest version of the v2 pipeline, most of these options are already configured appropriately when running the pipeline with -profile batch (which is also the default profile). In this case, you only need to change process.queue to point to the name of your job queue.↩︎"
  },
  {
    "objectID": "notebooks/2024-07-01_partial-hv.html",
    "href": "notebooks/2024-07-01_partial-hv.html",
    "title": "Investigating the v2 pipeline’s human-virus assignment behavior",
    "section": "",
    "text": "One question that came up in a recent team meeting about the refactored v2 pipeline is how higher-level taxa are handled during human-virus identification.\nAs a reminder, the process for HV identification currently looks like this:\nThis all works well for reads that are assigned to a taxon that is entirely composed of human-infecting viruses. The question is, what about reads that Kraken assigns to taxa that are only partially human-infecting? For example, the virus family Coronaviridae (taxid 11118) contains several coronaviruses that infect humans (e.g. SARS-CoV-2) and many that do not (e.g. assorted bat coronaviruses). What would happen to a read that was assigned by Kraken2 to this taxid?\nThe key process here is PROCESS_KRAKEN_HV (modules/local/processKrakenHV/main.nf), which calls bin/process_kraken_hv.py on the output of Kraken2. This script:\nAs such, the script checks whether the assigned taxid or any of its ancestors are HV taxa, but not whether any of its descendents are. Reads that are assigned to higher-level taxa will thus be treated as though they were assigned to a non-HV taxa, and filtered out during HV read identification.\nThis seems suboptimal. That said, it’s not obvious what the correct behavior is here; treating reads assigned to partially-HV taxa as HV comes with its own problems1. Someone should think more about what the right approach is here."
  },
  {
    "objectID": "notebooks/2024-07-01_partial-hv.html#footnotes",
    "href": "notebooks/2024-07-01_partial-hv.html#footnotes",
    "title": "Investigating the v2 pipeline’s human-virus assignment behavior",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn particular, since the Bowtie2 database used for initial screening only contains HV genomes, this approach would likely lead to closely-related non-human-infecting virus reads being classified as HV.↩︎"
  }
]