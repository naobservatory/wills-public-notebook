[
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260280",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/280",
    "text": "Nanodrop 260/280\nThe standard measures of nucleic acid purity from Nanodrop measurements are the 260/280 and 260/230 absorption ratios. The standard advice is that, for a pure RNA sample, 260/280 should be around 2.0, while pure DNA should be around 1.8. Lower ratios are typically indicative of contamination with protein, or some specific contaminants such as phenol.\nThe 260/280 ratios measured for the DNase-treated samples are as follows:\n\nCodeg_260_280 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.8, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 1.9, ymax = 2.1, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_280), shape = 16) +\n  scale_y_continuous(limits = c(0.8, 2.3), breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/280nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_280\n\n\n\nFigure 5: 260/280 ratios for DNase-treated samples.\n\n\n\nNone of the kits returned 260/280 ratios at the expected level for pure RNA, though most were not drastically low. Several returned ratios close to (though below) 1.8, which might be indicative of significant residual DNA in the sample; it’s hard to distinguish between this and protein contamination without doing DNA Qubit measurements on the DNase-treated samples. Free dNTP nucleotides produced by the DNase treatment still absorb at DNA-like ratios, so DNA-like readings may not be indicative of poor quality here.\nAs with yield, the Qiagen AllPrep PowerViral kit does notably worse than the other kits."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodrop-260230",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop 260/230",
    "text": "Nanodrop 260/230\nThe 260/230 absorption ratio gives a useful indicator of the presence of various contaminants, including phenol, carbohydrates, guanidine, and various salts. The standard advice is that a pure sample should have a 260/230 of around 2.0-2.2, with lower ratios indicating contamination. Readings for our DNase-treated samples are as follows:\n\nCodeg_260_230 &lt;- ggplot(data) +\n  geom_rect(ymin = 1.9, ymax = 2.3, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_rect(ymin = 2.0, ymax = 2.2, xmin = 0, xmax = Inf, fill = \"black\", alpha = 0.02) +\n  geom_point(aes(x=kit, y=nanodrop_rna_260_230), shape = 16) +\n  scale_y_continuous(#limits = c(0.8, 2.3),\n                     breaks = seq(0,5,0.2),\n                     name = \"Absorption ratio (260nm/230nm)\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_260_230\n\n\n\nFigure 6: 260/230 ratios for DNase-treated samples.\n\n\n\nAs we can see, many kits have significantly lower 260/230 ratios than the recommended range, indicating contamination – most likely with guanidinium salts involved in nucleic acid extraction. Combined with the 260/280 results above, this suggests that many kits would benefit from an additional cleanup step. As with other metrics, the Qiagen AllPrep PowerViral kit comes out looking worst.\nTwo kits (NucleoSpin Virus and Zymo quick-RNA) have at least one replicate with significantly higher 260/230 ratios than the recommended values. It’s not very clear what could cause this, but one resource I found suggested misblanking as the likely culprit. I’d recommend repeating the nanodrop measurements for these samples to see if the pattern persists."
  },
  {
    "objectID": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "href": "notebooks/2023-09-12_settled-solids-extraction-test.html#nanodropqubit-ratio",
    "title": "Extraction experiment 2: high-level results & interpretation",
    "section": "Nanodrop/Qubit ratio",
    "text": "Nanodrop/Qubit ratio\nAn informal heuristic measurement of quality is to compare the measured RNA concentration with Qubit (which is highly specific) to Nanodrop (which is not); the higher the latter compared to the former, the more other material is likely contributing to the Nanodrop reading. The ratios for our DNase-treated samples are as follows:\n\nCodeg_ratio &lt;- ggplot(data, aes(x=kit,y=qubit_nanodrop_ratio)) +\n  geom_point(shape = 16) + \n  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2), \n                     name = \"RNA concentration ratio (Qubit/Nanodrop)\", expand = c(0,0)) +\n  scale_x_discrete(name = \"Extraction Kit\") +\n  theme_base + \n  theme(axis.text.x = element_text(hjust = 1, vjust = 1, angle = 45),\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\")\ng_ratio\n\n\n\nFigure 7: Nanodrop/Qubit ratios for DNase-treated samples.\n\n\n\nAccording to this metric, the Invitrogen PureLink RNA, QIAamp MinElute Virus, and Zymo quick-RNA kits come out looking relatively good, while the Qiagen AllPrep PowerViral kit comes out looking worst."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will's Public NAO Notebook",
    "section": "",
    "text": "Automating BLAST validation of human viral read assignment\n\n\nExperiments with BLASTN remote mode\n\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n  \n\n\n\n\nProject Runway RNA-seq testing data: removing livestock reads\n\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWorkflow analysis of Project Runway RNA-seq testing data\n\n\nApplying a new workflow to some oldish data.\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEstimating the effect of read depth on duplication rate for Project Runway DNA data\n\n\nHow deep can we go?\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nComparing viral read assignments between pipelines on Project Runway data\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInitial analysis of Project Runway protocol testing data\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n  \n\n\n\n\nComparing options for read deduplication\n\n\nClumpify vs fastp\n\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nComparing Ribodetector and bbduk for rRNA detection\n\n\nIn search of quick rRNA filtering.\n\n\n\n\n\n\nOct 16, 2023\n\n\n\n\n\n\n  \n\n\n\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\n\n\nTwo tools – how do they perform?\n\n\n\n\n\n\nOct 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow does Element AVITI sequencing work?\n\n\nFindings of a shallow investigation\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExtraction experiment 2: high-level results & interpretation\n\n\nComparing RNA yields and quality across extraction kits for settled solids\n\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "href": "notebooks/2023-10-12_fastp-vs-adapterremoval.html",
    "title": "Comparing FASTP and AdapterRemoval for MGS pre-processing",
    "section": "",
    "text": "The first major step in our current MGS pipeline uses AdapterRemoval to automatically identify and remove sequencing adapters, as well as trimming low-quality bases and collapsing overlapping read pairs (it can also discard low-quality reads entirely, but our current pipeline doesn’t use this). An alternative tool, that can do all of this as well as read deduplication, is fastp. I asked the pipeline’s current primary maintainer if there was a good reason we were using one tool instead of the other, and he said that there wasn’t. So I decided to do a shallow investigation of their relative behavior on some example MGS datasets to see how they compare.\nThe data\nTo carry out this test, I selected three pairs of raw Illumina FASTQC files, corresponding to one sample each from two different published studies as well as one dataset provided to us by Marc Johnson:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR19607374\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nFor each sample, I generated FASTQC report files for the raw data, then ran FASTP and AdapterRemoval independently on the FASTQ files and tabulated the results\nThe commands\nFor processing with FASTP, I ran the following command:\nfastp -i &lt;raw-reads-1&gt; -I &lt;raw-reads-2&gt; -o &lt;output-path-1&gt; -O &lt;output-path-2&gt; --failed_out &lt;output-path-failed-reads&gt; --cut_tail --correction\n(I didn’t run deduplication for this test, as AdapterRemoval doesn’t have that functionality.)\nFor processing with AdapterRemoval, I first ran the following command to identify adapters:\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --identify-adapters --threads 4 &gt; adapter_report.txt\nI then ran the following command to actually carry out pre-processing, using the adapter sequences identified in the previous step (NB the minlength and maxns values are chosen to match the FASTP defaults):\nAdapterRemoval --file1 &lt;raw-reads-1&gt; --file2 &lt;raw-reads-2&gt; --basename &lt;output-prefix&gt; --adapter1 &lt;adapter1&gt; --adapter2 &lt;adapter2&gt; --gzip --trimns --trimqualities --minlength 15 --maxns 5\n1. Rothman et al. (SRR19607374)\nThis sample from Rothman et al. contains 11.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 39 seconds.\nFASTP detected and trimmed adapters on 3.88M reads (note: not read pairs).\nA total of 133 Mb of sequence was trimmed due to adapter trimming, and 55 Mb due to other trimming processes, for a total of 188 Mb of trimmed sequence.\nA total of 367,938 read pairs were discarded due to failing various filters, leaving 11.21M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 323.9 seconds (a bit under 5.5 minutes).\nAdapterRemoval detected and trimmed adapters on 3.96M reads (note: again, not read pairs).\nA total of 135 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 2,347 read pairs were discarded due to failing various filters, leaving the final read number almost unchanged.\n\n\nCode# Calculate read allocations for Rothman\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed = c(1748896043+1748896043,1627794563+1627794563,1681118328+1681115431)\nbp_discarded = c(0,54014538,276107+271850)\nbp_trimmed = c(0,bp_passed[1]-bp_passed[2]-bp_discarded[2],bp_passed[1]-bp_passed[3]-bp_discarded[3])\n# Tabulate\ntab_rothman &lt;- tibble(status = status, bp_passed = bp_passed, bp_discarded = bp_discarded, bp_trimmed = bp_trimmed)\ntab_rothman\n\n\n\n  \n\n\nCode# Visualize\ntab_rothman_gathered &lt;- gather(tab_rothman, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_rothman &lt;- ggplot(tab_rothman_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_rothman\n\n\n\n\nFASTQC results:\n\nPrior to adapter removal with either tool, the sequencing reads appear good quality, with a consistent average quality score of 30 across all bases in the forward read and ~29 in the reverse read. FASTP successfully raises the average quality score in the reverse read to 30 through trimming and read filtering, while AdapterRemoval leaves it unchanged.\nFASTQC judges the data to have iffy sequence composition (%A/C/G/T); neither tool affects this much.\nAll reads in the raw data are 151bp long; unsurprisingly, trimming by both tools results in a left tail in the sequencing length distribution that was absent in the raw data.\nAs previously observed, the raw data has very high duplicate levels, with only ~26% of sequences estimated by FASTQC to remain after deduplication. Increasing the comparison window to 100bp (from a default of 50bp) increases this to ~35%. Neither tool has much effect on this number – unsurprisingly, since neither carried out deduplication.\nFinally, adapter removal. Unsurprisingly, the raw data shows substantial adapter content. AdapterRemoval does a good job of removing adapters, resulting in a “pass” grade from FASTQC. Surprisingly, despite trimming adapters from fewer reads, fastp does even better (according to FASTQC) at removing adapters.\n\nThe images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n2. Crits-Christoph et al. (SRR23998357)\nThis sample from Rothman et al. contains 48.46M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 99 seconds.\nFASTP detected and trimmed adapters on 13.41M reads (note: not read pairs).\nA total of 270 Mb of sequence was trimmed due to adapter trimming, and 43 Mb due to other trimming processes, for a total of 313 Mb of trimmed sequence.\nA total of 1.99M read pairs were discarded due to failing various filters, leaving 47.47M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 1041.3 seconds (a bit over 17 minutes).\nAdapterRemoval detected and trimmed adapters on 8.22M reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 32,381 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\n\nCode# Calculate read allocations for CritsCristoph\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_cc = c(3683175308+3683175308,3465517525+3467624847,3634186441+3634143668)\nbp_discarded_cc = c(0,120701257,2057612+2224529)\nbp_trimmed_cc = c(0,bp_passed_cc[1]-bp_passed_cc[2]-bp_discarded_cc[2],bp_passed_cc[1]-bp_passed_cc[3]-bp_discarded_cc[3])\n# Tabulate\ntab_cc &lt;- tibble(status = status, bp_passed = bp_passed_cc, bp_discarded = bp_discarded_cc, bp_trimmed = bp_trimmed_cc)\ntab_cc\n\n\n\n  \n\n\nCode# Visualize\ntab_cc_gathered &lt;- gather(tab_cc, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_cc_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nFASTQC results:\n\nAs with Rothman, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp in this case) and high numbers of duplicates. They also, unsurprisingly, have high adaptor content.\nAs with Rothman, fastp successfully improves read quality scores, while AdapterRemoval has little effect. Also as with Rothman, neither tool (as configured) has much effect on sequence composition or duplicates.\n\nIn this case, fastp is highly effective at removing adapter sequences, while AdapterRemoval is only weakly effective. I wonder if I misconfigured AR somehow, because I’m surprised at how many adapter sequences remain in this case. The images below show raw, fastp, and AR adapter content:\n\n\n\n\n\n\n3. Johnson (COMO4)\nThis sample from Johnson contains 15.58M read pairs in the raw FASTQ files.\nFASTP:\n\nRunning FASTP took a total of 33 seconds.\nFASTP detected and trimmed adapters on 158,114 reads (note: not read pairs).\nA total of 1.3 Mb of sequence was trimmed due to adapter trimming, and 14.6 Mb due to other trimming processes, for a total of 15.9 Mb of trimmed sequence.\nA total of 0.33M read pairs were discarded due to failing various filters, leaving 15.25M read pairs remaining.\n\nAdapterRemoval:\n\nRunning AdapterRemoval took a total of 311.4 seconds (a bit over 5 minutes).\nAdapterRemoval detected and trimmed adapters on 155,360 reads (note: again, not read pairs).\nA total of 93.7 Mb of sequence was trimmed across all reads; the information isn’t provided to distinguish trimmed adapter sequences vs other trimming.\nOnly 5,512 read pairs were discarded due to failing various filters, leaving the final read number (again) almost unchanged.\n\nFASTQC results:\n\nAs with previous samples, the raw data shows good sequence quality (though with some tailing off at later read positions), poor sequence composition, uniform read length (76bp again) and high numbers of duplicates.\nUnlike previous samples, the raw data for this sample shows very low adapter content – plausibly they underwent adapter trimming before they were sent to us?\nNeither tool achieves much visible improvement on adapter content – unsurprisingly, given the very low levels in the raw data.\n\n\nCode# Calculate read allocations for Johnson\nstatus = c(\"raw\", \"fastp\", \"AdapterRemoval\")\nbp_passed_como = c(1183987584+1183987584,1157539804+1157540364,1182999562+1182970312)\nbp_discarded_como = c(0,36950446,418230+257734)\nbp_trimmed_como = c(0,bp_passed_como[1]-bp_passed_como[2]-bp_discarded_como[2],bp_passed_como[1]-bp_passed_como[3]-bp_discarded_como[3])\n# Tabulate\ntab_como &lt;- tibble(status = status, bp_passed = bp_passed_como, bp_discarded = bp_discarded_como, bp_trimmed = bp_trimmed_como)\ntab_como\n\n\n\n  \n\n\nCode# Visualize\ntab_como_gathered &lt;- gather(tab_como, key = sequence_group, value = bp, -status) |&gt;\n  mutate(sequence_group = sub(\"bp_\", \"\", sequence_group),\n         sequence_group = factor(sequence_group, \n                                 levels = c(\"passed\", \"trimmed\", \"discarded\")),\n         status = factor(status, levels = c(\"raw\", \"fastp\", \"AdapterRemoval\")))\ng_cc &lt;- ggplot(tab_como_gathered, aes(x=status, y=bp, fill = sequence_group)) +\n  geom_col(position = \"stack\") + scale_fill_brewer(palette = \"Dark2\") + \n  theme_base + theme(axis.title.x = element_blank())\ng_cc\n\n\n\n\nDeduplication with fastp\nGiven that all three of these samples contain high levels of sequence duplicates, I was curious to see to what degree fastp was able to improve on this metric. To test this, I reran fastp on all three samples, with the --dedup option enabled. I observed the following:\n\nRuntimes were consistently very slightly longer than without deduplication.\nThe number of successful output reads declined from 11.21M to 9.29M for the Rothman sample, from 47.47M to 31.47M, and from 15.25M to 11.03M for the Johnson sample.\nRelative to the raw data, and using the default FASTQC settings, the predicted fraction of reads surviving deduplication rose from 26% to 29% for the Rothman sample, from 45% to 64% for the Crits-Cristoph sample, and from 26% to 32% for the Johnson sample, following fastp deduplication. That is to say, by this metric, deduplication was mildly but not very effective.\nThis relative lack of efficacy may simply be because FASTP identifies duplicates as read pairs that are entirely identical in sequence, while FASTQC only looks at the first 50 base pairs of each read in isolation.\nI think I need to learn more about read duplicates and deduplication before I have strong takeaways here.\nConclusions\nTaken together, I think these data make a decent case for using FASTP, rather than AdapterRemoval, for pre-processing and adapter trimming.\n\nFASTP is much faster than AdapterRemoval.\nFor those samples with high adapter content, FASTP appeared more effective than AdapterRemoval at removing adapters, at least for those adapter sequences that could be detected by FASTQC.\nFASTP appears to be more aggressive at quality trimming reads than AdapterRemoval, resulting in better read quality distributions in FASTQC.\nFASTP provides substantially more functionality than AdapterRemoval, making it easier for us to add additional preprocessing steps like read filtering and (some) deduplication down the line.\n\nHowever, one important caveat is that it’s unclear how well either tool will perform on Element sequencing data – or how well FASTQC will be able to detect Element adapters that remain after preprocessing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html",
    "title": "How does Element AVITI sequencing work?",
    "section": "",
    "text": "In September 2023, the NAO team sent several samples to the MIT BioMicro Center, for library preparation and sequencing using their new Element AVITI sequencer. This machine works on quite different principles from Illumina sequencing, but also produces high-volume, paired-end, high-accuracy short reads. Since it looks like we might be using this machine quite a lot in the future, it pays to understand what it's doing. However, I found most quick explanations of Element sequencing much harder to follow than equivalent explanations of Illumina's sequencing technology (e.g. here).\nTo try and understand this better, I dug deeper, using a combination of talks by Element staff on YouTube, their core methods paper, and aggressive interrogation of Claude 2. Given my difficulty understanding this, I figured others on the team might also benefit from a quick-ish write-up of my current best understanding, presented here. Note that this does not go into the performance of Element sequencing, only the underlying mechanisms. Note also that, given the lack of very detailed documentation about many aspects of the process, my understanding here is inevitably more high-level than it would be for e.g. Illumina sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#a.-background-and-justification",
    "title": "How does Element AVITI sequencing work?",
    "section": "4a. Background and justification",
    "text": "4a. Background and justification\n\nWhen a polymerase binds a DNA strand, it first positions itself over the boundary between the double-stranded primer region and the single-stranded template region. It then recruits and positions a nucleotide complementary to the first base of the template region, using a combination of base pairing and direct interactions between the nucleotide and the polymerase enzyme itself. Finally, it incorporates the new nucleotide into the elongating daughter strand by connecting it to the end of that strand via a new phosphodiester bond.\n\nUsually, the polymerase then repeats the cycle by recruiting and incorporating a nucleotide complementary to the next base of the template strand; however, if the incorporated nucleotide is a chain terminator, it is unable to do this, and stalls.\n\nIn Illumina sequencing, the terminator nucleotide incorporated by the polymerase is fluorescently labeled, and is imaged following incorporation. The fluorophore is then cleaved off along with the terminator group, and the cycle repeats. As a result, the process of daughter strand elongation and base calling are closely bound together.\nIn Element sequencing, the goal is to separate the processes of daughter strand elongation (above) and base calling, so that the two can be optimized separately. To achieve this, the aim is to call the next unincorporated position in the template sequence, rather than (as in Illumina sequencing) the most recently incorporated position.\nOne theoretical way to do this would be to use an engineered polymerase that is able to recruit complementary nucleotides but not incorporate them. One could supply this polymerase with fluorescent nucleotides, and it would recruit the one complementary to the next position on the template strand. This would occur simultaneously at many different locations on each polony, corresponding to the different copies of the library sequence produced by RNA. One could then image the flow cell to identify the nucleotide type recruited at each polony.\nThe problem with the above approach is low signal persistence. Without incorporation, recruitment of nucleotides by the polymerase is weak and transient: the nucleotide binds its complementary base and the polymerase, remains for a short time, then dissociates. The result is that, for any given polony, too few nucleotides are recruited at any one time to give a sufficient signal for imaging.\nIn order for an approach like this to work, then, we need a way to improve signal persistence without relying on covalent incorporation of nucleotides. Enter avidity sequencing."
  },
  {
    "objectID": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "href": "notebooks/2023-10-12_how-does-element-sequencing-work.html#b.-base-calling-by-avidity",
    "title": "How does Element AVITI sequencing work?",
    "section": "4b. Base calling by avidity",
    "text": "4b. Base calling by avidity\n\nThe avidity of a molecular interaction is the accumulated strength of that interaction across multiple separate noncovalent bonds. Even if any single one of these bonds is weak and transient, the overall interaction can be strong and stable if the two molecules interact at many different points.\nIn Element avidity sequencing, the avidite is a large molecular construct, comprising a fluorescently labeled protein core connected to some number of (identical) nucleotides via flexible linker regions. Each of these nucleotide groups can be independently recruited by a polymerase bound to a polony, and positioned based on base-pairing interactions. While each of these nucleotide:template:polymerase interactions is too weak and transient to sustain a strong signal, the avidite as a whole is bound to the polony via multiple such interactions, producing a strong and stable interaction overall.\n\n\nExample avidite structure from the avidity sequencing paper. The core of the molecule consists of fluorescently labeled streptavidin, bound to linker regions via streptavidin:biotin interactions. Three of the four linkers shown here end in nucleotides (specifically, adenosine); the fourth mediates core:core interactions to produce an even larger avidite complex.\n\nExample avidite arm structure, with biotin at one end (top-left) and adenosine at the other (bottom-right).\n\nThe base-calling phase of the avidity sequencing cycle thus proceeds as follows:\n\nPrior to the base-calling phase, the polymerase and nucleotides involved in the elongation phase are detached and washed away.\nThe flow cell is then washed with a mixture containing an engineered polymerase as well as four fluorescently-labeled avidites (one each for A, C, G and T). The engineered polymerase (henceforth the avidite-binding polymerase, or ABP) is distinct from that used for elongation, and is capable of binding a template strand and recruiting a complementary nucleotide, but not capable of incorporation.\nThe ABPs bind to the double-stranded regions of each polony and position themselves at the boundary with the single-stranded template region. They then attempt to recruit nucleotides complementary to the next position on the template strand. The only nucleotides available are those attached to the avidites, which are thus recruited. \nSince each copy of the template sequence in each polony is synchronized, each polymerase bound to each polony attempts to recruit the same nucleotide type, and thus interacts with the same type of avidite. Each avidite molecule is thus recruited to multiple points on the polony, producing a stable overall interaction.\n\n\n\nMultiple copies of the same avidite molecule are thus recruited to each polony, producing a strong and uniform fluorescent signal.\n\n\n\nThe flow cell is then imaged to identify the avidite bound to each polony, and thus the next nucleotide in each read. After this, the ABPs and avidites are detached and washed away, and the cycle proceeds to the next elongation phase (see above)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html",
    "href": "notebooks/2023-10-13_rrna-removal.html",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "",
    "text": "See also:\nA useful step in processing wastewater MGS data is the removal of (primarily bacterial) ribosomal RNA sequences. These often make up a substantial fraction of all reads in the dataset, and their removal can both speed up downstream processing and potentially improve certain downstream metrics (e.g. library complexity). Our current pipeline counts and lists rRNA reads using Ribodetector, a deep-learning-based tool that is sensitive and specific, but slow. This slowness makes it annoying to work Ribodetector into our broader pipeline – for example, the time taken to classify reads with Ribodetector is much more than the time saved on downstream steps by excluding rRNA reads from our pipeline.\nI wanted to see if there were alternative rRNA detection methods that gave good-enough results while being fast enough to include in the preprocessing phase of the pipeline. To that end, I investigated bbduk, a k-mer based contamination-detection approach suggested in this biostars thread1.\nTo compare these tools, I applied bbduk and Ribodetector to three samples from pre-existing wastewater metagenomics datasets. Two of these, from Johnson and Crits-Christoph, were the same as in my last methods comparison post. I initially also used the same sample from the third dataset, Rothman et al. (2021), but switched to a different sample when I realised the first had undergone panel enrichment for respiratory viruses and so wasn’t truly untargeted.\nFor each sample, I ran rRNA removal on the FASTQ files that had been preprocessed with FASTP (without deduplication. For SortMeRNA and bbduk, I used reference files (1,2) downloaded from the SILVA database.\nThe commands I ran were as follows:"
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "href": "notebooks/2023-10-13_rrna-removal.html#high-level-output",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "High-level output",
    "text": "High-level output\nRunning Ribodetector in high-MCC mode set took 1266 seconds (just over 21 minutes). The tool identified 6.87M out of 15.25M read pairs as ribosomal: 45.0%. Re-running in high-recall mode took a similar amount of time (1264 seconds) and identified 7.76M reads as ribosomal: 50.9%.\nRunning bbduk took a total of 36 seconds. The tool identified 7.57M out of 15.25M read pairs as ribosomal: 49.63%, a little under the result for Ribodetector’ high-recall mode.\nThe default bbduk command given above uses the default k-mer size of 27. Increasing k will increase the precision and decrease the recall of the bbduk algorithm, potentially moving the results closer to the high-MCC version of Ribodetector. After some experimentation, I found that a k-mer length of 43 returned a ribosomal fraction of 45.39%, only slightly above high-MCC ribodetector."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "href": "notebooks/2023-10-13_rrna-removal.html#overlap-between-tools",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Overlap between tools",
    "text": "Overlap between tools\nTo compare the output of Ribodetector (high-MCC and high-recall) and bbduk (low and high k) in more detail, I extracted and downloaded the read IDs from their respective rRNA files and compared the overlap between the lists of IDs:\n\nCodedata_dir &lt;- \"../data/2023-10-16_ribodetection/\"\njohnson_bbduk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_failed_ids.txt.gz\")\njohnson_bbduk_highk_reads_path &lt;- file.path(data_dir, \"COMO4_bbduk_highk_failed_ids.txt.gz\")\njohnson_rd_precision_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_failed_ids_1.txt.gz\")\njohnson_rd_recall_reads_path &lt;- file.path(data_dir, \"COMO4_ribodetector_recall_failed_ids_1.txt.gz\")\njohnson_bbduk_reads &lt;- readLines(johnson_bbduk_reads_path)\njohnson_bbduk_highk_reads &lt;- readLines(johnson_bbduk_highk_reads_path)\njohnson_rd_precision_reads &lt;- readLines(johnson_rd_precision_reads_path)\njohnson_rd_recall_reads &lt;- readLines(johnson_rd_recall_reads_path)\njohnson_read_ids &lt;- list(bbduk = johnson_bbduk_reads,\n                 bbduk_highk = johnson_bbduk_highk_reads,\n                 rd_precision = johnson_rd_precision_reads,\n                 rd_recall = johnson_rd_recall_reads)\n\n\n\nCodeg_venn_johnson &lt;- ggVennDiagram(johnson_read_ids, label_alpha=0, edge_size = 0) +\n  scale_fill_gradient(low = \"#FFFFFF\", high = \"#4981BF\")\ng_venn_johnson\n\n\n\n\nSome observations:\n\nAs expected, the rRNA reads returned by bbduk with a high k-value are a strict subset of those returned with a low k-value, and those returned by Ribodetector in high-precision mode are a strict subset of those returned in high-recall mode.\n\nAcross all read IDs identified as ribosomal under any of the four conditions, 82% (6.42M read pairs) were identified as such by all four conditions. Of those that remain:\n\n5% are identified by all conditions except high-k bbduk;\n6% are identified by all conditions except high-precision Ribodetector;\n2% are identified by both high-recall conditions, but neither high-precision condition;\n3% are identified by high-recall Ribodetector only;\n1% are identified by low-k bbduk only;\n1% by some other combination of conditions\n\n\nOverall, it seems that, while the two high-recall methods exhibit quite good agreement (sharing &gt;95% of identified sequences), the two high-precision methods agree less well (with &gt;10% of all identified sequences identified by one but not the other)."
  },
  {
    "objectID": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "href": "notebooks/2023-10-13_rrna-removal.html#footnotes",
    "title": "Comparing Ribodetector and bbduk for rRNA detection",
    "section": "Footnotes",
    "text": "Footnotes\n\nI also began investigating SortMeRNA, a published tool based on heuristic alignment, which generally appeared to perform second-best on the quality metrics from the Ribodetector paper. However, I quickly dropped SortMeRNA, as it was substantially slower than Ribodetector.↩︎\nAt least as far as Kraken2 is concerned. I don’t entirely trust Kraken2’s assignments, but am going with them for now. I might come back and dig more into this aspect of things later if it seems useful.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-19_deduplication.html",
    "href": "notebooks/2023-10-19_deduplication.html",
    "title": "Comparing options for read deduplication",
    "section": "",
    "text": "See also:\n\nComparing FASTP and AdapterRemoval for MGS pre-processing\nComparing Ribodetector and bbduk for rRNA detection\n\nDuplicate read pairs can arise in sequencing data via several mechanisms.\n\nBiological duplicates are sequences that arise from different source nucleic-acid molecules that genuinely have the same sequence; these tend to arise when a particular gene or taxon is both extremely abundant and has low sequence diversity. In our case, the most likely cause of biological duplicates are ribosomal RNAs.\nTechnical duplicates, meanwhile, arise when the same input molecule produces multiple reads. Subgroups of technical duplicates include PCR duplicates arising from amplification of a single input sequence into many library molecules, and several forms of sequencing duplicates arising from errors in the sequencing process. For example, Illumina sequencing on unpatterned flow cells can give rise to optical duplicates, where a single cluster on the flow cell is falsely identified as two by the base-calling algorithm.\n\nIn general, we want to remove or collapse technical duplicates, while retaining biological duplicates. Unfortunately, in the absence of UMIs, there’s generally no way to distinguish biological and PCR duplicates; however, many forms of sequencing duplicates can often be identified from the sequence metadata provided in the FASTQ file.\nA number of tools are available that attempt to remove some or all of the duplicate sequences in a file. Some of these use cluster positioning information to distinguish sequencing duplicates from other duplicates, while others identify duplicates based purely on their base sequence. In the latter case, the tricky part is identifying the correct threshold for duplicate identification. Due to sequencing errors, requiring perfect base identity between two reads in order to designate them as duplicates often results in true duplicates surviving the deduplication process. On the other hand, designating two reads as duplicates based on too low a sequence identity (or too short a subsequence) will result in spurious deduplications that needlessly throw away useful data.\nAt the time of writing, our standard sequencing pipeline carries out deduplication very late in the process, during generation of JSON files for the dashboard (i.e. after generating clade counts). Read pairs are identified as duplicates if they are identical in the first 25 bases of both the forward and the reverse read, requiring 50nt of matches overall.\nI wanted to see if there was a widely-used read deduplication tool that we could apply to our pipeline, ideally early on as part of sample preprocessing. I started out comparing four approaches, before fairly quickly cutting down to two:\n\nfastp is a FASTQ pre-processing tool previously investigated here. If run with the --dedup`flag, it will remove read pairs that are exact duplicates of one another. As far as I know, fastp doesn’t have the ability to identify or remove inexact duplicates, or to distinguish sequencing duplicates from other duplicates. It thus represents a fast but relatively unsophisticated option.\nClumpify is a tool that was originally developed to reduce space and improve processing speed by rearranging fastq.gz files. It identifies duplicates by looking for reads (or read pairs) that match exactly in sequence, except for a specified number of permitted substitutions. This, to me, is the obvious way to detect duplicates, and this is the only deduplication tool I’ve found that does it this way. It also allows distinguishing of optical vs other duplicates and specific removal of only optical duplicates if desired.\nGATK Picard’s MarkDuplicates and samtools markdup are two functions for removing duplicate reads in SAM/BAM files. I tried both, but found them to be slow, confusing, and apparently unable to actually detect any duplicates in the files I ran them on. It’s possible that both of these tools only work on mapped reads (which would make sense given their demand for SAM/BAM files); it’s also possible that I could make them work given more time and effort, but I didn’t want to put this in unless the other tools I found proved inadequate.\n\nTo test fastp and Clumpify, I ran them on the same samples I used for looking into ribodetection:\n\n\nStudy\nBioproject\nSample\n\n\n\nRothman et al. (2021)\nPRJNA729801\nSRR14530880\n\n\nCrits-Cristoph et al. (2021)\nPRJNA661613\nSRR23998357\n\n\nJohnson (2023)\nN/A\nCOMO4\n\n\n\nIn each case, I tested deduplication at two points in the pipeline: immediately after preprocessing (with FASTP, without deduplication), and following segregation of rRNA reads using bbduk.\n1. Johnson (COMO4)\nFollowing fastp preprocessing, the Johnson sample contains 15.25M read pairs, 74.36% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (as identified by bbduk), this number fell to 69.72%, while for ribosomal reads it rose to 79.13%. Needless to say, all of these fail FASTQC’s read-duplication QC test.\nRunning clumpify on the fastp output took 16 seconds and removed 5.04M read pairs (33.0%) as duplicates, 11576 of which were optical. Running it on the bbduk non-ribosomal output took 9 seconds and removed 2.71M (35.3%) reads as duplicates, 7107 of which were optical. After deduplication with clumpify, FASTQC identifies 24.19% of sequences as duplicates in the post-fastp dataset, and 18.38% in the post-bbduk dataset. In both cases, this is a dramatic reduction, sufficient for FASTQC to now mark the duplication level QC as passing where it was previously failing.\nRunning fastp with deduplication enabled on the fastp output took 33 seconds and removed 4.21M read pairs (27.6%) as duplicates. Running it on the bbduk non-ribosomal output took 17 seconds and removed 2.33M read pairs (30.4%) as duplicates. In both cases, the number of reads removed is lower than that removed by clumpify, consistent with the fact that fastp requires complete identity between duplicates while clumpify allows a small number of mismatches. For whatever reason, this difference resulted in a dramatic difference in FASTQC quality metrics: after deduplication with fastp, FASTQC identifies 68.09% of reads as duplicates in the full dataset, and 59.78% in the post-bbduk dataset. Both of these are high enough to result in a failure for the corresponding QC test.\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n24.19\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n68.09\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n18.38\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n59.78\nFailed\n\n\n2. Rothman (SRR14530880)\nFollowing fastp preprocessing, the Rothman sample contains 13.61M read pairs, 81.27% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (6.20M read pairs, as identified by bbduk), this number fell to 78.79%. Both of these failed QC.\nRunning clumpify on the fastp output took 14 seconds and removed 7.41M read pairs (54.5%) as duplicates. Running it on the bbduk non-ribosomal output took 7 seconds and removed 2.96M read pairs (47.8%) as duplicates. In both cases, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 26 seconds and removed 6.87M read pairs (50.5%) as duplicates. Running it on the bbduk non-ribosomal output took 14 seconds and removed 2.71M read pairs (43.6%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n27.9\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n62.9\nFailed\n\n\nRibodepleted (BBDUK)\nClumpify\n33.0\nWarning\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n63.2\nFailed\n\n\n\nAs before, the difference is large, with Clumpify performing dramatically better.\n3. Crits-Christoph et al. (SRR23998357)\nFinally, we have the sample from Crits-Christoph et al. (2021). Following fastp processing, this sample contained 47.47M read pairs, 55.9% of which are identified by FASTQC as duplicates based on the first read in the pair. Among non-ribosomal reads (42.45M read pairs, as identified by bbduk), this number fell to 53.1%. Both of these failed QC.\nRunning clumpify on the fastp output took 90 seconds and removed 19.25M read pairs (40.6%) as duplicates. Running it on the bbduk non-ribosomal output took 65 seconds and removed 16.49M read pairs (38.8%) as duplicates. As with Rothman, trying to specifically detect optical reads caused the program to crash, suggesting that the relevant metadata was unavailable.\nRunning fastp with deduplication enabled on the fastp output took 94 seconds and removed 16.58M read pairs (34.9%) as duplicates. Running it on the bbduk non-ribosomal output took 85 seconds and removed 14.12M read pairs (33.3%) as duplicates. As before, the number of reads removed was lower than clumpify in both cases.\nFASTQ QC results were as follows:\n\nBoth programs perform much better here than for previous samples, with even fastp reducing levels of duplicates low enough to avoid failing. Nevertheless, clumpify continues to perform substantially better.\n\nDataset\nDedup Method\nFASTQC % Dup\nFASTQC Dup Test\n\n\n\nPreprocessed (FASTP)\nClumpify\n10.64\nPassed\n\n\nPreprocessed (FASTP)\nFASTP dedup\n35.22\nWarning\n\n\nRibodepleted (BBDUK)\nClumpify\n9.89\nPassed\n\n\nRibodepleted (BBDUK)\nFASTP dedup\n32.07\nWarning\n\n\n\n\nCodeduplicate_data_path &lt;- \"../data/2023-10-19_deduplication/duplicate-data.csv\"\nduplicate_data &lt;- read_csv(duplicate_data_path, show_col_types = FALSE) %&gt;%\n  mutate(sample_display = paste0(sample, \" (\", dataset, \")\"),\n         dedup_method = fct_inorder(dedup_method),\n         sample_display = fct_inorder(sample_display))\ng_duplicate &lt;- ggplot(duplicate_data, aes(x=dedup_method, y=fastqc_pc_dup, fill=sample_display)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(name = \"% Duplicates (FASTQC)\", limits = c(0,100),\n                     breaks = seq(0,100,20), expand = c(0,0)) +\n  scale_x_discrete(name = \"Deduplication method\") +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Sample\") +\n  facet_grid(. ~ processing_stage) +\n  theme_base\ng_duplicate\n\n\n\n\n4. Conclusion\nI was originally planning to do more validation here, but I honestly don’t think it’s required. Of the two methods I managed to successfully apply to these samples, Clumpify is faster; applies an intuitively more appealing deduplication method; removes more sequences; and achieves much better FASTQC results. It’s also very easy to use and configure. I recommend it as our deduplication tool going forward.\nIn terms of when to apply the tool, I think it probably makes most sense to apply deduplication downstream of counting (if we use Ribodetector) or detecting and filtering (if we use bbduk) ribosomal reads, to avoid spurious filtering of rRNA biological duplicates."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html",
    "href": "notebooks/2023-10-24_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-24_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-01_project-runway-comparison.html",
    "href": "notebooks/2023-11-01_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication, at least on its own, though it might still be successful if applied twice (after reverse-complementing one of the FASTQ files) or in combination with another tool.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I’ve updated negatively about Clumpify, which appears to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates). I’m not sure yet how to handle this; possibly it makes sense to run reads through multiple fast deduplication tools in order to catch as many duplicates as possible.\n\nFASTP deduplication is also unable to catch these cases.\nI’m current unaware of a public tool that will catch these. We might need to build our own."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html",
    "href": "notebooks/2023-10-31_project-runway-initial.html",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "",
    "text": "On 2023-10-23, we received the first batch of sequencing data from our wastewater protocol optimization work. This notebook contains the output of initial QC and analysis for this data."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-initial.html#footnotes",
    "title": "Initial analysis of Project Runway protocol testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nI initially ran FASTP without collapsing read pairs (--merge), as this makes for easier and more elegant downstream processing. However, this led to erroneous overidentification of read taxa during the Kraken2 analysis step by spuriously duplicating overlapping sequences. As such, I grudgingly implemented read-pair collapsing here as in the original pipeline.↩︎\nI initially ran FASTP without using pre-specified adapter sequences (–adapter_fasta adapters.fa). In most cases, FASTP successfully auto-detected and removed adapters, resulting in very low adapter prevalence in the preprocessed files. However, adapter trimming failed for two files: 230926EsvA_D23-13406-1_fastp_2.fastq.gz and 230926EsvA_D23-13406-2_fastp_2.fastq.gz. Digging into the FASTP logs, it looks like it failed to identify an adapter sequence for these files, resulting in inadequate adapter trimming. Rerunning FASTP while explicitly passing the Illumina TruSeq adapter sequences (alongside automated adapter detection) solved this problem in this instance.↩︎\nAfter running this, I realized this is probably an underestimate of the level of duplication, as it’s not counting duplicate reads from the same library sequenced on different lanes. I’ll look into cross-lane duplicates as part of a deeper investigation of duplication levels that I plan to do a bit later.↩︎\nIn the future, I plan to look at the effect of using different Kraken2 databases on read assignment, but for now I’m sticking with the database used in the main pipeline.↩︎"
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-comparison.html",
    "href": "notebooks/2023-11-02_project-runway-comparison.html",
    "title": "Comparing viral read assignments between pipelines on Project Runway data",
    "section": "",
    "text": "In my last notebook entry, I reviewed some basic initial analyses of Project Runway DNA sequencing data. One notable result was that the number of reads assigned to human-infecting viruses differed significantly between the pipeline I ran for that entry and the current public pipeline. In this entry, I dig into these differences in more depth, to see whether they tell us anything about which tools to incorporate into the next version of the public pipeline.\nAt a high level, there are three main differences between the two pipelines:\n\nThe public pipeline uses AdapterRemoval for removal of adapters and quality trimming, while my pipeline uses FASTP.\nMy pipeline uses bbduk to identify and remove ribosomal reads prior to Kraken analysis, while the public pipeline does not.\nMy pipeline applies deduplication prior to Kraken analysis using clumpify, while the public pipeline applies it after Kraken analysis via a manual method.\n\nIn principle, any of these differences could be responsible for the differences in read assignment we observe. However, since very few reads were identified as ribosomal or as duplicates by the new pipeline, it’s unlikely that these differences are those responsible.\nTo investigate this, I decided to manually identify and trace the reads assigned to human-infecting viruses in both pipelines, to see whether that tells us anything about the likely cause of the differences. To do this, I selected the three samples from the dataset that show the largest difference in the number of assigned human-infecting virus reads (henceforth HV reads):\n\nD23-13405-1 (14 HV reads assigned by the public pipeline vs 8 by the new pipeline)\nD23-13405-2 (12 vs 8)\nD23-13406-2 (17 vs 9)\n\nThe way the public pipeline does deduplication (after Kraken2 analysis, during dashboard generation) makes it difficult to directly extract the final list of HV read IDs for that pipeline, but it is quite easy to do this for the list of reads immediately prior to deduplication. Doing this for the samples specified above returned the following results:\n\nD23-13405-1: 14 read IDs (no reads lost during deduplication)\nD23-13405-2: 14 read IDs (2 reads to Simian Agent 10 lost during deduplication)\nD23-13406-2: 17 read IDs (no reads lost during deduplication)\n\nIn the first and third of these cases, I could thus directly compare the Kraken output from the two pipelines to investigate the source of the disagreements. In the second case, it wasn’t immediately possible to identify which two out of the three Simian Agent 10 reads were considered duplicates in the dashboard, but I was at least able to restrict the possibility to those three reads. (As we’ll see below, information from the new pipeline also helped narrow this down.)\n\nCodedata_dir &lt;- \"../data/2023-11-01_pr-comp\"\nread_status_path &lt;- file.path(data_dir, \"read-status.csv\")\nread_status &lt;- read_csv(read_status_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status))\ntheme_kit &lt;- theme_base + theme(\n  aspect.ratio = 1/2,\n  axis.text.x = element_text(hjust = 1, angle = 45),\n  axis.title.x = element_blank()\n)\ng_status &lt;- ggplot(read_status, aes(x=sample, y=n_reads, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Status\") +\n  scale_y_continuous(name = \"# Putative HV read pairs\", limits = c(0,10),\n                     breaks = seq(0,10,2), expand = c(0,0)) +\n  theme_base + theme_kit\ng_status\n\n\n\n\nD23-13405-1\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 reads were assigned by the the public pipeline.\nAll 8 of the reads assigned by the new pipeline were among the 14 HV reads assigned by the public pipeline.\n\nAmong the 6 remaining reads that were assigned by only the public pipeline:\n\n3 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n2 were included in the Kraken2 output for the new pipeline, but were not found to contain any HV k-mers and so were excluded from the list of hits. This is a more extreme case of the above situation: in this case, more stringent trimming by FASTQ has removed the putative HV k-mers.\n1 was found among the reads that FASTP discarded due to not passing quality filters; in this case, read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nIn all 6 cases, therefore, the difference in assignment between the two pipelines was found to be due to difference 1, i.e. the use of different preprocessing tools. In general, FASTP appears to be more stringent than AdapterRemoval in a way that resulted in fewer HV read assignments. But are these reads false positives for the old pipeline, or false negatives for the new one?\n\nTo address this, I extracted the raw sequences of the six read pairs, manually removed adapters, and manually analyzed them with NCBI BLAST (blastn vs viral nt, then vs full nt).\nIn all six cases, no match was found by blastn between the read sequence and the human-infecting virus putatively assigned by Kraken2 in the public pipeline. In four out of six cases, the best match for the read was to a bacterial sequence; in one case, the best match for the forward read was bacterial while the reverse matched a phage; and in one case no significant match was found for either read.\nThese results suggest to me that FASTP’s more stringent trimming and filtering is ensuring true negatives, rather than causing false ones.\n\n\nD23-13405-2\n\nIn this case, 8 HV reads were assigned by the new pipeline, and 14 by the public pipeline excluding deduplication; 2 of the latter were removed during deduplication for the dashboard.\n\nAll 8 of the reads assigned by the new pipeline were among the 14 pre-deduplication HV reads assigned by the public pipeline; however, two of these were among the group of three reads (all to Simian Agent 10) that were collapsed into one by deduplication in the public pipeline.\n\nThis indicates that one of the reads present in the new pipeline results was removed by deduplication in the public pipeline results – that is to say, the two pipelines disagree slightly more than the raw HV read counts would suggest.\nOne read was removed as a duplicate by both pipelines; I discarded this one from consideration, bringing the number of read IDs for consideration down to 13.\n\n\n\nAmong the remaining 5 HV reads from the public pipeline:\n\n4 appeared in the list of HV hits for the new pipeline; that is to say, for these four reads, the new pipeline was able to identify hits to human-infecting viruses but not make an overall assignment.\n1 was discarded by FASTP during quality filtering: read 2 was discarded due to low quality, and read 1 was then discarded due to lacking a read pair.\n\n\n\nAs before, I extracted the raw reads corresponding to these 5 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs full nt). 4 out of 5 read pairs showed no match to the assigned virus, while one showed a good, though partial, match.\n\nIn the case of the match, it looks like in both the public pipeline and the new pipeline, Kraken2 only identified a single k-mer from the origin virus (Sandfly fever Turkey virus); however, the public pipeline also identified 3 k-mers assigned to taxid 1 (root), while these were trimmed away in the new pipeline, and this was sufficient to prevent the overall assignment.\nI’m not sure how to feel about this case. Ex ante, making an assignment on the basis of a single unique k-mer and three uninformative k-mers feels quite dubious, but ex post it does appear that at least part of the read was correctly assigned.\n\n\n\nInvestigating the pair of reads that were flagged as duplicates by the public pipeline but not the new pipeline, I found that they were a perfect match, but with the sequence of the forward read in one pair matching the reverse read in the second pair.\n\nThis was surprising to me, as it suggests that clumpify won’t remove duplicates if one is in reverse-complement to the other, which seems like a major oversight. I checked this, and indeed Clumpify fails to detect the duplicate in the current state but succeeds if I swap the forward and reverse reads for one of the read pairs.\nThis makes me less excited about using Clumpify for deduplication. UPDATE: I found an option for Clumpify that seems to solve this problem, at least in this case. See Conclusions for more.\nIt’s worth noting that, due to the way FASTQC analyses files, it will also fail to detect RC duplicates.\n\n\nD23-13406-2\n\nIn this case, 9 HV reads were assigned by the new pipeline, and 17 by the public pipeline.\n\nAs before, I confirmed that all 9 HV reads assigned by the new pipeline were also assigned by the public pipeline, leaving 8 disagreements. Of these:\n\n6 appeared in the list of reads for which the new pipeline found HV hits, but wasn’t able to make an overall assignment; in all six of these cases, the HV hits were to the taxon assigned by the public pipeline.\n1 was included in the new pipeline’s Kraken output but had no viral hits.\nThe final clash is the most interesting, as this one was excluded not by FASTP or Kraken, but by bbduk: it was identified as ribosomal and discarded prior to deduplication.\n\n\n\nAs before, I extracted the raw reads corresponding to these 8 disagreements from the raw sequencing data, removed adapters manually, and ran the resulting sequences through NCBI BLAST (blastn vs viral nt, then vs nt).\n\nFor 7 of the 8 disagreements – all those arising from FASTP preprocessing – BLAST found no match between the read sequence and the taxon assigned by the public pipeline. As before, this suggests to be that FASTP is doing a good job preventing false positives through better preprocessing.\nThe BLAST result for the final disagreement is again the most interesting. The forward read indeed showed strong alignment to bacterial rRNA sequences, as found by bbduk. The reverse read, however, showed good alignment to Influenza A virus, which was the virus assigned by Kraken2 in the public pipeline. In this case, it appears we have a chimeric read, which both pipelines are in some sense processing correctly: bbduk is correctly identifying the forward read as ribosomal, and Kraken2 is correctly identifying the reverse read as viral. It’s not a priori obvious to me how we should handle these cases.\n\n\nSanity checking\n\nFinally, I wanted to check that my findings here weren’t just the result of some issue with how I’m using NCBI BLAST – that is, that BLAST as I’m using it is able to detect true positives rather than just failing to find matches all over the place.\nTo check this, I took the 24 read pairs (8 + 7 + 9) from the three samples above that both pipelines agreed arose from human-infecting viruses, and ran these through BLAST in the same way as the disagreed-upon read-pairs above.\nWhile the results weren’t as unequivocal as I’d hoped, they nevertheless showed a strong difference from those for the clashing read pairs. In total, 16/24 read pairs showed strong matching to the assigned virus, and an additional 2/24 showed a short partial match sufficient to explain the Kraken assignment. The remaining 6/24 failed to match the assigned virus. In total, 75% (18/24) of agreed-upon sequences showed a match to the assigned virus, compared to only 10% (2/20) for the clashing sequences.\nThis cautiously updates me further toward believing that the FASTP component of the new pipeline is mostly doing well at correctly rejecting true negatives, at least compared to the current public pipeline. That said, it also suggests that either (a) BLAST is generating a significant number of false negatives, or (b) even the new pipeline is generating a significant number of false positives.\n\n\nCoderead_hit_path &lt;- file.path(data_dir, \"read-hit-count.csv\")\nread_hit &lt;- read_csv(read_hit_path, show_col_types = FALSE) %&gt;%\n  mutate(status = fct_inorder(status),\n         p_hit = n_hit/n_reads)\ng_hit &lt;- ggplot(read_hit, aes(x=sample, y=p_hit, fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\", name = \"Status\") +\n  scale_y_continuous(name = \"% reads matching HV assignment\", limits = c(0,1),\n                     breaks = seq(0,1,0.2), expand = c(0,0), labels = function(y) y*100) +\n  theme_base + theme_kit\ng_hit\n\n\n\n\nConclusions\n\nMy updates from this exercise are different for different parts of the pipeline.\n\nFor difference 1 (preprocessing tool) I mostly found that, in cases where the new pipeline rejects a read due to preprocessing and the public pipeline does not, that read appears to be a true negative. I’m not super confident about this, since I don’t 100% trust BLAST to not be producing false negatives here, but overall I think the evidence points to FASTP doing a better job here than AdapterRemoval.\n\nI’d ideally like to shift to a version of the pipeline where we’re not relying on Kraken to make assignment decisions, and are instead running all Kraken hits through an alignment-based validation pipeline to determine final assignments. I’d be interested in seeing how these results look after making that change.\n\n\nFor difference 2 (ribodepletion) results here are equivocal. The single read pair I inspected that got removed during ribodepletion appears to include both a true ribosomal read and a true viral read. I think some internal discussion is needed to decide how to handle these cases.\n\nFor difference 3 (deduplication) I initially updated negatively about Clumpify, which appeared to be unable to handle duplicates where the forward and reverse reads in a pair are switched (this is also a case where FASTQC will be unable to detect these duplicates).\n\nHowever, I found an option for Clumpify which addresses this issue, at least in this case. Specifically, one can configure Clumpify to unpair reads, perform deduplication on the forward and reverse reads all together, and then restore pairing. This successfully removes this class of duplicates.\nI’m a little worried that this approach might sometimes cause complete loss of all duplicate reads (rather than all-but-one-pair) when the best-quality duplicate differs between forward and reverse reads. I tried this out by artificially modifying the quality scores for the duplicate reads from D23-13405-2, and this doesn’t seem to be the case at least in this instance: when quality across read pairs was concordant, I was able to control which read pair survived as expected, and when it was discordant, one read pair survived anyway. Still, this remains a niggling doubt."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "",
    "text": "One relevant question for both Project Runway and other NAO sequencing is: what is the maximum read depth at which we can sequence a given sample while retaining an acceptable level of sequence duplication?\nAs discussed in a previous entry, duplicate reads can arise in sequencing data from a variety of processes, including true biological duplicates present in the raw sample; processing duplicates arising from amplification and other processes during sample and library prep; and sequencing duplicates arising from various processes on the actual flow cell.\nAs we sequence more deeply, we expect the fraction of biological and processing duplicates (but not, I think, sequencing duplicates) in our read data to increase. In the former case, this is because we are capturing a larger fraction of all the input molecules in our sample; in the latter, because we are sequencing copies of the same sequence over and over again. Intuitively, I expect the increase in processing duplicates to swamp that in biological duplicates at high read depth, at least for library prep protocols that involve amplification1.\nOne simple approach to investigate the overall effect of read depth on duplication levels in the sample is rarefaction: downsampling a library to different numbers of reads and seeing how the duplication rate changes as a function of read count. In this notebook entry, I apply this approach to sequencing data from the Project Runway initial DNA dataset, to see how duplication rate behaves in this case."
  },
  {
    "objectID": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "href": "notebooks/2023-11-02_project-runway-dna-deduplication.html#footnotes",
    "title": "Estimating the effect of read depth on duplication rate for Project Runway DNA data",
    "section": "Footnotes",
    "text": "Footnotes\n\nIt might be worth explicitly modeling the difference in behavior between different kinds of duplicates as sequencing depth increases, to see if these intuitions are borne out.↩︎\nProbably the biggest two improvements that could be made to this model in future are (i) introducing biological duplicates, and (ii) introducing sequence-specific bias in PCR amplification and cluster formation.↩︎\nI’d appreciate it if someone else on the team can check my math here.↩︎"
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-10-31_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "",
    "text": "On 2023-11-06, we received the RNA-sequencing portion of the testing data we ordered in late September, partner to the DNA data I analyzed in a previous notebook. At the time, I didn’t do much in-depth analysis of these data, other than doing some basic QC and preprocessing and noting that the ribosomal fraction seemed very high. Now, having spent some time working on a new Nextflow workflow for analyzing sequencing data, I’m coming back to these data to apply that workflow and look at the results."
  },
  {
    "objectID": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "href": "notebooks/2023-12-19_project-runway-bmc-rna.html#footnotes",
    "title": "Workflow analysis of Project Runway RNA-seq testing data",
    "section": "Footnotes",
    "text": "Footnotes\n\nThough they might be too short; I don’t use CD-search enough to have a great sense of minimum length requirements.↩︎"
  },
  {
    "objectID": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "href": "notebooks/2023-12-22_bmc-rna-sequel.html",
    "title": "Project Runway RNA-seq testing data: removing livestock reads",
    "section": "",
    "text": "In my last entry, I presented my Nextflow workflow for analyzing viral MGS data, as well as the results of that workflow applied to our recent BMC RNA-seq dataset. One surprising thing I observed in those data was the presence of bovine and porcine sequences confounding my pipeline for identifying human-infecting-virus reads. To address this problem, I added a step to the pipeline to remove mammalian livestock sequences in a manner similar to the pre-existing human-removal step, by screening reads against cow and pig genomes using BBMap. In this short entry, I present the results of that change.\nAs expected, the bulk of the pipeline performed identically to the last analysis. Mammalian read depletion removed between 8k and 18k reads per protocol (0.007% to 0.017%):\n\nCode# Import stats\nkits &lt;- tibble(sample = c(\"1A\", \"1C\", \"2A\", \"2C\", \"6A\", \"6C\", \"SS1\", \"SS2\"),\n               kit = c(rep(\"Zymo quick-RNA kit\", 2),\n                       rep(\"Zymo quick-DNA/RNA kit\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (new)\", 2),\n                       rep(\"QIAamp Viral RNA mini kit (old)\", 2))) %&gt;%\n  mutate(kit = fct_inorder(kit))\nstages &lt;- c(\"raw_concat\", \"cleaned\", \"dedup\", \"ribo_initial\", \"remove_human\", \"remove_other\", \"ribo_secondary\")\ndata_dir_1 &lt;- \"../data/2023-12-19_rna-seq-workflow/\"\ndata_dir_2 &lt;- \"../data/2023-12-22_bmc-cow-depletion/\"\nbasic_stats_path &lt;- file.path(data_dir_2, \"qc_basic_stats.tsv\")\nadapter_stats_path &lt;- file.path(data_dir_2, \"qc_adapter_stats.tsv\")\nquality_base_stats_path &lt;- file.path(data_dir_2, \"qc_quality_base_stats.tsv\")\nquality_seq_stats_path &lt;- file.path(data_dir_2, \"qc_quality_sequence_stats.tsv\")\n# Extract stats\nbasic_stats &lt;- read_tsv(basic_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages))\nadapter_stats &lt;- read_tsv(adapter_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_base_stats &lt;- read_tsv(quality_base_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\nquality_seq_stats &lt;- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %&gt;% \n  inner_join(kits, by=\"sample\") %&gt;% mutate(stage = factor(stage, levels = stages), read_pair = fct_inorder(as.character(read_pair)))\n# Plot stages\nbasic_stats_stages &lt;- basic_stats %&gt;% group_by(kit,stage) %&gt;% \n  summarize(n_read_pairs = sum(n_read_pairs), n_bases_approx = sum(n_bases_approx), .groups = \"drop\", mean_seq_len = mean(mean_seq_len), percent_duplicates = mean(percent_duplicates))\ng_reads_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_read_pairs, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0)) +\n  theme_kit\ng_bases_stages &lt;- ggplot(basic_stats_stages, aes(x=kit, y=n_bases_approx, fill=stage)) +\n  geom_col(position=\"dodge\") + scale_fill_brewer(palette = \"Set3\", name=\"Stage\") +\n  scale_y_continuous(\"# Bases (approx)\", expand=c(0,0)) +\n  theme_kit\nlegend &lt;- get_legend(g_reads_stages)\ntnl &lt;- theme(legend.position = \"none\")\nplot_grid((g_reads_stages + tnl) + (g_bases_stages + tnl), legend, nrow = 2,\n          rel_heights = c(4,1))\n\n\n\n\n\nCode# Import composition data\ncomp_path &lt;- file.path(data_dir_2, \"taxonomic_composition.tsv\")\ncomp &lt;- read_tsv(comp_path, show_col_types = FALSE) %&gt;%\n  mutate(classification = sub(\"Other_filtered\", \"Other filtered\", classification)) %&gt;%\n  arrange(desc(p_reads)) %&gt;% mutate(classification = fct_inorder(classification))\ncomp_kits &lt;- inner_join(comp, kits, by=\"sample\") %&gt;%\n  group_by(kit, classification) %&gt;%\n  summarize(t_reads = sum(n_reads/p_reads), n_reads = sum(n_reads), .groups = \"drop\") %&gt;%\n  mutate(p_reads = n_reads/t_reads) %&gt;% ungroup\n# Plot overall composition\ng_comp &lt;- ggplot(comp_kits, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,1), breaks = seq(0,1,0.2),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp\n\n\n\nCode# Plot composition of minor components\nread_comp_minor &lt;- comp_kits %&gt;% filter(p_reads &lt; 0.1)\ng_comp_minor &lt;- ggplot(read_comp_minor, aes(x=kit, y=p_reads, fill=classification)) +\n  geom_col(position = \"stack\") +\n  scale_y_continuous(name = \"% Reads\", limits = c(0,0.02), breaks = seq(0,0.02,0.004),\n                     expand = c(0,0), labels = function(x) x*100) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  theme_kit + theme(aspect.ratio = 1/3)\ng_comp_minor\n\n\n\n\nTo identify human-viral reads, I used the multi-stage process specified in the last entry. Specifically, after removing human reads with BBmap, the remaining reads went through the following steps:\n\nAlign reads to a database of human-infecting virus genomes with Bowtie2, with permissive parameters, & retain reads with at least one match. (Roughly 20k read pairs per kit, or 0.25% of all surviving non-host reads.)\nRun reads that successfully align with Bowtie2 through Kraken2 (using the standard 16GB database) and exclude reads assigned by Kraken2 to any non-human-infecting-virus taxon. (Roughly 5500 surviving read pairs per kit.)\nCalculate length-adjusted alignment score \\(S=\\frac{\\text{bowtie2 alignment score}}{\\ln(\\text{read length})}\\). Filter out reads that don’t meet at least one of the following four criteria:\n\nThe read pair is assigned to a human-infecting virus by both Kraken and Bowtie2\nThe read pair contains a Kraken2 hit to the same taxid as it is assigned to by Bowtie2.\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the forward read\nThe read pair is unassigned by Kraken and \\(S&gt;15\\) for the reverse read\n\n\n\n\nCode# Import Bowtie2/Kraken data and perform filtering steps\nmrg_path &lt;- file.path(data_dir_2, \"hv_hits_putative_all.tsv\")\nmrg0 &lt;- read_tsv(mrg_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev))\nmrg1 &lt;- mrg0 %&gt;% filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2)\nmrg2 &lt;- mrg1 %&gt;% \n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\")))) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nmrg_all &lt;- bind_rows(mrg0, mrg1, mrg2)\n# Visualize\ng_mrg &lt;- ggplot(mrg_all, aes(x=adj_score_fwd, y=adj_score_rev, color=assigned_hv)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby Kraken2\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(filter_step~kit, labeller = labeller(filter_step=function(x) paste(\"Step\", x), kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg\n\n\n\n\nApplying all of these filtering steps left a total of 227 read pairs across all kits: 14 fewer than in the previous analysis. The 14 excluded read pairs included all 11 cow and pig reads, as well as three other read pairs classified as non-viral by BLAST. After removing these reads, the distribution of read scores vs BLAST HV status looked like the following, with many fewer high-scoring non-HV reads:\n\nCodemrg_old_path &lt;- file.path(data_dir_1, \"hv_hits_putative.tsv\")\nmrg_old &lt;- read_tsv(mrg_old_path, show_col_types = FALSE) %&gt;%\n  inner_join(kits, by=\"sample\") %&gt;% mutate(filter_step=1) %&gt;%\n  select(kit, sample, seq_id, taxid, assigned_taxid, adj_score_fwd, adj_score_rev, \n         classified, assigned_hv, query_seq_fwd, query_seq_rev, encoded_hits, \n         filter_step) %&gt;%\n  arrange(sample, desc(adj_score_fwd), desc(adj_score_rev)) %&gt;% \n  filter((!classified) | assigned_hv) %&gt;% mutate(filter_step=2) %&gt;%\n  mutate(hit_hv = !is.na(str_match(encoded_hits, paste0(\" \", as.character(taxid), \":\"))[1])) %&gt;%\n  filter(adj_score_fwd &gt; 15 | adj_score_rev &gt; 15 | assigned_hv | hit_hv) %&gt;% \n  mutate(filter_step=3)\nhv_blast_old &lt;- list(\n  `1A` = c(rep(TRUE, 40), TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE),\n  `1C` = c(rep(TRUE, 5), \"COWS\", TRUE, TRUE, FALSE, TRUE, rep(FALSE, 9)),\n  `2A` = c(rep(TRUE, 10), \"COWS\", TRUE, \"COWS\", \"COWS\", TRUE,\n           FALSE, \"COWS\", FALSE, TRUE, TRUE,\n           FALSE, FALSE, FALSE, FALSE, TRUE),\n  `2C` = c(rep(TRUE, 5), TRUE, \"COWS\", TRUE, TRUE, TRUE, \n           TRUE, TRUE, \"COWS\", TRUE, \"COWS\", \n           FALSE, FALSE, FALSE), \n  `6A` = c(rep(TRUE, 10), FALSE, TRUE, FALSE, FALSE, FALSE, \n           FALSE, TRUE, TRUE, FALSE), \n  `6C` = c(rep(TRUE, 5), \"PIGS\", TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, FALSE, FALSE, FALSE), \n  SS1  = c(rep(TRUE, 5), rep(FALSE, 10),\n           \"FALSE\", \"COWS\", \"FALSE\", \"FALSE\", \"FALSE\",\n           rep(FALSE, 15)\n           ),\n  SS2  = c(rep(TRUE, 25), TRUE, \"COWS\", TRUE, TRUE, TRUE,\n           TRUE, TRUE, TRUE, TRUE, TRUE,\n           FALSE, TRUE, TRUE, FALSE, FALSE,\n           FALSE, FALSE, TRUE, FALSE, FALSE,\n           rep(FALSE, 5),\n           FALSE, FALSE, FALSE, FALSE, TRUE,\n           FALSE, FALSE, TRUE, TRUE, FALSE)\n  )\nmrg_old_blast &lt;- mrg_old %&gt;% group_by(sample) %&gt;%\n  mutate(seq_num = row_number()) %&gt;% ungroup %&gt;%\n  mutate(hv_blast = unlist(hv_blast_old),\n         kraken_label = ifelse(assigned_hv, \"Kraken2 HV\\nassignment\",\n                               ifelse(hit_hv, \"Kraken2 HV\\nhit\",\n                                      \"No hit or\\nassignment\")))\nmrg_old_blast_filtered &lt;- mrg_old_blast %&gt;% filter(seq_id %in% mrg2$seq_id)\ng_mrg3_1 &lt;- mrg_old_blast_filtered %&gt;% mutate(hv_blast = hv_blast == \"TRUE\") %&gt;%\n  ggplot(aes(x=adj_score_fwd, y=adj_score_rev, color=hv_blast)) +\n  geom_point(alpha=0.5, shape=16) + \n  scale_color_brewer(palette=\"Set1\", name=\"Assigned to\\nhuman virus\\nby BLAST\") +\n  scale_x_continuous(name=\"S(forward read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  scale_y_continuous(name=\"S(reverse read)\", limits=c(0,65), breaks=seq(0,100,20), expand = c(0,0)) +\n  facet_grid(kraken_label~kit, labeller = labeller(kit = label_wrap_gen(20))) +\n  theme_base + theme(aspect.ratio=1)\ng_mrg3_1\n\n\n\n\nRepeating the exercise from the last entry, in which different score thresholds are assessed along different performance metrics, we find a significant improvement in optimal F1 score compared to the pre-filtering dataset, with a maximum F1 of 0.958 for a conjunctive threshold and 0.966 for a disjunctive threshold (both at a threshold score value of 20):\n\nCode# Test sensitivity and specificity\ntest_sens_spec &lt;- function(tab, score_threshold, conjunctive, include_special){\n  if (!include_special) tab &lt;- filter(tab, hv_blast %in% c(\"TRUE\", \"FALSE\"))\n  tab_retained &lt;- tab %&gt;% mutate(\n    conjunctive = conjunctive,\n    retain_score_conjunctive = (adj_score_fwd &gt; score_threshold & adj_score_rev &gt; score_threshold), \n    retain_score_disjunctive = (adj_score_fwd &gt; score_threshold | adj_score_rev &gt; score_threshold),\n    retain_score = ifelse(conjunctive, retain_score_conjunctive, retain_score_disjunctive),\n    retain = assigned_hv | hit_hv | retain_score) %&gt;%\n    group_by(hv_blast, retain) %&gt;% count\n  pos_tru &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  pos_fls &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", retain) %&gt;% pull(n) %&gt;% sum\n  neg_tru &lt;- tab_retained %&gt;% filter(hv_blast != \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  neg_fls &lt;- tab_retained %&gt;% filter(hv_blast == \"TRUE\", !retain) %&gt;% pull(n) %&gt;% sum\n  sensitivity &lt;- pos_tru / (pos_tru + neg_fls)\n  specificity &lt;- neg_tru / (neg_tru + pos_fls)\n  precision   &lt;- pos_tru / (pos_tru + pos_fls)\n  f1 &lt;- 2 * precision * sensitivity / (precision + sensitivity)\n  out &lt;- tibble(threshold=score_threshold, include_special = include_special, \n                conjunctive = conjunctive, sensitivity=sensitivity, \n                specificity=specificity, precision=precision, f1=f1)\n  return(out)\n}\nstats_conj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=TRUE) %&gt;% bind_rows\nstats_disj &lt;- lapply(15:45, test_sens_spec, tab=mrg_old_blast_filtered, include_special=TRUE, conjunctive=FALSE) %&gt;% bind_rows\nstats_all &lt;- bind_rows(stats_conj, stats_disj) %&gt;%\n  pivot_longer(!(threshold:conjunctive), names_to=\"metric\", values_to=\"value\") %&gt;%\n  mutate(conj_label = ifelse(conjunctive, \"Conjunctive\", \"Disjunctive\"))\nthreshold_opt &lt;- stats_all %&gt;% group_by(conj_label) %&gt;% filter(metric == \"f1\") %&gt;% filter(value == max(value)) %&gt;% filter(threshold == min(threshold))\ng_stats &lt;- ggplot(stats_all, aes(x=threshold, y=value, color=metric)) +\n  geom_line() +\n  geom_vline(data = threshold_opt, mapping=aes(xintercept=threshold), color=\"red\",\n             linetype = \"dashed\") +\n  scale_y_continuous(name = \"Value\", limits=c(0,1), breaks = seq(0,1,0.2), expand = c(0,0)) +\n  scale_x_continuous(name = \"Threshold\", expand = c(0,0)) +\n  facet_wrap(~conj_label) +\n  theme_base\ng_stats\n\n\n\n\nAs such, it appears that filtering mammalian livestock genomes is quite successful in improving our HV detection pipeline, at least for this dataset."
  },
  {
    "objectID": "notebooks/2024-01-30_blast-validation.html",
    "href": "notebooks/2024-01-30_blast-validation.html",
    "title": "Automating BLAST validation of human viral read assignment",
    "section": "",
    "text": "In previous entries, I presented the results of my Nextflow workflow on recent BMC RNA-seq data. To validate and calibrate my Bowtie2/Kraken2 pipeline for identifying human-viral reads, I manually BLASTed putative viral reads against NCBI’s nt database using their online BLAST tool, then assessed the results by eye to determine if each read likely came from a human virus. This approach was effective, but slow and manual, and thus hard to scale to other datasets I wanted to analyze with a higher relative abundance of viral sequences. I thus investigated options for automating the process to generate custom tabular BLAST results on the command line, which could then be read and processed programmatically.\nAmong the several options I investigated, the most promising were (1) Elastic-BLAST, NCBI’s own cloud-based BLAST tool, and (2) running regular command-line blast with the -remote option enabled to query NCBI’s online databases. I don’t currently have the AWS permissions needed to run Elastic-BLAST on our AWS service, and blastn -remote initially seemed too slow to be useful when run on an EC2 instance. However, I discovered that the latter option appears to run much more quickly when run on my local machine, making this a much more attractive option, at least until I’m able to run Elastic-BLAST.\nTo evaluate the potential for this approach to semi-automate the process of HV read validation, I ran BLASTN on the 227 putative HV reads from my previous BMC entry, using the following command:\nblastn -query &lt;input_path&gt; -out &lt;output_path&gt; -db nt -remote -perc_identity 60 -max_hsps 5 -num_alignments 50 -qcov_hsp_perc 30 -outfmt \"6 qseqid sseqid sgi staxid qlen evalue bitscore qcovs length pident mismatch gapopen sstrand qstart qend sstart send\"\nTo begin with, I first needed to identify a list of taxids I could use to designate a read as human-viral. To do this, I took the list of HV taxids generated by my Nextflow workflow and expanded it to include any missing descendent taxids using the NCBI taxid tree structure. I also generated another list of taxids that included any taxid descended from any of these taxids or the overall virus taxid (10239); this latter approach decreases the risk of false negatives at the cost of potentially increasing the risk of false positives from phage sequences. The two approaches generated lists of 28,105 and 234,499 taxids, respectively.\n\nCode# Set file paths\ndata_dir &lt;- \"../data/2024-01-30_blast/\"\nreads_db_path &lt;- file.path(data_dir, \"bmc-hv-putative.tsv\")\nblast_results_path &lt;- file.path(data_dir, \"bmc-hv-putative.blast\")\nhv_taxids_path &lt;- file.path(data_dir, \"human-virus-taxids-all.txt\")\ntax_nodes_path &lt;- file.path(data_dir, \"nodes.dmp.gz\")\n\n# Import files for viral taxid search\nhv_taxids &lt;- read_tsv(hv_taxids_path, show_col_types = FALSE, col_names = \"taxid\")\ntax_nodes &lt;- read_delim(tax_nodes_path, delim = \"\\t|\\t\", show_col_types = FALSE, \n                        col_names = FALSE) %&gt;%\n  select(X1:X3) %&gt;% rename(child_taxid = X1, parent_taxid = X2, rank = X3)\n\n# Define taxid search function\nexpand_taxids &lt;- function(taxids_in, nodes){\n  taxids_out &lt;- taxids_in\n  taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, !child_taxid %in% taxids_out) %&gt;%\n    pull(child_taxid) %&gt;% sort\n  while (length(taxids_new) &gt; 0){\n    taxids_out &lt;- c(taxids_out, taxids_new) %&gt;% sort\n    taxids_new &lt;- filter(nodes, parent_taxid %in% taxids_out, \n                         !child_taxid %in% taxids_out) %&gt;%\n      pull(child_taxid) %&gt;% sort\n  }\n  return(taxids_out)\n}\nv_taxids_1 &lt;- expand_taxids(hv_taxids$taxid, tax_nodes)\nv_taxids_2 &lt;- expand_taxids(c(10239, hv_taxids$taxid), tax_nodes)\n\n\nI next imported the BLAST alignment results and performed exploratory data analysis. Following visual inspection, it appeared that the following process would generate good results:\n\nIf both the forward and reverse reads from a read pair align to the same viral taxid (given the specified filters on query coverage, percent identity, etc), classify that read pair as human viral.\nIf only one of the reads in a read pair aligns to any given viral taxid, flag the read for manual inspection.\nIf neither read aligns to a viral taxid, classify that read pair as non-human-viral.\n\n\nCode# Prepare and import BLAST results\nreads_db &lt;- read_tsv(reads_db_path, show_col_types = FALSE)\nblast_cols &lt;- c(\"qseqid\", \"sseqid\", \"sgi\", \"staxid\", \"qlen\", \"evalue\", \"bitscore\", \"qcovs\", \"length\", \"pident\", \"mismatch\", \"gapopen\", \"sstrand\", \"qstart\", \"qend\", \"sstart\", \"send\")\nblast_results &lt;- read_tsv(blast_results_path, show_col_types = FALSE, col_names = blast_cols,\n                          col_types = cols(.default=\"c\"))\n\n# Filter BLAST results by read ID\nreads_db_seqs &lt;- reads_db %&gt;% mutate(read_id = paste(sample, seq_num, sep=\"_\")) %&gt;% pull(read_id) %&gt;% c(paste(., \"1\", sep=\"_\"), paste(., \"2\", sep=\"_\"))\nblast_results_filtered &lt;- blast_results %&gt;% filter(qseqid %in% reads_db_seqs)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_paired &lt;- blast_results_filtered %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_hviral &lt;- blast_results_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_out &lt;- blast_results_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  inner_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_inspected &lt;- blast_results_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status &lt;- blast_results_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_1 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_2 &lt;- blast_status %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance &lt;- bind_rows(blast_performance_1, blast_performance_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance\n\n\n\n  \n\n\n\nUsing the inferred list of human-virus taxids results in high precision and specificity, but low sensitivity, missing many reads flagged as human-viral by manual inspection. Conversely, using a list of all viral taxids as the reference achieves perfect sensitivity and near-perfect precision and specificity, with only one false positive result, resulting in an F1 score of over 99%.\nThat one false positive turned out to be a sequence with a high-identity but low-query-coverage human-viral match (to human enterovirus 71) which was excluded during my previous manual assessment; in my opinion, it’s unclear whether this should really be classed as a false positive. If we want to class this sequence as non-human-viral, increasing the query coverage threshold from 30% to 35% successfully excludes it without losing any true positives, resulting in perfect precision relative to manual assignments:\n\nCode# Filter BLAST results by query coverage\nblast_results_qcov &lt;- blast_results_filtered %&gt;% filter(as.numeric(qcovs) &gt;= 35)\n\n# Summarize BLAST results by read pair and subject taxid\nblast_results_qcov_paired &lt;- blast_results_qcov %&gt;%\n  separate(qseqid, c(\"sample\", \"seq_num\", \"read_pair\"), \"_\") %&gt;%\n  group_by(sample, seq_num, read_pair, staxid) %&gt;% filter(bitscore == max(bitscore)) %&gt;%\n  filter(length == max(length)) %&gt;% filter(row_number() == 1) %&gt;%\n  group_by(sample, seq_num, staxid) %&gt;%\n  mutate(bitscore = as.numeric(bitscore), seq_num = as.numeric(seq_num)) %&gt;%\n  summarize(bitscore_max = max(bitscore), bitscore_min = min(bitscore), \n            n_reads = n(), .groups = \"drop\")\n\n# Assign viral status to BLAST results\nblast_results_qcov_hviral &lt;- blast_results_qcov_paired %&gt;%\n  mutate(viral_1 = staxid %in% v_taxids_1,\n         viral_2 = staxid %in% v_taxids_2) %&gt;%\n  mutate(viral_1_full = viral_1 & n_reads == 2,\n         viral_2_full = viral_2 & n_reads == 2)\n\n# Assign putative viral status to read pairs\nblast_results_qcov_out &lt;- blast_results_qcov_hviral %&gt;%\n  group_by(sample, seq_num) %&gt;%\n  summarize(viral_status_1 = ifelse(any(viral_1_full), \"TRUE\", ifelse(any(viral_1), \"INSPECT\", \"FALSE\")),\n            viral_status_2 = ifelse(any(viral_2_full), \"TRUE\", ifelse(any(viral_2), \"INSPECT\", \"FALSE\")),\n            .groups = \"drop\") %&gt;%\n  full_join(reads_db %&gt;% select(sample, seq_num, hv_blast), by=c(\"sample\", \"seq_num\")) %&gt;%\n  mutate(viral_status_1 = replace_na(viral_status_1, \"FALSE\"),\n         viral_status_2 = replace_na(viral_status_2, \"FALSE\"))\n\n# Assume manual inspection produces same results as previous\nblast_results_qcov_inspected &lt;- blast_results_qcov_out %&gt;%\n  mutate(viral_status_1 = ifelse(viral_status_1 == \"INSPECT\", hv_blast, as.logical(viral_status_1)),\n         viral_status_2 = ifelse(viral_status_2 == \"INSPECT\", hv_blast, as.logical(viral_status_2)))\n\n# Evaluate performance vs fully manual inspection\nblast_status_qcov &lt;- blast_results_qcov_inspected %&gt;%\n    mutate(pos_tru_1 = viral_status_1 & hv_blast,\n           pos_fls_1 = viral_status_1 & !hv_blast,\n           neg_tru_1 = !viral_status_1 & !hv_blast,\n           neg_fls_1 = !viral_status_1 & hv_blast,\n           pos_tru_2 = viral_status_2 & hv_blast,\n           pos_fls_2 = viral_status_2 & !hv_blast,\n           neg_tru_2 = !viral_status_2 & !hv_blast,\n           neg_fls_2 = !viral_status_2 & hv_blast)\nblast_performance_qcov_1 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_1),\n            n_pos_fls = sum(pos_fls_1),\n            n_neg_tru = sum(neg_tru_1),\n            n_neg_fls = sum(neg_fls_1),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"Expanded human viral\")\nblast_performance_qcov_2 &lt;- blast_status_qcov %&gt;%\n  summarize(n_pos_tru = sum(pos_tru_2),\n            n_pos_fls = sum(pos_fls_2),\n            n_neg_tru = sum(neg_tru_2),\n            n_neg_fls = sum(neg_fls_2),\n            precision = n_pos_tru / (n_pos_tru + n_pos_fls),\n            sensitivity = n_pos_tru / (n_pos_tru + n_neg_fls),\n            specificity = n_neg_tru / (n_neg_tru + n_pos_fls),\n            f1 = 2 * precision * sensitivity / (precision + sensitivity)) %&gt;%\n  mutate(ref_taxids = \"All viral\")\nblast_performance_qcov &lt;- bind_rows(blast_performance_qcov_1, blast_performance_qcov_2) %&gt;% select(ref_taxids, n_pos_tru:f1)\nblast_performance_qcov\n\n\n\n  \n\n\n\nIn summary, using command-line blastn -remote, combined with tabular processing in R against a reference class of all viral taxids, works well as a substitute for manual inspection of online BLAST results for validating human-viral read assignments. It’s too slow and failure-prone to be added to the pipeline as a replacement or supplement to the Bowtie/Kraken automated approach, but will be my go-to approach in future for manual validation and refinement of human-viral assignment criteria."
  }
]